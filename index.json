[{"authors":["admin"],"categories":null,"content":"I am a full-time researcher (RTD) at the ItaliaNLP Lab, Institute for Computational Linguistics \u0026ldquo;A. Zampolli\u0026rdquo; (CNR-ILC, Pisa). In 2022, I received my PhD in Computer Science at the University of Pisa.\nMy research interests lie primarily in the context of Natural Language Processing (NLP) and in the study of Language Models (LM). I am particularly interested in the interpretability of large-scale LMs and in the evaluation of their internal representations, with a specific emphasis on understanding their inner linguistic abilities. Furthermore, I work in the development of NLP tools tailored for building educational applications.\nIn my free time, I enjoy going climbing, watching movies, listening to music, and reading books. Additionally, I have a passion for good beers and capturing moments with my analog camera.\n","date":1756252800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1756252800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://alemiaschi.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a full-time researcher (RTD) at the ItaliaNLP Lab, Institute for Computational Linguistics \u0026ldquo;A. Zampolli\u0026rdquo; (CNR-ILC, Pisa). In 2022, I received my PhD in Computer Science at the University of Pisa.\nMy research interests lie primarily in the context of Natural Language Processing (NLP) and in the study of Language Models (LM). I am particularly interested in the interpretability of large-scale LMs and in the evaluation of their internal representations, with a specific emphasis on understanding their inner linguistic abilities.","tags":null,"title":"Alessio Miaschi","type":"authors"},{"authors":["Davide Testa","Giovanni Bonetta","Raffaella Bernardi","Alessandro Bondielli","Alessandro Lenci","Alessio Miaschi","Lucia Passaro","Bernardo Magnini"],"categories":null,"content":"","date":1756252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756252800,"objectID":"9ccc424ff364a32b4555a61025bc12c9","permalink":"https://alemiaschi.github.io/publication/conference-paper/emnlp_2025/","publishdate":"2025-08-27T00:00:00Z","relpermalink":"/publication/conference-paper/emnlp_2025/","section":"publication","summary":"We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs' consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers.","tags":["Source Themes"],"title":"All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark","type":"publication"},{"authors":null,"categories":null,"content":"","date":1756252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756252800,"objectID":"cb6c16a1f2cc2fd4c9baaaa61b53a103","permalink":"https://alemiaschi.github.io/talk/emnlp2025/","publishdate":"2025-08-27T00:00:00Z","relpermalink":"/talk/emnlp2025/","section":"talk","summary":"","tags":null,"title":"EMNLP 2025 Findings Paper","type":"talk"},{"authors":["Cristiano Ciaccio","Marta Sartor","Alessio Miaschi","Felice Dell'Orletta"],"categories":null,"content":"","date":1753574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753574400,"objectID":"9059c56fa686ce987317dc770afa324e","permalink":"https://alemiaschi.github.io/publication/conference-paper/acl_2025_2/","publishdate":"2025-05-20T00:00:00Z","relpermalink":"/publication/conference-paper/acl_2025_2/","section":"publication","summary":"Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are 'character-blind' and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots. ","tags":["Source Themes"],"title":"Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models","type":"publication"},{"authors":["Cristiano Ciaccio","Alessio Miaschi","Felice Dell'Orletta"],"categories":null,"content":"","date":1753574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753574400,"objectID":"4e33cdf866a5f2c601734c2a532f5ab5","permalink":"https://alemiaschi.github.io/publication/conference-paper/acl_2025_3/","publishdate":"2025-05-20T00:00:00Z","relpermalink":"/publication/conference-paper/acl_2025_3/","section":"publication","summary":"We present a novel evaluation framework designed to assess the lexical proficiency and linguistic creativity of Transformer-based Language Models (LMs). We validate the framework by analyzing the performance of a set of LMs of different sizes, in both mono- and multilingual configuration, across tasks involving the generation, definition, and contextual usage of lexicalized words, neologisms, and nonce words. To support these evaluations, we developed a novel dataset of lexical entries for the Italian language, including curated definitions and usage examples sourced from various online platforms. The results highlight the robustness and effectiveness of our framework in evaluating multiple dimensions of LMs' linguistic understanding and offers an insight, through the assessment of their linguistic creativity, on the lexical generalization abilities of LMs.","tags":["Source Themes"],"title":"Evaluating Lexical Proficiency in Neural Language Models","type":"publication"},{"authors":["Andrea Pedrotti","Michele Papucci","Cristiano Ciaccio","Alessio Miaschi","Giovanni Puccetti","Felice Dell'Orletta","Andrea Esuli"],"categories":null,"content":"","date":1753574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753574400,"objectID":"a76ca77467d02a2c70911beeef48fbef","permalink":"https://alemiaschi.github.io/publication/conference-paper/acl_2025_1/","publishdate":"2025-05-20T00:00:00Z","relpermalink":"/publication/conference-paper/acl_2025_1/","section":"publication","summary":"Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we evaluate the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. We develop a pipeline that fine-tunes language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT), obtaining generations more challenging to detect by current models. Additionally, we analyze the linguistic shifts induced by the alignment and how detectors rely on 'linguistic shortcuts' to detect texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detecting performances. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts. We release code, models, and data to support future research on more robust MGT detection benchmarks.","tags":["Source Themes"],"title":"Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors","type":"publication"},{"authors":null,"categories":null,"content":"","date":1753275600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753275600,"objectID":"34ae0b35362583788536758f8e1713ed","permalink":"https://alemiaschi.github.io/talk/evalita_2026/","publishdate":"2025-07-23T00:00:00Z","relpermalink":"/talk/evalita_2026/","section":"talk","summary":"","tags":null,"title":"Co-organizing EVALITA 2026","type":"talk"},{"authors":["Chiara Alzetta","Alessio Miaschi","Felice Dell'Orletta","Giulia Venturi","Simonetta Montemagni"],"categories":null,"content":"","date":1751068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751068800,"objectID":"e356606ca890facc080b36330e33c12c","permalink":"https://alemiaschi.github.io/publication/journal-article/lre/","publishdate":"2025-06-28T00:00:00Z","relpermalink":"/publication/journal-article/lre/","section":"publication","summary":"The paper introduces Parallel Trees, a novel multilingual treebank collection that includes 20 treebanks for 10 languages. The distinguishing property of this resource is that the sentences of each language are annotated using two syntactic representation paradigms (SRPs), respectively based on the notions of dependency and constituency. By aligning the annotations of existing resources, Parallel Trees represents an example of exploiting pre-existing treebanks to adapt them to novel applications. To illustrate its potential, we present a case study where the resource is employed as a benchmark to investigate whether and how BERT, one of the first prominent neural language models (NLMs), is sensitive to the dependency- and constituency-based approaches for representing the syntactic structure of a sentence. The case study results indicate that the model’s sensitivity fluctuates across languages and experimental settings. The unique nature of the Parallel Trees resource creates the prerequisites for innovative studies comparing dependency and phrase-structure trees, allowing for more focused investigations without the interference of lexical variation.","tags":["Source Themes"],"title":"Parallel Trees: a novel resource with aligned dependency and constituency syntactic representations","type":"publication"},{"authors":null,"categories":null,"content":"","date":1747612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747612800,"objectID":"b1d03bba6c614d5f0c64427390e6cb75","permalink":"https://alemiaschi.github.io/talk/acl2025/","publishdate":"2025-05-19T00:00:00Z","relpermalink":"/talk/acl2025/","section":"talk","summary":"","tags":null,"title":"ACL 2025 Papers","type":"talk"},{"authors":["Luca Moroni","Giovanni Puccetti","Pere-Lluís Huguet Cabot","Andrei Stefan Bejgu","Alessio Miaschi","Edoardo Barba","Felice Dell'Orletta","Andrea Esuli","Roberto Navigli"],"categories":null,"content":"","date":1745798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745798400,"objectID":"5163d2e6d61bb8f298b865ad73c7fe64","permalink":"https://alemiaschi.github.io/publication/conference-paper/naacl_2025/","publishdate":"2025-04-28T00:00:00Z","relpermalink":"/publication/conference-paper/naacl_2025/","section":"publication","summary":"An increasing number of pretrained Large Language Models (LLMs) are being released, though the majority are predominantly designed for English. While they can often handle other languages due to contamination or some degree of multilingual pretraining data, English-centric LLMs are not optimized for non-English languages. This leads to inefficient encoding (high token 'fertility') and slower inference times for those languages. In this work, we explore various vocabulary adaptation techniques to tailor English LLMs for the Italian language. We introduce Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that learns neural mapping to accomplish vocabulary substitution, which achieve state-of-the-art performances on several downstream tasks. We adapted two LLMs: Mistral-7b-v0.1, reducing token fertility by 25%, and Llama-3.1-8b, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, after the adaptation of the vocabulary, these models can recover their performances with a relatively limited stage of continual training on the target language. Finally, we test the adapted models' capabilities on several multi-choice and generative tasks.","tags":["Source Themes"],"title":"Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation","type":"publication"},{"authors":["Quim Motger","Alessio Miaschi","Felice Dell'Orletta","Xavier Franch","Jordi Marco"],"categories":null,"content":"","date":1745366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745366400,"objectID":"8705c235a8e033556bf433f2fda76768","permalink":"https://alemiaschi.github.io/publication/journal-article/empirical_software/","publishdate":"2025-04-23T00:00:00Z","relpermalink":"/publication/journal-article/empirical_software/","section":"publication","summary":"Mobile app review analysis presents unique challenges due to the low quality, subjective bias, and noisy content of user-generated documents. Extracting features from these reviews is essential for tasks such as feature prioritization and sentiment analysis, but it remains a challenging task. Meanwhile, encoder-only models based on the Transformer architecture have shown promising results for classification and information extraction tasks for multiple software engineering processes. This study explores the hypothesis that encoder-only large language models can enhance feature extraction from mobile app reviews. By leveraging crowdsourced annotations from an industrial context, we redefine feature extraction as a supervised token classification task. Our approach includes extending the pre-training of these models with a large corpus of user reviews to improve contextual understanding and employing instance selection techniques to optimize model fine-tuning. Empirical evaluations demonstrate that these methods improve the precision and recall of extracted features and enhance performance efficiency. Key contributions include a novel approach to feature extraction, annotated datasets, extended pre-trained models, and an instance selection mechanism for cost-effective fine-tuning. This research provides practical methods and empirical evidence in applying large language models to natural language processing tasks within mobile app reviews, offering improved performance in feature extraction.","tags":["Source Themes"],"title":"Leveraging encoder-only large language models for mobile app review feature extraction","type":"publication"},{"authors":["Lorenzo Cima","Alessio Miaschi","Amaury Trujillo","Marco Avvenuti","Felice Dell'Orletta","Stefano Cresci"],"categories":null,"content":"","date":1745280002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745280002,"objectID":"62e288e57f9789eda9d9489c9e272530","permalink":"https://alemiaschi.github.io/publication/conference-paper/www_2025/","publishdate":"2025-04-22T00:00:02Z","relpermalink":"/publication/conference-paper/www_2025/","section":"publication","summary":"AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation. ","tags":["Source Themes"],"title":"Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1743724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743724800,"objectID":"d1727b12debd8ecaa405b3ec2b1ece4f","permalink":"https://alemiaschi.github.io/talk/nlp4re_2025/","publishdate":"2025-04-04T00:00:00Z","relpermalink":"/talk/nlp4re_2025/","section":"talk","summary":"","tags":null,"title":"Invited Talk at NLP4RE @ REFSQ 2025 (Barcelona, Spain)","type":"talk"},{"authors":null,"categories":null,"content":"","date":1739923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1739923200,"objectID":"1687d50b68b0280bf5a939cc25802a44","permalink":"https://alemiaschi.github.io/talk/fair_workshop/","publishdate":"2025-02-19T00:00:00Z","relpermalink":"/talk/fair_workshop/","section":"talk","summary":"","tags":null,"title":"Talk at FAIR Spoke Workshop 2025","type":"talk"},{"authors":null,"categories":null,"content":"","date":1737590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737590400,"objectID":"e25ba5bc1f7eb0dbe981fa689dc235d3","permalink":"https://alemiaschi.github.io/talk/naacl2025/","publishdate":"2025-01-23T00:00:00Z","relpermalink":"/talk/naacl2025/","section":"talk","summary":"","tags":null,"title":"NAACL 2025 Findings Paper","type":"talk"},{"authors":null,"categories":null,"content":"","date":1737590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737590400,"objectID":"90990b92ed27c631bd32f5978b69fda6","permalink":"https://alemiaschi.github.io/talk/www2025/","publishdate":"2025-01-23T00:00:00Z","relpermalink":"/talk/www2025/","section":"talk","summary":"","tags":null,"title":"WWW 2025 Paper","type":"talk"},{"authors":null,"categories":null,"content":"","date":1733702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733702400,"objectID":"6859dc859b3097c8c4fdadef3b21ad07","permalink":"https://alemiaschi.github.io/talk/phd_genova/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/talk/phd_genova/","section":"talk","summary":"","tags":null,"title":"Talk at AI Seminars 2024/25","type":"talk"},{"authors":["Cristiano Ciaccio","Felice Dell'Orletta","Alessio Miaschi","Giulia Venturi"],"categories":null,"content":"","date":1730160002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730160002,"objectID":"7a81999246bb5ac27502d5baf763d55b","permalink":"https://alemiaschi.github.io/publication/conference-paper/clic_2024/","publishdate":"2024-10-29T00:00:02Z","relpermalink":"/publication/conference-paper/clic_2024/","section":"publication","summary":"State-of-the-art Large Language Models (LLMs) demonstrate exceptional proficiency across diverse tasks, yet systematic evaluations of their linguistic abilities remain limited. This paper addresses this gap by proposing a new evaluation framework leveraging the potentialities of Controllable Text Generation. Our approach evaluates the models' capacity to generate sentences that adhere to specific linguistic constraints and their ability to recognize the linguistic properties of their own generated sentences, also in terms of consistency with the specified constraints. We tested our approach on six Italian LLMs using various linguistic constraints.","tags":["Source Themes"],"title":"Controllable Text Generation To Evaluate Linguistic Abilities of Italian LLMs","type":"publication"},{"authors":["Michele Papucci","Alessio Miaschi","Felice Dell'Orletta"],"categories":null,"content":"","date":1730160002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730160002,"objectID":"af8c2ac1d1cc9df84e35875183ee481b","permalink":"https://alemiaschi.github.io/publication/conference-paper/nl4ai_2024/","publishdate":"2024-10-29T00:00:02Z","relpermalink":"/publication/conference-paper/nl4ai_2024/","section":"publication","summary":"Generative language models, particularly adopting text-to-text frameworks, have shown significant success in NLP tasks. While much research has focused on input representations via prompting techniques, less attention has been given to optimizing output representations. Previous studies found inconsistent effects of label representations on model performance in classification tasks using these models. In this work, we introduce a novel method for selecting well-performing label representations by leveraging the attention mechanisms of Transformer models. We used an Italian T5 model fine-tuned on a topic classification task, trained on posts extracted from online forums and categorized into 11 classes, to evaluate different label representation selection strategies. We've employed a context-mixing score called Value Zeroing to assess each token's impact to select possible representations from the training set. Our results include a detailed qualitative analysis to identify which label choices most significantly affect classification outcomes, suggesting that using our approach to select label representations can enhance performance.","tags":["Source Themes"],"title":"Fantastic Labels and Where to Find Them: Attention-Based Label Selection for Text-to-Text Classification","type":"publication"},{"authors":["Alessio Miaschi","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1730073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730073600,"objectID":"5b4c5bbbfde88df198893fdd46b282f4","permalink":"https://alemiaschi.github.io/publication/conference-paper/emnlp_2024/","publishdate":"2024-10-28T00:00:00Z","relpermalink":"/publication/conference-paper/emnlp_2024/","section":"publication","summary":"Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models' linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs' sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling' approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.","tags":["Source Themes"],"title":"Evaluating Large Language Models via Linguistic Profiling","type":"publication"},{"authors":null,"categories":null,"content":"","date":1728604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728604800,"objectID":"55399f6106909f6f98a35e72fc6c89bd","permalink":"https://alemiaschi.github.io/talk/clic_2024/","publishdate":"2024-10-11T00:00:00Z","relpermalink":"/talk/clic_2024/","section":"talk","summary":"","tags":null,"title":"CLiC-it 2024 Paper","type":"talk"},{"authors":null,"categories":null,"content":"","date":1728604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728604800,"objectID":"0c029500ee6ab4e0fff2e29b2df130a7","permalink":"https://alemiaschi.github.io/talk/nl4ai_2024/","publishdate":"2024-10-11T00:00:00Z","relpermalink":"/talk/nl4ai_2024/","section":"talk","summary":"","tags":null,"title":"NL4AI 2024 Paper","type":"talk"},{"authors":null,"categories":null,"content":"","date":1727654400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727654400,"objectID":"a76e50466668f3597af67e838c95c7ee","permalink":"https://alemiaschi.github.io/talk/emnlp2024/","publishdate":"2024-09-30T00:00:00Z","relpermalink":"/talk/emnlp2024/","section":"talk","summary":"","tags":null,"title":"EMNLP 2024 Paper","type":"talk"},{"authors":null,"categories":null,"content":"","date":1718323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718323200,"objectID":"56678b26065bf132b352f2776c5fbbfd","permalink":"https://alemiaschi.github.io/talk/rtda_fair/","publishdate":"2024-06-14T00:00:00Z","relpermalink":"/talk/rtda_fair/","section":"talk","summary":"","tags":null,"title":"New Position","type":"talk"},{"authors":["Alessio Miaschi","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1708905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708905600,"objectID":"1d4436d1c87768976dda44c041c2000d","permalink":"https://alemiaschi.github.io/publication/conference-paper/coling2024/","publishdate":"2024-02-26T00:00:00Z","relpermalink":"/publication/conference-paper/coling2024/","section":"publication","summary":"In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.","tags":["Source Themes"],"title":"Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)","type":"publication"},{"authors":null,"categories":null,"content":"","date":1708905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708905600,"objectID":"48dd98e73e3ab19f4c0ae6261f5ade42","permalink":"https://alemiaschi.github.io/talk/coling_2024/","publishdate":"2024-02-26T00:00:00Z","relpermalink":"/talk/coling_2024/","section":"talk","summary":"","tags":null,"title":"LREC-COLING 2024 Paper","type":"talk"},{"authors":null,"categories":null,"content":"","date":1705708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705708800,"objectID":"e978eaff51f75974b498821d6c147432","permalink":"https://alemiaschi.github.io/talk/buzzetti_prize_aiucd_2024/","publishdate":"2024-01-20T00:00:00Z","relpermalink":"/talk/buzzetti_prize_aiucd_2024/","section":"talk","summary":"","tags":null,"title":"Premio di ricerca \"Dino Buzzetti\" 2023","type":"talk"},{"authors":["Quim Motger","Alessio Miaschi","Felice Dell'Orletta","Xavier Franch","Jordi Marco"],"categories":null,"content":"","date":1705017602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705017602,"objectID":"dd7a2bc2806932f75e7ba174bad72939","permalink":"https://alemiaschi.github.io/publication/conference-paper/saner_2024/","publishdate":"2024-01-12T00:00:02Z","relpermalink":"/publication/conference-paper/saner_2024/","section":"publication","summary":"Mobile app reviews are a large-scale data source for software-related knowledge generation activities, including software maintenance, evolution and feedback analysis. Effective extraction of features (i.e., functionalities or characteristics) from these reviews is key to support analysis on the acceptance of these features, identification of relevant new feature requests and prioritization of feature development, among others. Traditional methods focus on syntactic pattern-based approaches, typically context-agnostic, evaluated on a closed set of apps, difficult to replicate and limited to a reduced set and domain of apps. Meanwhile, the pervasiveness of Large Language Models (LLMs) based on the Transformer architecture in software engineering tasks lays the groundwork for empirical evaluation of the performance of these models to support feature extraction. In this study, we present T-FREX, a Transformer-based, fully automatic approach for mobile app review feature extraction. First, we collect a set of ground truth features from users in a real crowdsourced software recommendation platform and transfer them automatically into a dataset of app reviews. Then, we use this newly created dataset to fine-tune multiple LLMs on a named entity recognition task under different data configurations. We assess the performance of T-FREX with respect to this ground truth, and we complement our analysis by comparing T-FREX with a baseline method from the field. Finally, we assess the quality of new features predicted by T-FREX through an external human evaluation. Results show that T-FREX outperforms on average the traditional syntactic-based method, especially when discovering new features from a domain for which the model has been fine-tuned.","tags":["Source Themes"],"title":"T-FREX: A Transformer-based Feature Extraction Method for Mobile App Reviews","type":"publication"},{"authors":null,"categories":null,"content":"","date":1703116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703116800,"objectID":"429cc6ed360dc3a12c61af91a0497d47","permalink":"https://alemiaschi.github.io/talk/saner2024/","publishdate":"2023-12-21T00:00:00Z","relpermalink":"/talk/saner2024/","section":"talk","summary":"","tags":null,"title":"SANER 2024 Paper","type":"talk"},{"authors":["Michele Papucci","Alessio Miaschi","Felice Dell'Orletta"],"categories":null,"content":"","date":1697500802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697500802,"objectID":"905022c8c8bd6df16d5206bf04936b3c","permalink":"https://alemiaschi.github.io/publication/conference-paper/clic_2023_2/","publishdate":"2023-10-17T00:00:02Z","relpermalink":"/publication/conference-paper/clic_2023_2/","section":"publication","summary":"In this paper, we present an evaluation of the influence of label selection on the performance of a Sequence-to-Sequence Transformer model in a classification task. Our study investigates whether the choice of words used to represent classification categories affects the model's performance, and if there exists a relationship between the model's performance and the selected words. To achieve this, we fine-tuned an Italian T5 model on topic classification using various labels. Our results indicate that the different label choices can significantly impact the model's performance. That being said, we did not find a clear answer on how these choices affect the model performances, highlighting the need for further research in optimizing label selection.","tags":["Source Themes"],"title":"Lost in Labels: An Ongoing Quest to Optimize Text-to-Text Label Selection for Classification","type":"publication"},{"authors":["Chiara Alzetta","Felice Dell'Orletta","Chiara Fazzone","Alessio Miaschi","Giulia Venturi"],"categories":null,"content":"","date":1697500802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697500802,"objectID":"9cbd2e2666e2eef87ed676c1a64887be","permalink":"https://alemiaschi.github.io/publication/conference-paper/clic_2023/","publishdate":"2023-10-17T00:00:02Z","relpermalink":"/publication/conference-paper/clic_2023/","section":"publication","summary":"Traditional genre-based approaches for book recommendations face challenges due to the vague definition of genres. To overcome this, we propose a novel task called Book Author Prediction, where we predict the author of a book based on user-generated reviews' writing style. To this aim, we first introduce the `Literary Voices Corpus' (LVC), a dataset of Italian book reviews, and use it to train and test machine learning models. Our study contributes valuable insights for developing user-centric systems that recommend leisure readings based on individual readers' interests and writing styles.","tags":["Source Themes"],"title":"Unmasking the Wordsmith: Revealing Author Identity through Reader Reviews","type":"publication"},{"authors":null,"categories":null,"content":"","date":1697155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697155200,"objectID":"ca915892aa9ab4b4ff7718156162af48","permalink":"https://alemiaschi.github.io/talk/bright_2023/","publishdate":"2023-10-13T00:00:00Z","relpermalink":"/talk/bright_2023/","section":"talk","summary":"","tags":null,"title":"Lab @ Bright Night 2023","type":"talk"},{"authors":null,"categories":null,"content":"","date":1696982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696982400,"objectID":"2b78b3fd5490ce15f8dfbc9b8986fca1","permalink":"https://alemiaschi.github.io/talk/clic-it_2023/","publishdate":"2022-10-11T00:00:00Z","relpermalink":"/talk/clic-it_2023/","section":"talk","summary":"","tags":null,"title":"CLiC-it 2023 Papers","type":"talk"},{"authors":["Chiara Alzetta","Dominique Brunato","Felice Dell'Orletta","Alessio Miaschi","Kenji Sagae","Claudia H. Sánchez-Gutiérrez","Giulia Venturi"],"categories":null,"content":"","date":1693785601,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693785601,"objectID":"4aa2ac061dbf91455fe73622c64deaa1","permalink":"https://alemiaschi.github.io/publication/conference-paper/evalita_2023/","publishdate":"2023-09-04T00:00:01Z","relpermalink":"/publication/conference-paper/evalita_2023/","section":"publication","summary":"Language Learning Development (LangLearn) is the EVALITA 2023 shared task on automatic language development assessment, which consists in predicting the evolution of the written language abilities of learners across time. LangLearn is conceived to be multilingual, relying on written productions of Italian and Spanish learners, and representative of L1 and L2 learning scenarios. A total of 9 systems were submitted by 5 teams. The results highlight the open challenges of automatic language development assessment.","tags":["Source Themes"],"title":"LangLearn at EVALITA 2023: Overview of the Language Learning Development Task","type":"publication"},{"authors":null,"categories":null,"content":"","date":1687392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687392000,"objectID":"0cf66e99ba8acb04f0eb31f50ed7d1ac","permalink":"https://alemiaschi.github.io/talk/documentation/","publishdate":"2023-06-22T00:00:00Z","relpermalink":"/talk/documentation/","section":"talk","summary":"","tags":null,"title":"Journal of Documentation 2023","type":"talk"},{"authors":["Chiara Alzetta","Felice Dell'Orletta","Alessio Miaschi","Elena Prat","Giulia Venturi"],"categories":null,"content":"","date":1687392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687392000,"objectID":"b4d925787ae660e33f398536bb4a68ea","permalink":"https://alemiaschi.github.io/publication/journal-article/documentation/","publishdate":"2023-06-22T00:00:00Z","relpermalink":"/publication/journal-article/documentation/","section":"publication","summary":"Purpose The authors’ goal is to investigate variations in the writing style of book reviews published on different social reading platforms and referring to books of different genres, which enables acquiring insights into communication strategies adopted by readers to share their reading experiences. Design/methodology/approach The authors propose a corpus-based study focused on the analysis of A Good Review, a novel corpus of online book reviews written in Italian, posted on Amazon and Goodreads, and covering six literary fiction genres. The authors rely on stylometric analysis to explore the linguistic properties and lexicon of reviews and the authors conducted automatic classification experiments using multiple approaches and feature configurations to predict either the review's platform or the literary genre. Findings The analysis of user-generated reviews demonstrates that language is a quite variable dimension across reading platforms, but not as much across book genres. The classification experiments revealed that features modelling the syntactic structure of the sentence are reliable proxies for discerning Amazon and Goodreads reviews, whereas lexical information showed a higher predictive role for automatically discriminating the genre. Originality/value The high availability of cultural products makes information services necessary to help users navigate these resources and acquire information from unstructured data. This study contributes to a better understanding of the linguistic characteristics of user-generated book reviews, which can support the development of linguistically-informed recommendation services. Additionally, the authors release a novel corpus of online book reviews meant to support the reproducibility and advancements of the research. ","tags":["Source Themes"],"title":"Tell me how you write and I'll tell you what you read: a study on the writing style of book reviews","type":"publication"},{"authors":null,"categories":null,"content":"","date":1686268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686268800,"objectID":"78c9ce28973c1dbda376b1bb3d23ffa6","permalink":"https://alemiaschi.github.io/talk/dcp_2023/","publishdate":"2023-06-05T00:00:00Z","relpermalink":"/talk/dcp_2023/","section":"talk","summary":"","tags":null,"title":"Talk at DCP23","type":"talk"},{"authors":null,"categories":null,"content":"","date":1684800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684800000,"objectID":"966de954c787ab7dd2b7eeb107604d24","permalink":"https://alemiaschi.github.io/talk/lectures_2023/","publishdate":"2023-05-23T00:00:00Z","relpermalink":"/talk/lectures_2023/","section":"talk","summary":"","tags":null,"title":"Lab @ Lectures on Computational Linguistics 2023","type":"talk"},{"authors":null,"categories":null,"content":"","date":1677024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677024000,"objectID":"10733eccb2734d47ac466d3dafbae212","permalink":"https://alemiaschi.github.io/talk/information/","publishdate":"2023-02-22T00:00:00Z","relpermalink":"/talk/information/","section":"talk","summary":"","tags":null,"title":"Information 2023, Volume 14, Number 3","type":"talk"},{"authors":["Alessio Miaschi","Chiara Alzetta","Dominique Brunato","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1677024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677024000,"objectID":"ea57fc17cba6c84606f38c554171c1e7","permalink":"https://alemiaschi.github.io/publication/journal-article/information/","publishdate":"2023-02-22T00:00:00Z","relpermalink":"/publication/journal-article/information/","section":"publication","summary":"The outstanding performance recently reached by neural language models (NLMs) across many natural language processing (NLP) tasks has steered the debate towards understanding whether NLMs implicitly learn linguistic competence. Probes, i.e., supervised models trained using NLM representations to predict linguistic properties, are frequently adopted to investigate this issue. However, it is still questioned if probing classification tasks really enable such investigation or if they simply hint at surface patterns in the data. This work contributes to this debate by presenting an approach to assessing the effectiveness of a suite of probing tasks aimed at testing the linguistic knowledge implicitly encoded by one of the most prominent NLMs, BERT. To this aim, we compared the performance of probes when predicting gold and automatically altered values of a set of linguistic features. Our experiments were performed on Italian and were evaluated across BERT’s layers and for sentences with different lengths. As a general result, we observed higher performance in the prediction of gold values, thus suggesting that the probing model is sensitive to the distortion of feature values. However, our experiments also showed that the length of a sentence is a highly influential factor that is able to confound the probing model’s predictions.","tags":["Source Themes"],"title":"Testing the Effectiveness of the Diagnostic Probing Paradigm on Italian Treebanks","type":"publication"},{"authors":null,"categories":null,"content":"","date":1671022800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671022800,"objectID":"17579fd32be6f05dc2966c6c4924630b","permalink":"https://alemiaschi.github.io/talk/evalita_2022/","publishdate":"2022-12-14T00:00:00Z","relpermalink":"/talk/evalita_2022/","section":"talk","summary":"","tags":null,"title":"LangLearn (Shared task at EVALITA 2023)","type":"talk"},{"authors":null,"categories":null,"content":"","date":1669939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669939200,"objectID":"f367fac9a63eba9aaafa0647166be6ef","permalink":"https://alemiaschi.github.io/talk/taslp/","publishdate":"2022-12-02T00:00:00Z","relpermalink":"/talk/taslp/","section":"talk","summary":"","tags":null,"title":"IEEE/ACM Transactions on Audio, Speech and Language Processing","type":"talk"},{"authors":["Alessio Miaschi","Dominique Brunato","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1669939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669939200,"objectID":"db26d7baf1a94e373f0ce288720997f6","permalink":"https://alemiaschi.github.io/publication/journal-article/taslp/","publishdate":"2022-12-02T00:00:00Z","relpermalink":"/publication/journal-article/taslp/","section":"publication","summary":"In this paper, we propose a comprehensive linguistic study aimed at assessing the implicit behaviour of one of the most prominent Neural Language Model (NLM) based on Transformer architectures, BERT (Devlin et al., 2019), when dealing with a particular source of noisy data, namely essays written by L1 Italian learners containing a variety of errors targeting grammar, orthography and lexicon. Differently from previous works, we focus on the pre-training stage and we devise two evaluation tasks aimed at assessing the impact of errors on sentence-level inner representations from two complementary perspectives, i.e. robustness and sensitivity. Our experiments show that BERT's ability to compute sentence similarity and to correctly encode a set of raw and morpho-syntactic properties of a sentence are differently modulated by the category of errors and that the error hierarchies in terms of robustness and sensitivity change across layer-wise representations.","tags":["Source Themes"],"title":"On Robustness and Sensitivity of a Neural Language Model: A Case Study on Italian L1 Learner Errors","type":"publication"},{"authors":null,"categories":null,"content":"","date":1669766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669766400,"objectID":"69cc9981bbe1bdfa301c1a117a6ab991","permalink":"https://alemiaschi.github.io/talk/nl4ai_2022/","publishdate":"2022-11-09T00:00:00Z","relpermalink":"/talk/nl4ai_2022/","section":"talk","summary":"","tags":null,"title":"NL4AI 2022 (AIxIA) Paper","type":"talk"},{"authors":["Michele Papucci","Chiara De Nigris","Alessio Miaschi","Felice Dell'Orletta"],"categories":null,"content":"","date":1669248002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669248002,"objectID":"67bfb67a3c9a4d7f22691476e96a0252","permalink":"https://alemiaschi.github.io/publication/conference-paper/nl4ai_2022/","publishdate":"2022-11-24T00:00:02Z","relpermalink":"/publication/conference-paper/nl4ai_2022/","section":"publication","summary":"In this paper, we propose an extensive evaluation of the first text-to-text Italian Neural Language Model (NLM), IT5, on a classification scenario. In particular, we test the performance of IT5 on several tasks involving both the classification of the topic and the style of a set of Italian posts. We assess the model in two different configurations, single- and multi-task classification, and we compare it with a more traditional NLM based on the Transformer architecture (i.e. BERT). Moreover, we test its performance in a few-shot learning scenario. We also perform a qualitative investigation on the impact of label representations in modeling the classification of the IT5 model. Results show that IT5 could achieve good results, although generally lower than the BERT model. Nevertheless, we observe a significant performance improvement of the Text-to-text model in a multi-task classification scenario. Finally, we found that altering the representation of the labels mainly impacts the classification of the topic","tags":["Source Themes"],"title":"Evaluating Text-To-Text Framework for Topic and Style Classification of Italian texts","type":"publication"},{"authors":null,"categories":null,"content":"","date":1666828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666828800,"objectID":"28fbb73bdad92a5b7853293782dbfb72","permalink":"https://alemiaschi.github.io/talk/pi_school_2022/","publishdate":"2022-10-27T00:00:00Z","relpermalink":"/talk/pi_school_2022/","section":"talk","summary":"","tags":null,"title":"Tech Talk","type":"talk"},{"authors":null,"categories":null,"content":"","date":1663718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663718400,"objectID":"c2d8f34313962e1670ada862ad8f6c7b","permalink":"https://alemiaschi.github.io/talk/summer_school_ai/","publishdate":"2022-08-03T00:00:00Z","relpermalink":"/talk/summer_school_ai/","section":"talk","summary":"","tags":null,"title":"Summer School - Advances in AI","type":"talk"},{"authors":["Alessio Miaschi","Gabriele Sarti","Dominique Brunato","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1658966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658966400,"objectID":"f3ac9836f4a155490911c20691a059a2","permalink":"https://alemiaschi.github.io/publication/journal-article/ijcol_2022/","publishdate":"2022-07-28T00:00:00Z","relpermalink":"/publication/journal-article/ijcol_2022/","section":"publication","summary":"In this paper, we present an in-depth investigation of the linguistic knowledge encoded by the transformer models currently available for the Italian language. In particular, we investigate how the complexity of two different architectures of probing models affects the performance of the Transformers in encoding a wide spectrum of linguistic features. Moreover, we explore how this implicit knowledge varies according to different textual genres and language varieties.","tags":["Source Themes"],"title":"Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties","type":"publication"},{"authors":["Alessio Miaschi","Andrea Amelio Ravelli","Felice Dell'Orletta"],"categories":null,"content":"","date":1658188802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658188802,"objectID":"7dd9b2a4f72860a757bdfa627fbad48c","permalink":"https://alemiaschi.github.io/publication/conference-paper/advance_ai_2022/","publishdate":"2022-07-19T00:00:02Z","relpermalink":"/publication/conference-paper/advance_ai_2022/","section":"publication","summary":"In this paper, we propose an evaluation of a Transformer-based punctuation restoration model for the Italian language. Experimenting with a BERT-base model, we perform several fine-tuning with different training data and sizes and tested them in an in- and cross-domain scenario. Moreover, we conducted an error analysis of the main weaknesses of the model related to specific punctuation marks. Finally, we test our system either quantitatively and qualitatively, by offering a typical task-oriented and a perception-based acceptability evaluation.","tags":["Source Themes"],"title":"Punctuation Restoration in Spoken Italian Transcripts with Transformers","type":"publication"},{"authors":null,"categories":null,"content":"","date":1653350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653350400,"objectID":"4ac3c1cc03122d417b4f20b099eea47b","permalink":"https://alemiaschi.github.io/talk/phd_thesis/","publishdate":"2022-05-24T00:00:00Z","relpermalink":"/talk/phd_thesis/","section":"talk","summary":"","tags":null,"title":"PhD Thesis Defense","type":"talk"},{"authors":["Alessio Miaschi"],"categories":null,"content":"","date":1653350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653350400,"objectID":"5fc7d868078ddec1ee2f927da0c157a4","permalink":"https://alemiaschi.github.io/publication/thesis/phd_thesis/","publishdate":"2022-05-24T00:00:00Z","relpermalink":"/publication/thesis/phd_thesis/","section":"publication","summary":"In the last few years, the analysis of the inner workings of state-of-the-art Neural Language Models (NLMs) has become one of the most addressed line of research in Natural Language Processing (NLP). Several techniques have been devised to obtain meaningful explanations and to understand how these models are able to capture semantic and linguistic knowledge. The goal of this thesis is to investigate whether exploiting NLP methods for studying human linguistic competence and, specifically, the process of written language evolution is it possible to understand the behaviour of state-of-the-art Neural Language Models (NLMs). First, we present an NLP-based stylometric approach for tracking the evolution of written language competence in L1 and L2 learners using a wide set of linguistically motivated features capturing stylistic aspects of a text. Then, relying on the same set of linguistic features, we propose different approaches aimed at investigating the linguistic knowledge implicitly learned by NLMs. Finally, we propose a study in order to investigate the robustness of one of the most prominent NLM, i.e. BERT, when dealing with different types of errors extracted from authentic texts written by L1 Italian learners.","tags":["Source Themes"],"title":"Tracking Linguistic Abilities in Neural Language Models","type":"publication"},{"authors":["Giorgia Albertin","Alessio Miaschi","Dominique Brunato"],"categories":null,"content":"","date":1638316802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316802,"objectID":"44b82134be89ee81c0403ea212b6c3eb","permalink":"https://alemiaschi.github.io/publication/conference-paper/clic_2021_2/","publishdate":"2021-11-12T00:00:02Z","relpermalink":"/publication/conference-paper/clic_2021_2/","section":"publication","summary":"In this paper we present a new evaluation resource for Italian aimed at assessing the role of textual connectives in the comprehension of the meaning of a sentence. The resource is arranged in two sections (acceptability assessment and cloze test), each one corresponding to a distinct challenge task conceived to test how subtle modifications involving connectives in real usage sentences influence the perceived acceptability of the sentence by native speakers and  Neural Language Models (NLMs). Although the main focus is the presentation of the dataset,  we also provide some preliminary data comparing human judgments and NLMs performance in the two tasks.","tags":["Source Themes"],"title":"On the role of Textual Connectives in Sentence Comprehension: a new Dataset for Italian","type":"publication"},{"authors":["Alessio Miaschi","Chiara Alzetta","Dominique Brunato","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1638316802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316802,"objectID":"6191cbf914eeab8983e1a6ce6e668af4","permalink":"https://alemiaschi.github.io/publication/conference-paper/clic_2021/","publishdate":"2021-11-12T00:00:02Z","relpermalink":"/publication/conference-paper/clic_2021/","section":"publication","summary":"Probing tasks are frequently used to evaluate whether the representations of Neural Language Models (NLMs) encode linguistic information. However, it is still questioned if probing classification tasks really enable such investigation or they simply hint for surface patterns in the data. We present a method to investigate this question by comparing the accuracies of a set of probing tasks on gold and automatically generated control datasets. Our results suggest that probing tasks can be used as reliable diagnostic methods to investigate the linguistic information encoded in NLMs representations.","tags":["Source Themes"],"title":"Probing Tasks Under Pressure","type":"publication"},{"authors":["Alessio Miaschi","Andrea Amelio Ravelli","Felice Dell'Orletta"],"categories":null,"content":"","date":1636675202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636675202,"objectID":"c5e2985fc1f00f8e82117aaf073fafd6","permalink":"https://alemiaschi.github.io/publication/conference-paper/nl4ai_2021/","publishdate":"2021-11-12T00:00:02Z","relpermalink":"/publication/conference-paper/nl4ai_2021/","section":"publication","summary":"In this paper, we propose an evaluation of a Transformer-based punctuation restoration model for the Italian language. Experimenting with a BERT-base model, we perform several fine-tuning with different training data and sizes and tested them in an in- and cross-domain scenario. Moreover, we offer a comparison in a multilingual setting with the same model fine-tuned on English transcriptions. Finally, we conclude with an error analysis of the main weaknesses of the model related to specific punctuation marks.","tags":["Source Themes"],"title":"Evaluating Transformer Models for Punctuation Restoration in Italian","type":"publication"},{"authors":null,"categories":null,"content":"","date":1636675200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636675200,"objectID":"e75c381093a3ed8961c49827c446fc1d","permalink":"https://alemiaschi.github.io/talk/nl4ai_2021/","publishdate":"2021-11-12T00:00:00Z","relpermalink":"/talk/nl4ai_2021/","section":"talk","summary":"","tags":null,"title":"NL4AI 2021 (AIxIA) Paper","type":"talk"},{"authors":null,"categories":null,"content":"","date":1635206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635206400,"objectID":"3f500d4363e378bd972c0497df7a3ed9","permalink":"https://alemiaschi.github.io/talk/clic-it_2021/","publishdate":"2021-10-26T00:00:00Z","relpermalink":"/talk/clic-it_2021/","section":"talk","summary":"","tags":null,"title":"CLiC-it 2021 Papers","type":"talk"},{"authors":["Giovanni Puccetti","Alessio Miaschi","Felice Dell'Orletta"],"categories":null,"content":"","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"ddea273666ee287e09ce8aeca246adca","permalink":"https://alemiaschi.github.io/publication/conference-paper/deelio2021_2/","publishdate":"2021-05-24T00:00:00Z","relpermalink":"/publication/conference-paper/deelio2021_2/","section":"publication","summary":"Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties.","tags":["Source Themes"],"title":"How Do BERT Embeddings Organize Linguistic Knowledge?","type":"publication"},{"authors":["Alessio Miaschi","Dominique Brunato","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"530967d887d9fa4f7e78183d9c97c22c","permalink":"https://alemiaschi.github.io/publication/conference-paper/deelio2021_1/","publishdate":"2021-05-24T00:00:00Z","relpermalink":"/publication/conference-paper/deelio2021_1/","section":"publication","summary":"This paper presents an investigation aimed at studying how the linguistic structure of a sentence affects the perplexity of two of the most popular Neural Language Models (NLMs), BERT and GPT-2. We first compare the sentence-level likelihood computed with BERT and the GPT-2’s perplexity showing that the two metrics are correlated. In addition, we exploit linguistic features capturing a wide set of morpho-syntactic and syntactic phenomena showing how they contribute to predict the perplexity of the two NLMs.","tags":["Source Themes"],"title":"What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity","type":"publication"},{"authors":["Lucio Messina","Lucia Busso","Claudia Roberta Combei","Ludovica Pannitto","Alessio Miaschi","Gabriele Sarti","Malvina Nissim"],"categories":null,"content":"","date":1619481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619481600,"objectID":"c6b2313a2cec181a172c3dc0a3218bf3","permalink":"https://alemiaschi.github.io/publication/conference-paper/teachingnlp2021_1/","publishdate":"2021-04-27T00:00:00Z","relpermalink":"/publication/conference-paper/teachingnlp2021_1/","section":"publication","summary":"We describe and make available the game-based material developed for a laboratory run at several Italian science festivals to popularize NLP among young students.","tags":["Source Themes"],"title":"A dissemination workshop for introducing young Italian students to NLP","type":"publication"},{"authors":["Ludovica Pannitto","Lucia Busso","Claudia Roberta Combei","Lucio Messina","Alessio Miaschi","Gabriele Sarti","Malvina Nissim"],"categories":null,"content":"","date":1619481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619481600,"objectID":"944539413ae057cf0a90732dbe75bb39","permalink":"https://alemiaschi.github.io/publication/conference-paper/teachingnlp2021_2/","publishdate":"2021-04-27T00:00:00Z","relpermalink":"/publication/conference-paper/teachingnlp2021_2/","section":"publication","summary":"Although Natural Language Processing (NLP) is at the core of many tools young people use in their everyday life, high school curricula (in Italy) do not include any computational linguistics education. This lack of exposure makes the use of such tools less responsible than it could be and makes choosing computational linguistics as a university degree unlikely. To raise awareness, curiosity, and longer-term interest in young people, we have developed an interactive workshop designed to illustrate the basic principles of NLP and computational linguistics to high school Italian students aged between 13 and 18 years. The workshop takes the form of a game in which participants play the role of machines needing to solve some of the most common problems a computer faces in understanding language: from voice recognition to Markov chains to syntactic parsing. Participants are guided through the workshop with the help of instructors, who present the activities and explain core concepts from computational linguistics. The workshop was presented at numerous outlets in Italy between 2019 and 2021, both face-to-face and online.","tags":["Source Themes"],"title":"Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students","type":"publication"},{"authors":null,"categories":null,"content":"","date":1618790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618790400,"objectID":"5a87322f0677ac99e38583c71393b6b1","permalink":"https://alemiaschi.github.io/talk/science_web_festival_2021/","publishdate":"2021-04-19T00:00:00Z","relpermalink":"/talk/science_web_festival_2021/","section":"talk","summary":"","tags":null,"title":"Science Web Festival Workshop","type":"talk"},{"authors":null,"categories":null,"content":"","date":1618790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618790400,"objectID":"10d89030d4ad0b78d2094d6a2baca8e6","permalink":"https://alemiaschi.github.io/talks/science_web_festival_2021/","publishdate":"2021-04-19T00:00:00Z","relpermalink":"/talks/science_web_festival_2021/","section":"talks","summary":"","tags":null,"title":"Science Web Festival Workshop","type":"talks"},{"authors":null,"categories":null,"content":"","date":1618531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618531200,"objectID":"11699ebc2705c84bb4f031511308dc9e","permalink":"https://alemiaschi.github.io/talk/workshops_naacl_2021/","publishdate":"2021-04-16T00:00:00Z","relpermalink":"/talk/workshops_naacl_2021/","section":"talk","summary":"","tags":null,"title":"NAACL 2021 Workshop Papers","type":"talk"},{"authors":null,"categories":null,"content":"","date":1618531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618531200,"objectID":"2b047d01e36c49248185d64763872031","permalink":"https://alemiaschi.github.io/talks/workshops_naacl_2021/","publishdate":"2021-04-16T00:00:00Z","relpermalink":"/talks/workshops_naacl_2021/","section":"talks","summary":"","tags":null,"title":"NAACL 2021 Workshop Papers","type":"talks"},{"authors":["Alessio Miaschi","Dominique Brunato","Felice Dell'Orletta"],"categories":null,"content":"","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"07ded6d5b06e62234756e1d954881f88","permalink":"https://alemiaschi.github.io/publication/journal-article/jowr/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/publication/journal-article/jowr/","section":"publication","summary":"In this study we present a Natural Language Processing (NLP)-based stylometric approach for tracking the evolution of written language competence in Italian L1 learners. The approach relies on a wide set of linguistically motivated features capturing stylistic aspects of a text, which were extracted from students' essays contained in CItA (Corpus Italiano di Apprendenti L1), the first longitudinal corpus of texts written by Italian L1 learners enrolled in the first and second year of lower secondary school. We address the problem of modeling written language development as a supervised classification task consisting in predicting the chronological order of essays written by the same student at different temporal spans. The promising results obtained in several classification scenarios allow us to conclude that it is possible to automatically model the highly relevant changes affecting written language evolution across time, as well as identifying which features are more predictive of this process. In the last part of the article, we focus the attention on the possible influence of background variables on language learning and we present preliminary results of a pilot study aiming at understanding how the observed developmental patterns are affected by information related to the school environment of the student.","tags":["Source Themes"],"title":"A NLP-based stylometric approach for tracking the evolution of L1 written language compentece","type":"publication"},{"authors":null,"categories":null,"content":"","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"e4dc8f498d288ee9a5f66225b7b1e3da","permalink":"https://alemiaschi.github.io/talk/jowr_paper/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/talk/jowr_paper/","section":"talk","summary":"","tags":null,"title":"Journal of Writing Research (JoWR) Paper","type":"talk"},{"authors":["Alessio Miaschi","Chiara Alzetta","Dominique Brunato","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1607990402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990402,"objectID":"e9a9c1e95aeb3db164f17c4a342a0683","permalink":"https://alemiaschi.github.io/publication/conference-paper/clic_2020_2/","publishdate":"2020-12-15T00:00:02Z","relpermalink":"/publication/conference-paper/clic_2020_2/","section":"publication","summary":"This paper explores the relationship between Neural Language Model (NLM) perplexity and sentence readability. Starting from the evidence that NLMs implicitly acquire sophisticated linguistic knowledge from a huge amount of training data, our goal is to investigate whether perplexity  is affected by linguistic features used to automatically assess sentence readability and if there is a correlation between the two metrics. Our findings suggest that this correlation is actually quite weak and the two metrics are affected by different linguistic phenomena.","tags":["Source Themes"],"title":"Is Neural Language Model Perplexity Related to Readability?","type":"publication"},{"authors":["Alessio Miaschi","Gabriele Sarti","Dominique Brunato","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1607990402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990402,"objectID":"95dd7a6c26a60ff5c15fab3cfcb7bb04","permalink":"https://alemiaschi.github.io/publication/conference-paper/clic_2020/","publishdate":"2020-12-15T00:00:02Z","relpermalink":"/publication/conference-paper/clic_2020/","section":"publication","summary":"In this paper we present an in-depth investigation of the linguistic knowledge encoded by the transformer models currently available for the Italian language. In particular, we investigate whether and how using different architectures of probing models affects the performance of Italian transformers in encoding a wide spectrum of linguistic features. Moreover, we explore how this implicit knowledge varies according to different textual genres.","tags":["Source Themes"],"title":"Italian Transformers Under the Linguistic Lens","type":"publication"},{"authors":["Chiara Alzetta","Alessio Miaschi","Felice Dell'Orletta","Frosina Koceva","Ilaria Torre"],"categories":null,"content":"","date":1607990401,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990401,"objectID":"00bbb65fd3e7a59ff784cb550fa4b9ae","permalink":"https://alemiaschi.github.io/publication/conference-paper/evalita_2020_2/","publishdate":"2020-12-15T00:00:01Z","relpermalink":"/publication/conference-paper/evalita_2020_2/","section":"publication","summary":"The Prerequisite Relation Learning (PRELEARN) task is the EVALITA 2020 shared task on concept prerequisite learning, which consists of classifying prerequisite relations between pairs of concepts distinguishing between prerequisite pairs and non-prerequisite pairs. Four sub-tasks were defined: two of them define different types of features that participants are allowed to use when training their model, while the other two define the classification scenarios where the proposed models would be tested. In total, 14 runs were submitted by 3 teams comprising 9 total individual participants.","tags":["Source Themes"],"title":"PRELEARN @ EVALITA 2020: Overview of the Prerequisite Relation Learning Task for Italian","type":"publication"},{"authors":["Lorenzo De Mattei","Graziella De Martino","Andrea Iovine","Alessio Miaschi","Marco Polignano","Giulia Rambelli"],"categories":null,"content":"","date":1607990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"be40face0e3d56c0f48de6a12ce60201","permalink":"https://alemiaschi.github.io/publication/conference-paper/evalita_2020/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/publication/conference-paper/evalita_2020/","section":"publication","summary":"Over the last years, the rise of novel sentiment analysis techniques to assess aspect-based opinions on product reviews has become a key component for providing valuable insights to both consumers and businesses. To this extent, we propose ATE ABSITA: the EVALITA 2020 shared task on Aspect Term Extraction and Aspect-Based Sentiment Analysis. In particular, we approach the task as a cascade of three subtasks: Aspect Term Extraction (ATE), Aspect-based Sentiment Analysis (ABSA) and Sentiment Analysis (SA). Therefore, we invited participants to submit systems designed to automatically identify the 'aspect term' in each review and to predict the sentiment expressed for each aspect, along with the sentiment of the entire review. The task received broad interest, with 27 teams registered and more than 45 participants. However, only three teams submitted their working systems. The results obtained underline the task’s difficulty, but they also show how it is possible to deal with it using innovative approaches and models. Indeed, two of them are based on large pre-trained language models as typical in the current state of the art for the English language.","tags":["Source Themes"],"title":"ATE_ABSITA @ EVALITA2020: Overview of the Aspect Term Extraction and Aspect-based Sentiment Analysis Task","type":"publication"},{"authors":null,"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"5c25604ac79f879438fbf445e76140b2","permalink":"https://alemiaschi.github.io/talk/coling_outstanding_2020/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/talk/coling_outstanding_2020/","section":"talk","summary":"","tags":null,"title":"COLING 2020 Outstanding Paper Award","type":"talk"},{"authors":null,"categories":null,"content":"","date":1602547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602547200,"objectID":"8e96dd93979c73e38512aa70ff17a632","permalink":"https://alemiaschi.github.io/talk/clic-it_2020/","publishdate":"2020-10-13T00:00:00Z","relpermalink":"/talk/clic-it_2020/","section":"talk","summary":"","tags":null,"title":"CLiC-it 2020 Papers","type":"talk"},{"authors":["Alessio Miaschi","Dominique Brunato","Felice Dell'Orletta","Giulia Venturi"],"categories":null,"content":"","date":1601942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601942400,"objectID":"28d4b6881f2def31ec83a202133bab50","permalink":"https://alemiaschi.github.io/publication/conference-paper/coling2020/","publishdate":"2020-10-06T00:00:00Z","relpermalink":"/publication/conference-paper/coling2020/","section":"publication","summary":"In this paper we investigate the linguistic knowledge learned by a Neural Language Model (NLM) before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT's capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence.","tags":["Source Themes"],"title":"Linguistic Profiling of a Neural Language Model","type":"publication"},{"authors":null,"categories":null,"content":"","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601424000,"objectID":"d0ec907081ea002a7ac479a827c69982","permalink":"https://alemiaschi.github.io/talk/coling_2020/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/talk/coling_2020/","section":"talk","summary":"","tags":null,"title":"COLING 2020 Paper","type":"talk"},{"authors":["Alessio Miaschi","Felice Dell'Orletta"],"categories":null,"content":"","date":1592784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592784000,"objectID":"cd6849c31545fea85680f43a7ec0705d","permalink":"https://alemiaschi.github.io/publication/conference-paper/repl4nlp2020/","publishdate":"2020-06-22T00:00:00Z","relpermalink":"/publication/conference-paper/repl4nlp2020/","section":"publication","summary":"In this paper we present a comparison between the linguistic knowledge encoded in the internal representations of a contextual Language Model (BERT) and a contextual-independent one (Word2vec). We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that, although BERT is capable of understanding the full context of each word in an input sequence, the implicit knowledge encoded in its aggregated sentence representations is still comparable to that of a contextual-independent model. We also find that BERT is able to encode sentence-level properties even within single-word embeddings, obtaining comparable or even superior results than those obtained with sentence representations.","tags":["Source Themes"],"title":"Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation","type":"publication"},{"authors":["Alessio Miaschi","Sam Davidson","Dominique Brunato","Felice Dell'Orletta","Kenji Sagae","Claudia Helena Sanchez-Gutierrez","Giulia Venturi"],"categories":null,"content":"","date":1592784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592784000,"objectID":"3bfbc11a453522c800c3d08ac60430bb","permalink":"https://alemiaschi.github.io/publication/conference-paper/bea2020/","publishdate":"2020-06-22T00:00:00Z","relpermalink":"/publication/conference-paper/bea2020/","section":"publication","summary":"In this paper we present an NLP-based approach for tracking the evolution of written language competence in L2 Spanish learners using a wide range of linguistic features automatically extracted from students’ written productions. Beyond reporting classification results for different scenarios, we explore the connection between the most predictive features and the teaching curriculum, finding that our set of linguistic features often reflect the explicit instructions that students receive during each course.","tags":["Source Themes"],"title":"Tracking the Evolution of Written Language Competence in L2 Spanish Learners","type":"publication"},{"authors":null,"categories":null,"content":"","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589241600,"objectID":"0551e41448da08a7b5a6f37739004714","permalink":"https://alemiaschi.github.io/talk/bea_2020/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/talk/bea_2020/","section":"talk","summary":"","tags":null,"title":"BEA-2020 Paper","type":"talk"},{"authors":null,"categories":null,"content":"","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"cf93e7a23ba4d7b5fb2e7ba857e84709","permalink":"https://alemiaschi.github.io/talk/repl4nlp_2020/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/talk/repl4nlp_2020/","section":"talk","summary":"","tags":null,"title":"RepL4NLP-2020 Paper","type":"talk"},{"authors":null,"categories":null,"content":"","date":1584363600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584363600,"objectID":"7f083a7a05612ffb2f57cdc4ea4a4d66","permalink":"https://alemiaschi.github.io/talk/evalita_2020/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/talk/evalita_2020/","section":"talk","summary":"","tags":null,"title":"Shared tasks at EVALITA 2020","type":"talk"},{"authors":null,"categories":null,"content":"","date":1574254800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574254800,"objectID":"1a98fed91eb3c2e90814aca6ba5a3b4a","permalink":"https://alemiaschi.github.io/talk/giveback_event/","publishdate":"2019-11-20T00:00:00Z","relpermalink":"/talk/giveback_event/","section":"talk","summary":"","tags":null,"title":"PhD Giveback Event","type":"talk"},{"authors":["Chiara Alzetta","Alessio Miaschi","Giovanni Adorni","Felice Dell'Orletta","Frosina Koceva","Samuele Passalacqua","Ilaria Torre"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"e8c9cf2830d05dc5a2ee35fd16d2bc96","permalink":"https://alemiaschi.github.io/publication/conference-paper/clic_2019/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/conference-paper/clic_2019/","section":"publication","summary":"This paper presents a method for prerequisite learning classification between educational concepts. The proposed system was developed by adapting a classification algorithm designed for sequencing Learning Objects to the task of ordering concepts from a computer science textbook. In order to apply the system to the new task, for each concept we automatically created a learning unit from the textbook using two criteria based on concept occurrences and burst intervals. Results are promising and suggest that further improvements could highly benefit the results.","tags":["Source Themes"],"title":"Prerequisite or Not Prerequisite? That's the problem! An NLP-based Approach for Concept Prerequisite Learning","type":"publication"},{"authors":["Alessio Miaschi","Chiara Alzetta","Franco Alberto Cardillo","Felice Dell'Orletta"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"c62ec06a50a3f160be0a7aeea618bddf","permalink":"https://alemiaschi.github.io/publication/conference-paper/bea2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/conference-paper/bea2019/","section":"publication","summary":"We present a new concept prerequisite learning method for Learning Object (LO) ordering that exploits only linguistic features extracted from textual educational resources. The method was tested in a cross- and in- domain scenario both for Italian and English. Additionally, we performed experiments based on a incremental training strategy to study the impact of the training set size on the classifier performances. The paper also introduces ITA-PREREQ, to the best of our knowledge the first Italian dataset annotated with prerequisite relations between pairs of educational concepts, and describe the automatic strategy devised to build it.","tags":["Source Themes"],"title":"Linguistically-Driven Strategy for Concept Prerequisites Learning on Italian","type":"publication"},{"authors":["Chiara Alzetta","Dominique Brunato","Andrea Cimino","Felice Dell'Orletta","Alessio Miaschi","Simonetta Montemagni","Valeria Quochi","Giulia Venturi"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"34581e7eebd6825636e066df4d380d00","permalink":"https://alemiaschi.github.io/publication/conference-paper/italia_2019/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/conference-paper/italia_2019/","section":"publication","summary":"Il contributo illustra le attività portate avanti dal Laboratorio ItaliaNLP Lab nel contesto dell’educazione, mostrando come strumenti di Trattamento Automatico della Lingua (TAL) per la profilazione linguistica del testo e l’accesso al contenuto permettano di: i) modellare le abilità linguistiche e di valutarne l’evoluzione nel corso dell’apprendimento e ii) supportare la creazione di risorse e percorsi didattici personalizzati rispetto alle competenze degli apprendenti e alle nuove modalità di fruizione anche in contesti di e-learning.","tags":["Source Themes"],"title":"Trattamento Automatico della Lingua per la creazione di percorsi didattici personalizzati","type":"publication"},{"authors":["Enrica Salvatori","Roberto Rosselli Del Turco","Chiara Alzetta","Chiara Di Pietro","Chiara Mannari","Alessio Miaschi"],"categories":null,"content":"","date":1506816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506816000,"objectID":"83f103dcb139e0d0ac2fb55956c18e40","permalink":"https://alemiaschi.github.io/publication/journal-article/pelavicino/","publishdate":"2017-10-01T00:00:00Z","relpermalink":"/publication/journal-article/pelavicino/","section":"publication","summary":"The Codice Pelavicino Digitale Project aims to publish an online digital edition of the relevant manuscript of the XIII century. In this paper features of the edition and related issues are addressed. Secondly we explain motivations for choosing a digital edition as a medium: we address the background, and common concerns in the context of Academy and clerical and historical archives. Finally we give insights on the international standard adopted to markup the text, i.e. XML-TEI, and EVT, a tool adopted to generate the final website and display texts and images.","tags":["Source Themes"],"title":"Il Codice Pelavicino tra edizione digitale e Public History","type":"publication"},{"authors":["Giuseppe Attardi","Laura Gorrieri","Alessio Miaschi","Ruggero Petrolito"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"f750ade3a8ac2a86c8584dd06b862c2d","permalink":"https://alemiaschi.github.io/publication/conference-paper/clic_2015/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/clic_2015/","section":"publication","summary":"Distributional Semantic Models (DSM) that represent words as vectors of weights  over a high dimensional feature space have proved very effective in representing semantic or syntactic word similarity. For certain tasks however it is important to represent contrasting aspects such as  polarity, opposite senses or idiomatic use of words. We present a method for computing discriminative word embeddings can be used in sentiment classification or any other task where one needs to discriminate between con-trasting semantic aspects. We present an experiment in the identification of reports on natural disasters in tweets by means of these embeddings.","tags":["Source Themes"],"title":"Deep learning for social sensing from tweets","type":"publication"},{"authors":["Enrica Salvatori","Emilio Riccardini","Laura Balletto","Roberto Rosselli Del Turco","Chiara Alzetta","Chiara Di Pietro","Chiara Mannari","Raffaele Masotti","Alessio Miaschi"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"ca597729b6141a749df46656093035c3","permalink":"https://alemiaschi.github.io/publication/book-article/pelavicino/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/book-article/pelavicino/","section":"publication","summary":"","tags":["Source Themes"],"title":"Codice Pelavicino. Edizione Digitale","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://alemiaschi.github.io/privacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"https://alemiaschi.github.io/terms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/terms/","section":"","summary":"","tags":null,"title":"","type":"page"}]