<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>News on Alessio Miaschi</title><link>https://alemiaschi.github.io/news/</link><description>Recent content in News on Alessio Miaschi</description><generator>Hugo -- 0.150.1</generator><language>en</language><lastBuildDate>Mon, 29 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://alemiaschi.github.io/news/index.xml" rel="self" type="application/rss+xml"/><item><title>Best Student Paper Award @ CLiC-it 2025</title><link>https://alemiaschi.github.io/news/best_student_paper_clic2025/</link><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/best_student_paper_clic2025/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Crossword Space: Latent Manifold Learning for Italian Crosswords and beyond&amp;rsquo;, lead by the PhD student Cristiano Ciaccio, won the CLiC-it 2025 Best Student Paper Award!
&lt;br/&gt;
You can check the paper at the following link: &lt;a href='https://clic2025.unica.it/wp-content/uploads/2025/09/25_main_long.pdf'&gt;&lt;a href="https://clic2025.unica.it/wp-content/uploads/2025/09/25_main_long.pdf" target="_blank"&gt;https://clic2025.unica.it/wp-content/uploads/2025/09/25_main_long.pdf&lt;/a&gt;&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>LM4DH @ RANLP 2025 Invited Talk</title><link>https://alemiaschi.github.io/news/lm4dh_2025/</link><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/lm4dh_2025/</guid><description>&lt;p&gt;Last week I had the pleasure to be invited as speaker at the LM4DH Workshop co-located with RANLP 2025. You can find the slides of my talk here: &lt;a href='Talk_LM4DH_2025.pdf'&gt;Slides&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>Cruciverb-IT (Shared task at EVALITA 2026)</title><link>https://alemiaschi.github.io/news/cruciverbit_evalita_2026/</link><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/cruciverbit_evalita_2026/</guid><description>&lt;p&gt;I am happy to announce that I will be co-organizing a shared task at EVALITA 2026, the evaluation campaign of NLP and Speech Tools for Italian, that will have place in Bari on February 26-27 2026. &lt;br/&gt;
For more information please visit the shared task web page:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href='https://sites.google.com/view/cruciverbit2026'&gt;Cruciverb-IT&lt;/a&gt;: Crossword Solving at EVALITA 2026&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>CLiC-it 2025 Papers</title><link>https://alemiaschi.github.io/news/clicit_2025/</link><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/clicit_2025/</guid><description>&lt;p&gt;I&amp;rsquo;m happy to share that I got three papers acceted at CLiC-it 2025:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Crossword Space: Latent Manifold Learning for Italian Crosswords and Beyond&lt;/b&gt; (Ciaccio C., Sarti G., Miaschi A., Dell'Orletta F.)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;The OuLiBench Benchmark: Formal Constraints as a Lens into LLM Linguistic Competence&lt;/b&gt; (Calderaro S., Miaschi A., Dell'Orletta F.)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;MAIA: a Benchmark for Multimodal AI Assessment&lt;/b&gt; (Testa D., Bonetta G., Bernardi R., Bondielli A., Lenci A., Miaschi A., Passaro L., Magnini B.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More info coming soon!&lt;/p&gt;</description></item><item><title>EMNLP 2025 Findings Paper</title><link>https://alemiaschi.github.io/news/emnlp2025/</link><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/emnlp2025/</guid><description>&lt;p&gt;I&amp;rsquo;m happy to share that I got a paper accepted at the Findings of EMNLP 2025! The paper, entitled &amp;lsquo;All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark&amp;rsquo; (with Testa D., Bonetta G., Bernardi R., Bondielli A., Lenci A., Passaro L. and Magnini B.) introduces MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs&amp;rsquo; consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers. &lt;br/&gt;&lt;/p&gt;</description></item><item><title>Co-organizing EVALITA 2026</title><link>https://alemiaschi.github.io/news/evalita_2026/</link><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/evalita_2026/</guid><description>&lt;p&gt;I am happy to announce that I will be co-organizing the 2026 edition of EVALITA. EVALITA is a periodic evaluation campaign of Natural Language Processing (NLP) and speech tools for the Italian language. The deadline for submitting your task proposal is &lt;b&gt;July 28th 2026&lt;/b&gt;.&lt;br&gt;
For more information please visit the EVALITA call for tasks webpage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href='https://www.evalita.it/campaigns/evalita-2026/evalita-2026-second-call-for-tasks/'&gt;EVALITA 2026 Call for Tasks&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>ACL 2025 Papers</title><link>https://alemiaschi.github.io/news/acl2025/</link><pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/acl2025/</guid><description>&lt;p&gt;I&amp;rsquo;m happy to share that I got three papers acceted at ACL 2025: one at the main conference and two in the Findings!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Main Conference&lt;/b&gt;: Evaluating Lexical Proficiency in Neural Language Models (with Ciaccio C. and Dell'Orletta F.)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Findings&lt;/b&gt;: Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models (with Ciaccio C., Sartor M. and Dell'Orletta F.)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Findings&lt;/b&gt;: Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors (with Pedrotti A., Papucci M., Ciaccio C., Puccetti G., Dell'Orletta F. and Esuli A.)
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More info coming soon!&lt;/p&gt;</description></item><item><title>Invited Talk at NLP4RE @ REFSQ 2025 (Barcelona, Spain)</title><link>https://alemiaschi.github.io/news/nlp4re_2025/</link><pubDate>Fri, 04 Apr 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/nlp4re_2025/</guid><description>&lt;p&gt;On April 7 I will give an invated talk during the REFSQ 2025 Workshop NLP4RE.&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Title&lt;/b&gt;: Evaluating Linguistic Abilities of Neural Language Models&lt;/h3&gt;&lt;br/&gt;
&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: The field of Natural Language Processing (NLP) has witnessed remarkable advancements in recent years, driven largely by the shift from traditional approaches to state-of-the-art neural network-based algorithms. Among these, Large-scale Language Models (LLMs) have shown remarkable performance across a wide range of tasks and in generating coherent and contextually relevant texts. This improvement, however, comes at the cost of interpretability, since deep neural models offer little transparency about their inner workings and their abilities. In response, a growing body of research is dedicated to evaluating and interpreting LLMs, aiming to shed light on the inner workings and linguistic abilities encoded by these systems. This talk explores recent studies that shed light on these abilities, highlighting how such insights enhance our understanding of model behaviour across various applications.&lt;/p&gt;</description></item><item><title>Talk at FAIR Spoke Workshop 2025</title><link>https://alemiaschi.github.io/news/fair_workshop/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/fair_workshop/</guid><description>&lt;p&gt;On February 20 I will give a talk during the FAIR Spoke Workshop 2025 at Sapienza Università di Roma.&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Title&lt;/b&gt;: Controllable Text Generation for Evaluating LLMs' Linguistic Competence&lt;/h3&gt;&lt;br/&gt;
&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: In this talk, I will provide an overview of the results obtained in the context of the FAIR project (Spoke 5) focused on the evaluation of the linguistic abilities of Large Language Models (LLMs). Specifically, I will highlight results from research on Controllable Text Generation (CTG), with a specific focus on the assessment of LLMs&amp;rsquo; abilities to generate text while adhering to specific linguistic constraints.&lt;/p&gt;</description></item><item><title>NAACL 2025 Findings Paper</title><link>https://alemiaschi.github.io/news/naacl2025/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/naacl2025/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation&amp;rsquo; (with Luca Moroni, Giovanni Puccetti, Pere-Lluís Huguet Cabot, Andrei Stefan Bejgu, Edoardo Barba, Felice Dell&amp;rsquo;Orletta, Andrea Esuli and Roberto Navigli) has been accepted at NAACL 2025 (Findings)! In this work, we explore various vocabulary adaptation techniques to tailor English LLMs for the Italian language. We introduce Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that learns neural mapping to accomplish vocabulary substitution, which achieve state-of-the-art performances on several downstream tasks. We adapted two LLMs: Mistral-7b-v0.1, reducing token fertility by 25%, and Llama-3.1-8b, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, after the adaptation of the vocabulary, these models can recover their performances with a relatively limited stage of continual training on the target language. Finally, we test the adapted models&amp;rsquo; capabilities on several multi-choice and generative tasks.&lt;/p&gt;</description></item><item><title>WWW 2025 Paper</title><link>https://alemiaschi.github.io/news/www2025/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/www2025/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation&amp;rsquo; (with Lorenzo Cima, Amaury Trujillo, Marco Avvenuti, Felice Dell&amp;rsquo;Orletta and Stefano Cresci) has been accepted at WWW 2025! In this paper we propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.&lt;/p&gt;</description></item><item><title>Talk at AI Seminars 2024/25</title><link>https://alemiaschi.github.io/news/phd_genova/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/phd_genova/</guid><description>&lt;p&gt;On December 9 I gave a talk during the AI Seminars of the PhD program in Digital Humanities of the University of Genova. The aim of the seminars is to present experiences and practices of using AI in different fields, to increase knowledge and develop awareness about the opportunities and risks of using AI in our research fields.&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Title&lt;/b&gt;: Evaluating Linguistic Abilities of Neural Language Models&lt;/h3&gt;&lt;br/&gt;
&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: The field of Natural Language Processing (NLP) has witnessed remarkable advancements in recent years, driven largely by the shift from traditional approaches to state-of-the-art neural network-based algorithms. Among these, Large-scale Language Models (LLMs) have shown remarkable performance across a wide range of tasks and in generating coherent and contextually relevant texts. This improvement, however, comes at the cost of interpretability, since deep neural models offer little transparency about their inner workings and their abilities. In response, a growing body of research is dedicated to analyzing and interpreting LLMs, aiming to shed light on the inner workings and linguistic abilities encoded by these systems.
This talk will be divided into two parts. The first part offers an overview of Language Models (LMs) and the recent advancements achieved by these models in the past few years. In the second part, we will focus on recent studies that examine these models’ implicit linguistic abilities, exploring how these insights can enhance our understanding of model behaviour across various tasks and applications.&lt;/p&gt;</description></item><item><title>CLiC-it 2024 Paper</title><link>https://alemiaschi.github.io/news/clic_2024/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/clic_2024/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Controllable Text Generation To Evaluate Linguistic Abilities of Italian LLMs&amp;rsquo; (with Cristiano Ciaccio, Felice Dell&amp;rsquo;Orletta and Giulia Venturi) has been accepted to CLiC-it 2024! In this paper we propose a new evaluation framework leveraging the potentialities of Controllable Text Generation. Our approach evaluates the models&amp;rsquo; capacity to generate sentences that adhere to specific linguistic constraints and their ability to recognize the linguistic properties of their own generated sentences, also in terms of consistency with the specified constraints. We tested our approach on six Italian LLMs using various linguistic constraints.&lt;/p&gt;</description></item><item><title>NL4AI 2024 Paper</title><link>https://alemiaschi.github.io/news/nl4ai_2024/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/nl4ai_2024/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Fantastic Labels and Where to Find Them: Attention-Based Label Selection for Text-to-Text Classification&amp;rsquo; (with Michele Papucci and Felice Dell&amp;rsquo;Orletta) has been accepted to NL4AI 2024! In this work, we introduce a novel method for selecting well-performing label representations by leveraging the attention mechanisms of Transformer models. We used an Italian T5 model fine-tuned on a topic classification task, trained on posts extracted from online forums and categorized into 11 classes, to evaluate different label representation selection strategies. We&amp;rsquo;ve employed a context-mixing score called Value Zeroing to assess each token&amp;rsquo;s impact to select possible representations from the training set. Our results include a detailed qualitative analysis to identify which label choices most significantly affect classification outcomes, suggesting that using our approach to select label representations can enhance performance.&lt;/p&gt;</description></item><item><title>EMNLP 2024 Paper</title><link>https://alemiaschi.github.io/news/emnlp2024/</link><pubDate>Mon, 30 Sep 2024 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/emnlp2024/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Evaluating Large Language Models via Linguistic Profiling&amp;rsquo; (with Felice Dell&amp;rsquo;Orletta and Giulia Venturi) has been accepted at EMNLP 2024! In this paper, we introduce a novel evaluation methodology designed to test LLMs&amp;rsquo; sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling&amp;rsquo; approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.&lt;/p&gt;</description></item><item><title>New Position</title><link>https://alemiaschi.github.io/news/rtda_fair/</link><pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/rtda_fair/</guid><description>&lt;p&gt;I am happy to announce that I have started a new position as a full-time researcher (RTDA) at CNR-ILC!
&lt;br/&gt;In this role, I will be contributing to the &lt;a href='https://fondazione-fair.it/en/'&gt;PNRR FAIR&lt;/a&gt; project, which focuses on addressing critical research questions, methodologies, models, and technologies in Artificial Intelligence (AI). Specifically, I am part of Spoke 5 of FAIR, which is focused on the analysis and the developemnt of high-quality AI systems.&lt;/p&gt;</description></item><item><title>LREC-COLING 2024 Paper</title><link>https://alemiaschi.github.io/news/coling_2024/</link><pubDate>Mon, 26 Feb 2024 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/coling_2024/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Linguistic Knowledge Can Enhance Encoder-Decoder Models (&lt;em&gt;If You Let It&lt;/em&gt;) (with Felice Dell&amp;rsquo;Orletta and Giulia Venturi) has been accepted at LREC-COLING 2024! &lt;br/&gt;
In the paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.&lt;/p&gt;</description></item><item><title>Premio di ricerca "Dino Buzzetti" 2023</title><link>https://alemiaschi.github.io/news/buzzetti_prize_aiucd_2024/</link><pubDate>Sat, 20 Jan 2024 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/buzzetti_prize_aiucd_2024/</guid><description>&lt;p&gt;I am really happy to announce that I was awarded by &lt;a href='http://www.aiucd.it/'&gt;AIUCD&lt;/a&gt; with the 2023 Prize &amp;lsquo;Premio di ricerca Dino Buzzetti&amp;rsquo;! The prize will fund a small project, based on the development of a book recommender system. In particular, during the project, I will first of all collect reviews from popular Digital Social Reading platforms, such as Goodreads and Anobii. Then, I plan to conduct a comprehensive human evaluation campaign to verify whether (and how) certain reviews are likely to be of interest for future readers. Another aspect of the project involves exploring the capabilities of Language Models (LLMs) to generate fictional reviews and assessing their impact on the overall evaluation campaign. &lt;br/&gt; All the informations are available at this link: &lt;a href='http://www.aiucd.it/premio-buzzetti-2023-a-alessio-miaschi/'&gt;&lt;a href="http://www.aiucd.it/premio-buzzetti-2023-a-alessio-miaschi/" target="_blank"&gt;http://www.aiucd.it/premio-buzzetti-2023-a-alessio-miaschi/&lt;/a&gt;&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>SANER 2024 Paper</title><link>https://alemiaschi.github.io/news/saner2024/</link><pubDate>Thu, 21 Dec 2023 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/saner2024/</guid><description>&lt;p&gt;Our paper &amp;lsquo;T-FREX: A Transformer-based Feature Extraction Method from Mobile App Reviews&amp;rsquo; (with Quim Motger, Felice Dell&amp;rsquo;Orletta, Xavier Franch and Jordi Marco) has been accepted at the IEEE International Conference on Software Analysis, Evaluation and Reengineering (SANER) 2024. In the paper we present T-FREX, a Transformer-based, fully automatic approach for mobile app review feature extraction. First, we collect a set of ground truth features from users in a real crowdsourced software recommendation platform and transfer them automatically into a dataset of app reviews. Then, we use this newly created dataset to fine-tune multiple LLMs on a named entity recognition task under different data configurations. We assess the performance of T-FREX with respect to this ground truth, and we complement our analysis by comparing T-FREX with a baseline method from the field. Finally, we assess the quality of new features predicted by T-FREX through an external human evaluation. Results show that T-FREX outperforms on average the traditional syntactic-based method, especially when discovering new features from a domain for which the model has been fine-tuned.&lt;/p&gt;</description></item><item><title>Lab @ Bright Night 2023</title><link>https://alemiaschi.github.io/news/bright_2023/</link><pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/bright_2023/</guid><description>&lt;p&gt;Friday September 29 at the Bright Night, we presented the activity of our laboratory. You can find a brief interview about the lab and our activities at the following link: &lt;a href='https://www.cnrweb.tv/il-bright-del-cnr-in-centro-citta-a-pisa/' target='_blank'&gt;&lt;a href="https://www.cnrweb.tv/il-bright-del-cnr-in-centro-citta-a-pisa/" target="_blank"&gt;https://www.cnrweb.tv/il-bright-del-cnr-in-centro-citta-a-pisa/&lt;/a&gt;&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>CLiC-it 2023 Papers</title><link>https://alemiaschi.github.io/news/clic-it_2023/</link><pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/clic-it_2023/</guid><description>&lt;p&gt;Two papers accepted at CLiC-it 2023! In &amp;lsquo;Lost in Labels&amp;rsquo; (with Michele Papucci and Felice Dell&amp;rsquo;Orletta) we present an evaluation of the influence of label selection on the performance of a Sequence-to-Sequence Transformer model in a classification task. Our study investigates whether the choice of words used to represent classification categories affects the model&amp;rsquo;s performance, and if there exists a relationship between the model&amp;rsquo;s performance and the selected words. To achieve this, we fine-tuned an Italian T5 model on topic classification using various labels. Our results indicate that the different label choices can significantly impact the model&amp;rsquo;s performance. That being said, we did not find a clear answer on how these choices affect the model performances, highlighting the need for further research in optimizing label selection. &lt;br/&gt;
In &amp;lsquo;Unmasking the Wordsmith: Revealing Author Identity through Reader Reviews&amp;rsquo; (with Chiara Alzetta, Felice Dell&amp;rsquo;Orletta, Chiara Fazzone and Giulia Venturi) we propose a novel task called Book Author Prediction, where we predict the author of a book based on user-generated reviews&amp;rsquo; writing style. To this aim, we first introduce the Literary Voices Corpus (LVC), a dataset of Italian book reviews, and use it to train and test machine learning models. Our study contributes valuable insights for developing user-centric systems that recommend leisure readings based on individual readers&amp;rsquo; interests and writing styles.&lt;/p&gt;</description></item><item><title>Journal of Documentation 2023</title><link>https://alemiaschi.github.io/news/documentation/</link><pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/documentation/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Tell me how you write and I&amp;rsquo;ll tell you what you read: a study on the writing style of book reviews&amp;rsquo; (with Chiara Alzetta, Felice Dell&amp;rsquo;Orletta, Elena Prat and Giulia Venturi) has been accepted for publication in the next issue of the journal of Documentation. In this work we investigate variations in the writing style of book reviews published on different social reading platforms and referring to books of different genres. In particular, we propose a corpus-based study focused on the analysis of A Good Review, a novel corpus of online book reviews written in Italian, posted on Amazon and Goodreads, and covering six literary fiction genres. We rely on stylometric analysis to explore the linguistic properties and lexicon of reviews and we conducted automatic classification experiments using multiple approaches and feature configurations to predict either the review&amp;rsquo;s platform or the literary genre.&lt;/p&gt;</description></item><item><title>Talk at DCP23</title><link>https://alemiaschi.github.io/news/dcp_2023/</link><pubDate>Mon, 05 Jun 2023 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/dcp_2023/</guid><description>&lt;p&gt;I am glad to announce that on Friday, June 9th, I will give a talk at the DCP23 Workshop in Pisa. DCP is an inter-disciplinary workshop focused on non-linear dynamics, statistical mechanics and complexity in multiple areas, from mathematics to philosophy, biology, physiology, economy and social sciences, among others.&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Title&lt;/b&gt;: Opening Large Language Models&lt;/h3&gt;&lt;br/&gt;
&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: As language models become increasingly complex and sophisticated, the processes leading to their predictions are growing increasingly difficult to understand. Research in NLP interpretability focuses on explaining the rationales driving model predictions and is crucial for building trust and transparency in the usage of these systems in real-world scenarios. In this talk, we will first introduce state-of-the-art Neural Language Models (NLMs) and discuss their characteristics. Then we will cover the most commonly applied analysis methods for understanding the inner behaviour of NLMs based on Transformer architectures and how they implicitly encode linguistic knowledge.&lt;/p&gt;</description></item><item><title>Lab @ Lectures on Computational Linguistics 2023</title><link>https://alemiaschi.github.io/news/lectures_2023/</link><pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/lectures_2023/</guid><description>&lt;p&gt;I am glad to announce that on May 31 I will be hosting with &lt;a href="https://gsarti.com/"&gt;Gabriele Sarti&lt;/a&gt; a laboratory focused on the interpretability of Neural Language Models (NLMs) at the 2023 edition of the Lectures on Computational Linguistics. &lt;br/&gt;Below you can find title and abstract of the lab:
&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Title&lt;/b&gt;: Explaining Neural Language Models from Internal Representations to Model Predictions&lt;/h3&gt;&lt;br/&gt;
&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: As language models become increasingly complex and sophisticated, the processes leading to their predictions are growing increasingly difficult to understand. Research in NLP interpretability focuses on explaining the rationales driving model predictions and is crucial for building trust and transparency in the usage of these systems in real-world scenarios.&lt;/p&gt;</description></item><item><title>Information 2023, Volume 14, Number 3</title><link>https://alemiaschi.github.io/news/information/</link><pubDate>Wed, 22 Feb 2023 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/information/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Testing the Effectiveness of the Diagnostic Probing Paradigm on Italian Treebankss&amp;rsquo; (with Chiara Alzetta, Dominique Brunato, Felice Dell&amp;rsquo;Orletta and Giulia Venturi) has been accepted for publication in the next issue of the Information journal. In this work we contribute to the debate on the effectiveness of the linguistic probing paradigm by presenting an approach to assessing the effectiveness of a suite of probing tasks aimed at testing the linguistic knowledge implicitly encoded by one of the most prominent NLMs, BERT. To this aim, we compared the performance of probes when predicting gold and automatically altered values of a set of linguistic features. Our experiments were performed on Italian and were evaluated across BERT’s layers and for sentences with different lengths. As a general result, we observed higher performance in the prediction of gold values, thus suggesting that the probing model is sensitive to the distortion of feature values. However, our experiments also showed that the length of a sentence is a highly influential factor that is able to confound the probing model’s predictions.&lt;/p&gt;</description></item><item><title>LangLearn (Shared task at EVALITA 2023)</title><link>https://alemiaschi.github.io/news/evalita_2022/</link><pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/evalita_2022/</guid><description>&lt;p&gt;I am happy to announce that I will be co-organizing a shared task at EVALITA 2023, the evaluation campaign of NLP and Speech Tools for Italian, that will have place in Parma on September 7-8 2023. &lt;br/&gt;
For more information please visit the shared task web page:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href='https://sites.google.com/view/langlearn2023'&gt;LangLearn&lt;/a&gt;: Language Learning Development at EVALITA 2023&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>IEEE/ACM Transactions on Audio, Speech and Language Processing</title><link>https://alemiaschi.github.io/news/taslp/</link><pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/taslp/</guid><description>&lt;p&gt;Our paper &amp;lsquo;On Robustness and Sensitivity of a Neural Language Model: A Case Study on Italian L1 Learner Errors&amp;rsquo; (with Dominique Brunato, Felice Dell&amp;rsquo;Orletta and Giulia Venturi) has been accepted for publication in the next issue of the IEEE/ACM Transactions on Audio, Speech and Language Processing journal. In this work, we propose a comprehensive linguistic study aimed at assessing the implicit behaviour of one of the most prominent Neural Language Model (NLM) based on Transformer architectures, BERT (Devlin et al., 2019), when dealing with a particular source of noisy data, namely essays written by L1 Italian learners containing a variety of errors targeting grammar, orthography and lexicon. Differently from previous works, we focus on the pre-training stage and we devise two evaluation tasks aimed at assessing the impact of errors on sentence-level inner representations from two complementary perspectives, i.e. robustness and sensitivity. Our experiments show that BERT’s ability to compute sentence similarity and to correctly encode a set of raw and morpho-syntactic properties of a sentence are differently modulated by the category of errors and that the error hierarchies in terms of robustness and sensitivity change across layer-wise representations.&lt;/p&gt;</description></item><item><title>NL4AI 2022 (AIxIA) Paper</title><link>https://alemiaschi.github.io/news/nl4ai_2022/</link><pubDate>Wed, 09 Nov 2022 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/nl4ai_2022/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Evaluating Text-To-Text Framework for Topic and Style Classification of Italian texts&amp;rsquo; (with Michele Papucci, Chiara De Nigris and Felice Dell&amp;rsquo;Orletta) has been accepted at the NL4AI Workshop (AIxIA Conference). In this paper, we propose an extensive evaluation of the first text-to-text Italian Neural Language Model (NLM), IT5, on a classification scenario. In particular, we test the performance of IT5 on several tasks involving both the classification of the topic and the style of a set of Italian posts. We assess the model in two different configurations, single- and multi-task classification, and we compare it with a more traditional NLM based on the Transformer architecture (i.e. BERT). Moreover, we test its performance in a few-shot learning scenario. We also perform a qualitative investigation on the impact of label representations in modeling the classification of the IT5 model. Results show that IT5 could achieve good results, although generally lower than the BERT model. Nevertheless, we observe a significant performance improvement of the Text-to-text model in a multi-task classification scenario. Finally, we found that altering the representation of the labels mainly impacts the classification of the topic.&lt;/p&gt;</description></item><item><title>Tech Talk</title><link>https://alemiaschi.github.io/news/pi_school_2022/</link><pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/pi_school_2022/</guid><description>&lt;p&gt;I am glad to announce that on October 27 I will give a
tech talk at &lt;a href="https://picampus-school.com/"&gt;Pi School&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Title&lt;/b&gt;: Interpreting Neural Language Models&lt;/h3&gt;&lt;br/&gt;
&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: The field of Natural Language Processing (NLP) has seen an
unprecedented progress in the last few years. Much of this progress is
due to the replacement of traditional systems with newer and more
powerful algorithms based on neural networks and deep learning. This
improvement, however, comes at the cost of interpretability, since deep
neural models offer little transparency about their inner workings and
their abilities. Therefore, in the last few years, an increasingly large
body of work has been devoted to the analysis and interpretation of
these models.
This talk will be divided into two parts. In the first part, we will
briefly introduce Neural Language Models (NLMs) and the main techniques
developed for interpreting their decisions and their inner linguistic
knowledge. In the second part, we will see how to fine-tune one of the
most popular NLM and then analyze its decisions according to two
different interpretability methods: integrated gradients and analysis of
attention matrices.&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;</description></item><item><title>Summer School - Advances in AI</title><link>https://alemiaschi.github.io/news/summer_school_ai/</link><pubDate>Wed, 03 Aug 2022 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/summer_school_ai/</guid><description>&lt;p&gt;I am glad to announce that on September 21 I will give a
talk at the International Summer School on &amp;ldquo;Advances in Artificial
Intelligence&amp;rdquo; (see below for the details). The main purpose of the school is to gather scholars,
researchers and PhD students to learn and explore the main advanced
topics offered by AI with a wide look towards new perspectives coming
by innovative technological scenarios. &lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Title&lt;/b&gt;: Profiling Neural Language Models&lt;/h3&gt;&lt;br/&gt;
&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: The field of Natural Language Processing (NLP) has
seen an unprecedented progress in the last years. Much of this progress
is due to the replacement of traditional systems with newer and more
powerful algorithms based on neural networks and deep learning. This
improvement, however, comes at the cost of interpretability, since deep
neural models offer little transparency about their inner workings and their abilities. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of these models.&lt;br/&gt;&lt;/p&gt;</description></item><item><title>PhD Thesis Defense</title><link>https://alemiaschi.github.io/news/phd_thesis/</link><pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/phd_thesis/</guid><description>&lt;p&gt;I am glad to announce that on May 24 I have successfully defended my PhD thesis, &lt;i&gt;&amp;lsquo;Tracking Linguistic Abilities in Neural Language Models&amp;rsquo;&lt;/i&gt;. &lt;br/&gt; You can find the pdf of my thesis at the following link: &lt;a href='https://etd.adm.unipi.it/theses/available/etd-05062022-162420/'&gt;&lt;a href="https://etd.adm.unipi.it/theses/available/etd-05062022-162420/" target="_blank"&gt;https://etd.adm.unipi.it/theses/available/etd-05062022-162420/&lt;/a&gt;&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>NL4AI 2021 (AIxIA) Paper</title><link>https://alemiaschi.github.io/news/nl4ai_2021/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/nl4ai_2021/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Evaluating Transformer Models for Punctuation Restoration in Italian&amp;rsquo; (with Andrea Amelio Ravelli and Felice Dell&amp;rsquo;Orletta) has been accepted at the NL4AI Workshop (AIxIA Conference). In this paper, we propose an evaluation of a Transformer-based punctuation restoration model for the Italian language. Experimenting with a BERT-base model, we perform several fine-tuning with different training data and sizes and tested them in an in- and cross-domain scenario. Moreover, we offer a comparison in a multilingual setting with the same model fine-tuned on English transcriptions. Finally, we conclude with an error analysis of the main weaknesses of the model related to specific punctuation marks.&lt;/p&gt;</description></item><item><title>CLiC-it 2021 Papers</title><link>https://alemiaschi.github.io/news/clic-it_2021/</link><pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/clic-it_2021/</guid><description>&lt;p&gt;Two papers accepted at CLiC-it 2021! In &amp;lsquo;Probing Tasks Under Stress&amp;rsquo; (with Chiara Alzetta, Dominique Brunato, Felice Dell&amp;rsquo;Orletta and Giulia Venturi) we introduced a new approach to put increasingly under pressure the effectiveness of a suite of probing tasks to test the linguistic knowledge implicitly encoded by a BERT Italian model. To achieve this goal, we set up a number of experiments aimed at comparing the performance of a regression model trained with BERT representations to predict the values of a set of linguistic properties extracted from the Italian Universal Dependency Treebank and from a suite of control datasets we specifically built for the purpose of this study.&lt;br/&gt;
In &amp;lsquo;On the role of Textual Connectives in Sentence Comprehension: a new Dataset for Italian&amp;rsquo; (with Giorgia Albertin, Alessio Miaschi and Dominique Brunato) we presented a new evaluation resource for Italian aimed at assessing the role of textual connectives in the comprehension of the meaning of a sentence. The resource is arranged in two sections, corresponding to a distinct challenge task conceived to test how subtle modifications involving connectives in real usage sentences influence the perceived acceptability of the sentence by native speakers and Neural Language Models (NLMs). Although the main focus is the presentation of the dataset, we also provided some preliminary data comparing human judgments and NLMs performance in the two tasks.&lt;/p&gt;</description></item><item><title>Science Web Festival Workshop</title><link>https://alemiaschi.github.io/news/science_web_festival_2021/</link><pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/science_web_festival_2021/</guid><description>&lt;p&gt;Last saturday at the &lt;a href='https://www.sciencewebfestival.it/' target='_blank'&gt;Science Web Festival&lt;/a&gt; we presented our educational workshop &lt;i&gt;&amp;lsquo;Ehi Siri, che cos&amp;rsquo;è la Linguistica Computazionale?&amp;rsquo;&lt;/i&gt; organized in collaboration with &lt;a href='https://www.ai-lc.it/' target='_blank'&gt;AILC (Associazione Italiana di Linguistica Computazionale)&lt;/a&gt;. You can find the video of the presentation (in Italian) at the following link: &lt;a href='https://www.youtube.com/watch?v=HGTpAXXRkWA' target='_blank'&gt;&lt;a href="https://www.youtube.com/watch?v=HGTpAXXRkWA" target="_blank"&gt;https://www.youtube.com/watch?v=HGTpAXXRkWA&lt;/a&gt;&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>NAACL 2021 Workshop Papers</title><link>https://alemiaschi.github.io/news/workshops_naacl_2021/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/workshops_naacl_2021/</guid><description>&lt;p&gt;4 papers accepted at NAACL 2021 workshops!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;'What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity'&lt;/i&gt; (with Dominique Brunato, Felice Dell'Orletta and Giulia Venturi): We studied how the linguistic structure of a sentence affects the perplexity of BERT and GPT-2 models (accepted at &lt;a href='https://sites.google.com/view/deelio-ws/' target='_blank'&gt;DeeLIO 2021&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;i&gt;'How Do BERT Embeddings Organize Linguistic Knowledge?'&lt;/i&gt; (with Giovanni Puccetti e Felice Dell'Orletta): We proposed a study, based on Lasso regression, to understand how the information encoded by BERT sentence-level representations is arrange within its hidden units (accepted at &lt;a href='https://sites.google.com/view/deelio-ws/' target='_blank'&gt;DeeLIO 2021&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;i&gt;'Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students'&lt;/i&gt; (with Ludovica Pannitto, Lucia Busso, Claudia Roberta Combei, Lucio Messina, Gabriele Sarti e Malvina Nissim): We illustrated an interactive workshop designed to delinate the basic principles of NLP and computational linguistics to high school Italian students aged between 13 and 18 years (accepted at &lt;a href='https://sites.google.com/view/teaching-nlp-workshop/home?authuser=0' target='_blank'&gt;Teaching NLP 2021&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;i&gt;'A dissemination workshop for introducing young Italian students to NLP'&lt;/i&gt; (with Lucio Messina, Lucia Busso, Claudia Roberta Combei, Ludovica Pannitto, Gabriele Sarti e Malvina Nissim): We described and made available the game-based material developed for the laboratory mentioned in &lt;i&gt;'A dissemination workshop for introducing young Italian students to NLP'&lt;/i&gt; (accepted at &lt;a href='https://sites.google.com/view/teaching-nlp-workshop/home?authuser=0' target='_blank'&gt;Teaching NLP 2021&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Journal of Writing Research (JoWR) Paper</title><link>https://alemiaschi.github.io/news/jowr_paper/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/jowr_paper/</guid><description>&lt;p&gt;Our paper &lt;i&gt;&amp;lsquo;A NLP-based stylometric approach for tracking the evolution of L1 written language compentece&amp;rsquo;&lt;/i&gt; (with Dominique Brunato and Felice Dell&amp;rsquo;Orletta) is finally out and will feature in the next issue of JoWR! In this paper we demonstrated that linguistic features automatically extracted from text not only allow making explicit the relevant transformations occurring in L1 learners’ writing competence but can be exploited as effective predictors in the automatic classification of the chronological order of essays written by the same student, especially at more distant temporal spans. &lt;img src='1.png'&gt;&lt;br/&gt; We showed that features related to the error annotation, as well as features belonging to the use of grammatical categories and to the inflectional properties of verbs, acquire much more relevance as the temporal span increase. &lt;br/&gt; Finally, we found that the student&amp;rsquo;slearning curve varies according at least to the geographical area where the school is located: when a higher temporal span is considered, the classifier is more confident about its decision for texts written by students who belong to suburban schools. &lt;img src='2.png'&gt;&lt;/p&gt;</description></item><item><title>COLING 2020 Outstanding Paper Award</title><link>https://alemiaschi.github.io/news/coling_outstanding_2020/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/coling_outstanding_2020/</guid><description>&lt;p&gt;I am very proud to announce that our paper &amp;lsquo;Linguistic Profiling of a Neural Language Model&amp;rsquo; (with Dominique Brunato, Felice Dell&amp;rsquo;Orletta and Giulia Venturi) has been awarded as Outstanding Paper for COLING 2020! Here&amp;rsquo;s the official announcement: &lt;a href='https://coling2020.org/2020/11/29/outstanding-papers.html'&gt;&lt;a href="https://coling2020.org/2020/11/29/outstanding-papers.html" target="_blank"&gt;https://coling2020.org/2020/11/29/outstanding-papers.html&lt;/a&gt;&lt;/a&gt;&lt;br/&gt;
The paper will be presented on Tuesday 8 at 17:00 - 17:30 (CET).&lt;/p&gt;</description></item><item><title>CLiC-it 2020 Papers</title><link>https://alemiaschi.github.io/news/clic-it_2020/</link><pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/clic-it_2020/</guid><description>&lt;p&gt;Two papers accepted at CLiC-it 2020! In &amp;lsquo;Italian Transformers Under the Linguistic Lens&amp;rsquo; (with Gabriele Sarti, Dominique Brunato, Felice Dell&amp;rsquo;Orletta and Giulia Venturi) we present an in-depth investigation of the lingusitic knowledge encoded by the Transformer models currently available for the Italian language. In particular, we showed that Multilayer Perceptron is the best model for inferring the amount of information implicitly encoded in the Transformers representations. We also observed that BERT-base-italian achieved best scores in average, but the linguistic generalization abilities of the examined models vary according to specific groups of linguistic phenomena and according to distinct textual genres.&lt;br/&gt;
In &amp;lsquo;Is Neural Language Model Perplexity Related to Readability?&amp;rsquo; (with Chiara Alzetta, Dominique Brunato, Felice Dell&amp;rsquo;Orletta and Giulia Venturi) we explore the relationship between Neural Language Model (NLM) perplexity and (automatically assessed) sentence readability. Starting from the evidence that NLMs implicitly acquire sophisticated linguistic knowledge from a huge amount of training data, our goal is to investigate whether perplexity is affected by linguistic features used to automatically assess sentence readability and if there is a correlation between the two metrics. Our findings highlight that no significant correlation can be found, either between the two metrics and the set of linguistic features that mostly impact their values.&lt;/p&gt;</description></item><item><title>COLING 2020 Paper</title><link>https://alemiaschi.github.io/news/coling_2020/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/coling_2020/</guid><description>&lt;p&gt;Our paper &amp;lsquo;Linguistic Profiling of a Neural Language Model&amp;rsquo; (with Dominique Brunato, Felice Dell&amp;rsquo;Orletta and Giulia Venturi) has been accepted at COLING 2020! In this paper we investigate the linguistic knowledge learned by a Neural Language Model (BERT) before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds toa distinct sentence-level feature extracted from different levels of linguistic annotation. In particular, we showed that BERT encodes a wide range of linguistic properites, but the order in which they are stored in the internal representations does not necessarily reflect the traditional division with respect to the linguistic annotation levels. &lt;img src='1.png'&gt;&lt;br/&gt; We also found that BERT tends to lose its precision in encoding linguistic features after a fine-tuning process (Native Language Identification), probably because it is storing more task–related information for solving the task. &lt;img src='2.png'&gt;&lt;br/&gt; Finally, we showed that the implicit linguistic knowledge encoded by the NLM positively affects its ability to solve the tested downstream tasks. &lt;img src='3.png'&gt;&lt;/p&gt;</description></item><item><title>BEA-2020 Paper</title><link>https://alemiaschi.github.io/news/bea_2020/</link><pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/bea_2020/</guid><description>&lt;p&gt;Our paper &lt;i&gt;&amp;lsquo;Tracking the Evolution of Written Language Competence in L2 Spanish Learners&amp;rsquo;&lt;/i&gt; (with Sam Davidson, Dominique Brunato, Felice Dell&amp;rsquo;Orletta, Kenji Sagae, Claudia Helena Sanchez-Gutierrez and Giulia Venturi) has been accepted at BEA-2020! In this paper we presented an NLP-based approach for tracking the evolution of written language competence in L2 Spanish learners using a wide range of linguistic features automatically extracted from students&amp;rsquo; written productions. Beyond reporting classification results for different scenarios, we explored the connection between the most predictive features and the teaching curriculum, finding that our set of linguistic features often reflects the explicit instruction that students receive during each course.&lt;/p&gt;</description></item><item><title>RepL4NLP-2020 Paper</title><link>https://alemiaschi.github.io/news/repl4nlp_2020/</link><pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/repl4nlp_2020/</guid><description>&lt;p&gt;Our paper &lt;i&gt;&amp;lsquo;Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation&amp;rsquo;&lt;/i&gt; (with Felice Dell&amp;rsquo;Orletta) has been accepted at RepL4NLP-2020! In this paper we performed an in-depth linguistic analysis aimed at understanding the implicit knowledge encoded in a contextual and a contextual-independent Neural Language Model (NLM). In particular: we evaluated the best method for obtaining sentence-level representations out of single-word embeddings; we compared the results obtained by the two NLMs according to the different combining methods; we studied whether the contextualized model is able to encode sentence-level properties within its single word representations.&lt;/p&gt;</description></item><item><title>Shared tasks at EVALITA 2020</title><link>https://alemiaschi.github.io/news/evalita_2020/</link><pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/evalita_2020/</guid><description>&lt;p&gt;I am happy to announce that I will be co-organizing two shared tasks at EVALITA 2020, the evaluation campaign of NLP and Speech Tools for Italian, that will be held as a co-located event of the 7th Conference on Computational Linguistics - CLiC-it 2020 (Bologna, Nov 30th - Dec 3rd 2020). &lt;br/&gt;
For more information please visit the shared tasks web pages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href='https://sites.google.com/view/prelearn20/home'&gt;PRELEARN&lt;/a&gt;: Prerequisite RElation LEARNing&lt;/li&gt;
&lt;li&gt;&lt;a href='http://www.di.uniba.it/~swap/ate_absita/index.html'&gt;ATE_ABSITA&lt;/a&gt;: Aspect Term Extraction and Aspect-based Sentiment Analysis&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>PhD Giveback Event</title><link>https://alemiaschi.github.io/news/giveback_event/</link><pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate><guid>https://alemiaschi.github.io/news/giveback_event/</guid><description>&lt;p&gt;I am glad to announce the first PhD Giveback Event, which I organized together with Irene Sucameli and prof. Ferragina. It will be a great half-day event, during which former PhD students of our Department who now works for prestigious companies, will present themselves to the current PhD and MA students, talking about their post-doc experience as well as their current research activities within the respective companies. &lt;br/&gt;For more information please visit the &lt;a href='http://givebackevent.di.unipi.it'&gt;website&lt;/a&gt;.&lt;/p&gt;</description></item></channel></rss>