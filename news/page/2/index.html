<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>News | Alessio Miaschi</title><meta name=keywords content><meta name=description content="News - Alessio Miaschi"><meta name=author content="Alessio Miaschi"><link rel=canonical href=https://alemiaschi.github.io/news/><link crossorigin=anonymous href=/assets/css/stylesheet.ac30954b7ea20660ca1cb473400ecede9e601f17674446536180d3ca00f03328.css integrity="sha256-rDCVS36iBmDKHLRzQA7O3p5gHxdnREZTYYDTygDwMyg=" rel="preload stylesheet" as=style><link rel=icon href=https://alemiaschi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://alemiaschi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://alemiaschi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://alemiaschi.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://alemiaschi.github.io/news/index.xml><link rel=alternate hreflang=en href=https://alemiaschi.github.io/news/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://alemiaschi.github.io/news/"><meta property="og:site_name" content="Alessio Miaschi"><meta property="og:title" content="News"><meta property="og:description" content="This website hosts Alessio Miaschi research papers, news, courses, projects and contact information."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="News"><meta name=twitter:description content="This website hosts Alessio Miaschi research papers, news, courses, projects and contact information."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"News","item":"https://alemiaschi.github.io/news/"}]}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{equation*}",right:"\\end{equation*}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1})})</script></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://alemiaschi.github.io/ accesskey=h title="Alessio Miaschi"><img src=https://alemiaschi.github.io/logo.jpg alt aria-label=logo height=18 width=18>Alessio Miaschi</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://alemiaschi.github.io/news/ title=News><span class=active>News</span></a></li><li><a href=https://alemiaschi.github.io/papers/ title=Papers><span>Papers</span></a></li><li><a href=https://alemiaschi.github.io/courses/ title="Courses & Theses"><span>Courses & Theses</span></a></li><li><a href=https://alemiaschi.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://alemiaschi.github.io/projects/ title="Resources & Projects"><span>Resources & Projects</span></a></li></ul></nav></header><main class=main><header class=page-header><h1></h1></header><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/bright_2023/featured_hu_540b6edb68206120.png 360w,https://alemiaschi.github.io/news/bright_2023/featured_hu_43298927efd9bce2.png 480w,https://alemiaschi.github.io/news/bright_2023/featured_hu_8998a10b63365cfb.png 720w,https://alemiaschi.github.io/news/bright_2023/featured.png 856w' src=https://alemiaschi.github.io/news/bright_2023/featured.png sizes="(min-width: 768px) 720px, 100vw" width=856 height=463 alt></figure><header class=entry-header><h2 class=entry-hint-parent>Lab @ Bright Night 2023</h2></header><div class=entry-content><p>Friday September 29 at the Bright Night, we presented the activity of our laboratory. You can find a brief interview about the lab and our activities at the following link: https://www.cnrweb.tv/il-bright-del-cnr-in-centro-citta-a-pisa/.</p></div><footer class=entry-footer><span title='2023-10-13 00:00:00 +0000 UTC'>October 2023</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to Lab @ Bright Night 2023" href=https://alemiaschi.github.io/news/bright_2023/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/clic-it_2023/featured_hu_35f0b5aec26832ea.png 360w,https://alemiaschi.github.io/news/clic-it_2023/featured_hu_d70c582aa789324b.png 480w,https://alemiaschi.github.io/news/clic-it_2023/featured_hu_4067d6b54861a9af.png 720w,https://alemiaschi.github.io/news/clic-it_2023/featured.png 1003w' src=https://alemiaschi.github.io/news/clic-it_2023/featured.png sizes="(min-width: 768px) 720px, 100vw" width=1003 height=489 alt></figure><header class=entry-header><h2 class=entry-hint-parent>CLiC-it 2023 Papers</h2></header><div class=entry-content><p>Two papers accepted at CLiC-it 2023! In ‘Lost in Labels’ (with Michele Papucci and Felice Dell’Orletta) we present an evaluation of the influence of label selection on the performance of a Sequence-to-Sequence Transformer model in a classification task. Our study investigates whether the choice of words used to represent classification categories affects the model’s performance, and if there exists a relationship between the model’s performance and the selected words. To achieve this, we fine-tuned an Italian T5 model on topic classification using various labels. Our results indicate that the different label choices can significantly impact the model’s performance. That being said, we did not find a clear answer on how these choices affect the model performances, highlighting the need for further research in optimizing label selection. In ‘Unmasking the Wordsmith: Revealing Author Identity through Reader Reviews’ (with Chiara Alzetta, Felice Dell’Orletta, Chiara Fazzone and Giulia Venturi) we propose a novel task called Book Author Prediction, where we predict the author of a book based on user-generated reviews’ writing style. To this aim, we first introduce the Literary Voices Corpus (LVC), a dataset of Italian book reviews, and use it to train and test machine learning models. Our study contributes valuable insights for developing user-centric systems that recommend leisure readings based on individual readers’ interests and writing styles.</p></div><footer class=entry-footer><span title='2023-10-11 00:00:00 +0000 UTC'>October 2023</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to CLiC-it 2023 Papers" href=https://alemiaschi.github.io/news/clic-it_2023/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/documentation/featured_hu_d671acdba1cef158.png 360w,https://alemiaschi.github.io/news/documentation/featured_hu_1b8a890f3ea5d5ab.png 480w,https://alemiaschi.github.io/news/documentation/featured_hu_1d093846df7f344b.png 720w,https://alemiaschi.github.io/news/documentation/featured.png 830w' src=https://alemiaschi.github.io/news/documentation/featured.png sizes="(min-width: 768px) 720px, 100vw" width=830 height=360 alt></figure><header class=entry-header><h2 class=entry-hint-parent>Journal of Documentation 2023</h2></header><div class=entry-content><p>Our paper ‘Tell me how you write and I’ll tell you what you read: a study on the writing style of book reviews’ (with Chiara Alzetta, Felice Dell’Orletta, Elena Prat and Giulia Venturi) has been accepted for publication in the next issue of the journal of Documentation. In this work we investigate variations in the writing style of book reviews published on different social reading platforms and referring to books of different genres. In particular, we propose a corpus-based study focused on the analysis of A Good Review, a novel corpus of online book reviews written in Italian, posted on Amazon and Goodreads, and covering six literary fiction genres. We rely on stylometric analysis to explore the linguistic properties and lexicon of reviews and we conducted automatic classification experiments using multiple approaches and feature configurations to predict either the review’s platform or the literary genre.</p></div><footer class=entry-footer><span title='2023-06-22 00:00:00 +0000 UTC'>June 2023</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to Journal of Documentation 2023" href=https://alemiaschi.github.io/news/documentation/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/dcp_2023/featured_hu_a2fdd82b32ca8741.png 360w,https://alemiaschi.github.io/news/dcp_2023/featured_hu_c31b811f450f3b04.png 480w,https://alemiaschi.github.io/news/dcp_2023/featured_hu_44f2e0d71d7d6bea.png 720w,https://alemiaschi.github.io/news/dcp_2023/featured_hu_b412d05bc70a266c.png 1080w,https://alemiaschi.github.io/news/dcp_2023/featured.png 1455w' src=https://alemiaschi.github.io/news/dcp_2023/featured.png sizes="(min-width: 768px) 720px, 100vw" width=1455 height=845 alt></figure><header class=entry-header><h2 class=entry-hint-parent>Talk at DCP23</h2></header><div class=entry-content><p>I am glad to announce that on Friday, June 9th, I will give a talk at the DCP23 Workshop in Pisa. DCP is an inter-disciplinary workshop focused on non-linear dynamics, statistical mechanics and complexity in multiple areas, from mathematics to philosophy, biology, physiology, economy and social sciences, among others.
Title: Opening Large Language Models Abstract: As language models become increasingly complex and sophisticated, the processes leading to their predictions are growing increasingly difficult to understand. Research in NLP interpretability focuses on explaining the rationales driving model predictions and is crucial for building trust and transparency in the usage of these systems in real-world scenarios. In this talk, we will first introduce state-of-the-art Neural Language Models (NLMs) and discuss their characteristics. Then we will cover the most commonly applied analysis methods for understanding the inner behaviour of NLMs based on Transformer architectures and how they implicitly encode linguistic knowledge.</p></div><footer class=entry-footer><span title='2023-06-09 00:00:00 +0000 UTC'>June 2023</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to Talk at DCP23" href=https://alemiaschi.github.io/news/dcp_2023/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/lectures_2023/featured_hu_51a4b5c7381a17e1.png 360w,https://alemiaschi.github.io/news/lectures_2023/featured_hu_290ef252028543d0.png 480w,https://alemiaschi.github.io/news/lectures_2023/featured_hu_7dca35ce40eda8dd.png 720w,https://alemiaschi.github.io/news/lectures_2023/featured.png 835w' src=https://alemiaschi.github.io/news/lectures_2023/featured.png sizes="(min-width: 768px) 720px, 100vw" width=835 height=626 alt></figure><header class=entry-header><h2 class=entry-hint-parent>Lab @ Lectures on Computational Linguistics 2023</h2></header><div class=entry-content><p>I am glad to announce that on May 31 I will be hosting with Gabriele Sarti a laboratory focused on the interpretability of Neural Language Models (NLMs) at the 2023 edition of the Lectures on Computational Linguistics. Below you can find title and abstract of the lab: Title: Explaining Neural Language Models from Internal Representations to Model Predictions Abstract: As language models become increasingly complex and sophisticated, the processes leading to their predictions are growing increasingly difficult to understand. Research in NLP interpretability focuses on explaining the rationales driving model predictions and is crucial for building trust and transparency in the usage of these systems in real-world scenarios.
...</p></div><footer class=entry-footer><span title='2023-05-23 00:00:00 +0000 UTC'>May 2023</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to Lab @ Lectures on Computational Linguistics 2023" href=https://alemiaschi.github.io/news/lectures_2023/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/information/featured_hu_4e3fba773731b03c.png 360w,https://alemiaschi.github.io/news/information/featured_hu_af0aa12147ef37b5.png 480w,https://alemiaschi.github.io/news/information/featured.png 532w' src=https://alemiaschi.github.io/news/information/featured.png sizes="(min-width: 768px) 720px, 100vw" width=532 height=140 alt></figure><header class=entry-header><h2 class=entry-hint-parent>Information 2023, Volume 14, Number 3</h2></header><div class=entry-content><p>Our paper ‘Testing the Effectiveness of the Diagnostic Probing Paradigm on Italian Treebankss’ (with Chiara Alzetta, Dominique Brunato, Felice Dell’Orletta and Giulia Venturi) has been accepted for publication in the next issue of the Information journal. In this work we contribute to the debate on the effectiveness of the linguistic probing paradigm by presenting an approach to assessing the effectiveness of a suite of probing tasks aimed at testing the linguistic knowledge implicitly encoded by one of the most prominent NLMs, BERT. To this aim, we compared the performance of probes when predicting gold and automatically altered values of a set of linguistic features. Our experiments were performed on Italian and were evaluated across BERT’s layers and for sentences with different lengths. As a general result, we observed higher performance in the prediction of gold values, thus suggesting that the probing model is sensitive to the distortion of feature values. However, our experiments also showed that the length of a sentence is a highly influential factor that is able to confound the probing model’s predictions.</p></div><footer class=entry-footer><span title='2023-02-22 00:00:00 +0000 UTC'>February 2023</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to Information 2023, Volume 14, Number 3" href=https://alemiaschi.github.io/news/information/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/evalita_2022/featured_hu_c40aa4d0d2a6b972.png 360w,https://alemiaschi.github.io/news/evalita_2022/featured_hu_62d89d55c6ad63dd.png 480w,https://alemiaschi.github.io/news/evalita_2022/featured_hu_95c5b366867681a7.png 720w,https://alemiaschi.github.io/news/evalita_2022/featured.png 871w' src=https://alemiaschi.github.io/news/evalita_2022/featured.png sizes="(min-width: 768px) 720px, 100vw" width=871 height=170 alt></figure><header class=entry-header><h2 class=entry-hint-parent>LangLearn (Shared task at EVALITA 2023)</h2></header><div class=entry-content><p>I am happy to announce that I will be co-organizing a shared task at EVALITA 2023, the evaluation campaign of NLP and Speech Tools for Italian, that will have place in Parma on September 7-8 2023. For more information please visit the shared task web page:
LangLearn: Language Learning Development at EVALITA 2023</p></div><footer class=entry-footer><span title='2022-12-14 13:00:00 +0000 UTC'>December 2022</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to LangLearn (Shared task at EVALITA 2023)" href=https://alemiaschi.github.io/news/evalita_2022/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/taslp/featured_hu_6b2f4168c6dc50b.png 360w,https://alemiaschi.github.io/news/taslp/featured_hu_702e0a1de4c7f062.png 480w,https://alemiaschi.github.io/news/taslp/featured.png 620w' src=https://alemiaschi.github.io/news/taslp/featured.png sizes="(min-width: 768px) 720px, 100vw" width=620 height=145 alt></figure><header class=entry-header><h2 class=entry-hint-parent>IEEE/ACM Transactions on Audio, Speech and Language Processing</h2></header><div class=entry-content><p>Our paper ‘On Robustness and Sensitivity of a Neural Language Model: A Case Study on Italian L1 Learner Errors’ (with Dominique Brunato, Felice Dell’Orletta and Giulia Venturi) has been accepted for publication in the next issue of the IEEE/ACM Transactions on Audio, Speech and Language Processing journal. In this work, we propose a comprehensive linguistic study aimed at assessing the implicit behaviour of one of the most prominent Neural Language Model (NLM) based on Transformer architectures, BERT (Devlin et al., 2019), when dealing with a particular source of noisy data, namely essays written by L1 Italian learners containing a variety of errors targeting grammar, orthography and lexicon. Differently from previous works, we focus on the pre-training stage and we devise two evaluation tasks aimed at assessing the impact of errors on sentence-level inner representations from two complementary perspectives, i.e. robustness and sensitivity. Our experiments show that BERT’s ability to compute sentence similarity and to correctly encode a set of raw and morpho-syntactic properties of a sentence are differently modulated by the category of errors and that the error hierarchies in terms of robustness and sensitivity change across layer-wise representations.</p></div><footer class=entry-footer><span title='2022-12-02 00:00:00 +0000 UTC'>December 2022</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to IEEE/ACM Transactions on Audio, Speech and Language Processing" href=https://alemiaschi.github.io/news/taslp/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/nl4ai_2022/featured_hu_a2a290e77054f432.png 360w,https://alemiaschi.github.io/news/nl4ai_2022/featured_hu_49e84baef31657cc.png 480w,https://alemiaschi.github.io/news/nl4ai_2022/featured.png 678w' src=https://alemiaschi.github.io/news/nl4ai_2022/featured.png sizes="(min-width: 768px) 720px, 100vw" width=678 height=360 alt></figure><header class=entry-header><h2 class=entry-hint-parent>NL4AI 2022 (AIxIA) Paper</h2></header><div class=entry-content><p>Our paper ‘Evaluating Text-To-Text Framework for Topic and Style Classification of Italian texts’ (with Michele Papucci, Chiara De Nigris and Felice Dell’Orletta) has been accepted at the NL4AI Workshop (AIxIA Conference). In this paper, we propose an extensive evaluation of the first text-to-text Italian Neural Language Model (NLM), IT5, on a classification scenario. In particular, we test the performance of IT5 on several tasks involving both the classification of the topic and the style of a set of Italian posts. We assess the model in two different configurations, single- and multi-task classification, and we compare it with a more traditional NLM based on the Transformer architecture (i.e. BERT). Moreover, we test its performance in a few-shot learning scenario. We also perform a qualitative investigation on the impact of label representations in modeling the classification of the IT5 model. Results show that IT5 could achieve good results, although generally lower than the BERT model. Nevertheless, we observe a significant performance improvement of the Text-to-text model in a multi-task classification scenario. Finally, we found that altering the representation of the labels mainly impacts the classification of the topic.</p></div><footer class=entry-footer><span title='2022-11-30 00:00:00 +0000 UTC'>November 2022</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to NL4AI 2022 (AIxIA) Paper" href=https://alemiaschi.github.io/news/nl4ai_2022/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/pi_school_2022/featured_hu_ddd78c6c97ac03ea.png 360w,https://alemiaschi.github.io/news/pi_school_2022/featured_hu_e1bc8789d97156d7.png 480w,https://alemiaschi.github.io/news/pi_school_2022/featured.png 514w' src=https://alemiaschi.github.io/news/pi_school_2022/featured.png sizes="(min-width: 768px) 720px, 100vw" width=514 height=134 alt></figure><header class=entry-header><h2 class=entry-hint-parent>Tech Talk</h2></header><div class=entry-content><p>I am glad to announce that on October 27 I will give a tech talk at Pi School.
Title: Interpreting Neural Language Models Abstract: The field of Natural Language Processing (NLP) has seen an unprecedented progress in the last few years. Much of this progress is due to the replacement of traditional systems with newer and more powerful algorithms based on neural networks and deep learning. This improvement, however, comes at the cost of interpretability, since deep neural models offer little transparency about their inner workings and their abilities. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of these models. This talk will be divided into two parts. In the first part, we will briefly introduce Neural Language Models (NLMs) and the main techniques developed for interpreting their decisions and their inner linguistic knowledge. In the second part, we will see how to fine-tune one of the most popular NLM and then analyze its decisions according to two different interpretability methods: integrated gradients and analysis of attention matrices.
...</p></div><footer class=entry-footer><span title='2022-10-27 00:00:00 +0000 UTC'>October 2022</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to Tech Talk" href=https://alemiaschi.github.io/news/pi_school_2022/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/summer_school_ai/featured_hu_4a730afaba19b998.png 360w,https://alemiaschi.github.io/news/summer_school_ai/featured_hu_7deaba7c0cff40b0.png 480w,https://alemiaschi.github.io/news/summer_school_ai/featured_hu_fde5e32c79532211.png 720w,https://alemiaschi.github.io/news/summer_school_ai/featured_hu_eae39f748f45f8b.png 1080w,https://alemiaschi.github.io/news/summer_school_ai/featured.png 1088w' src=https://alemiaschi.github.io/news/summer_school_ai/featured.png sizes="(min-width: 768px) 720px, 100vw" width=1088 height=572 alt></figure><header class=entry-header><h2 class=entry-hint-parent>Summer School - Advances in AI</h2></header><div class=entry-content><p>I am glad to announce that on September 21 I will give a talk at the International Summer School on “Advances in Artificial Intelligence” (see below for the details). The main purpose of the school is to gather scholars, researchers and PhD students to learn and explore the main advanced topics offered by AI with a wide look towards new perspectives coming by innovative technological scenarios. Title: Profiling Neural Language Models Abstract: The field of Natural Language Processing (NLP) has seen an unprecedented progress in the last years. Much of this progress is due to the replacement of traditional systems with newer and more powerful algorithms based on neural networks and deep learning. This improvement, however, comes at the cost of interpretability, since deep neural models offer little transparency about their inner workings and their abilities. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of these models.
...</p></div><footer class=entry-footer><span title='2022-09-21 00:00:00 +0000 UTC'>September 2022</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to Summer School - Advances in AI" href=https://alemiaschi.github.io/news/summer_school_ai/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/phd_thesis/featured_hu_472bc243e8c82a0b.png 360w,https://alemiaschi.github.io/news/phd_thesis/featured_hu_545e625154166c5d.png 480w,https://alemiaschi.github.io/news/phd_thesis/featured.png 600w' src=https://alemiaschi.github.io/news/phd_thesis/featured.png sizes="(min-width: 768px) 720px, 100vw" width=600 height=314 alt></figure><header class=entry-header><h2 class=entry-hint-parent>PhD Thesis Defense</h2></header><div class=entry-content><p>I am glad to announce that on May 24 I have successfully defended my PhD thesis, ‘Tracking Linguistic Abilities in Neural Language Models’. You can find the pdf of my thesis at the following link: https://etd.adm.unipi.it/theses/available/etd-05062022-162420/.</p></div><footer class=entry-footer><span title='2022-05-24 00:00:00 +0000 UTC'>May 2022</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to PhD Thesis Defense" href=https://alemiaschi.github.io/news/phd_thesis/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/nl4ai_2021/featured_hu_250d50a20ca84c9.png 360w,https://alemiaschi.github.io/news/nl4ai_2021/featured_hu_c626df02180aece2.png 480w,https://alemiaschi.github.io/news/nl4ai_2021/featured_hu_655812cacb875f27.png 720w,https://alemiaschi.github.io/news/nl4ai_2021/featured.png 937w' src=https://alemiaschi.github.io/news/nl4ai_2021/featured.png sizes="(min-width: 768px) 720px, 100vw" width=937 height=426 alt></figure><header class=entry-header><h2 class=entry-hint-parent>NL4AI 2021 (AIxIA) Paper</h2></header><div class=entry-content><p>Our paper ‘Evaluating Transformer Models for Punctuation Restoration in Italian’ (with Andrea Amelio Ravelli and Felice Dell’Orletta) has been accepted at the NL4AI Workshop (AIxIA Conference). In this paper, we propose an evaluation of a Transformer-based punctuation restoration model for the Italian language. Experimenting with a BERT-base model, we perform several fine-tuning with different training data and sizes and tested them in an in- and cross-domain scenario. Moreover, we offer a comparison in a multilingual setting with the same model fine-tuned on English transcriptions. Finally, we conclude with an error analysis of the main weaknesses of the model related to specific punctuation marks.</p></div><footer class=entry-footer><span title='2021-11-12 00:00:00 +0000 UTC'>November 2021</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to NL4AI 2021 (AIxIA) Paper" href=https://alemiaschi.github.io/news/nl4ai_2021/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/clic-it_2021/featured_hu_c2620e20c87d9f1.png 360w,https://alemiaschi.github.io/news/clic-it_2021/featured_hu_de8e7fa24607b5d0.png 480w,https://alemiaschi.github.io/news/clic-it_2021/featured_hu_f3117bbcbf241db6.png 720w,https://alemiaschi.github.io/news/clic-it_2021/featured_hu_88b77723c8788c35.png 1080w,https://alemiaschi.github.io/news/clic-it_2021/featured.png 1282w' src=https://alemiaschi.github.io/news/clic-it_2021/featured.png sizes="(min-width: 768px) 720px, 100vw" width=1282 height=395 alt></figure><header class=entry-header><h2 class=entry-hint-parent>CLiC-it 2021 Papers</h2></header><div class=entry-content><p>Two papers accepted at CLiC-it 2021! In ‘Probing Tasks Under Stress’ (with Chiara Alzetta, Dominique Brunato, Felice Dell’Orletta and Giulia Venturi) we introduced a new approach to put increasingly under pressure the effectiveness of a suite of probing tasks to test the linguistic knowledge implicitly encoded by a BERT Italian model. To achieve this goal, we set up a number of experiments aimed at comparing the performance of a regression model trained with BERT representations to predict the values of a set of linguistic properties extracted from the Italian Universal Dependency Treebank and from a suite of control datasets we specifically built for the purpose of this study. In ‘On the role of Textual Connectives in Sentence Comprehension: a new Dataset for Italian’ (with Giorgia Albertin, Alessio Miaschi and Dominique Brunato) we presented a new evaluation resource for Italian aimed at assessing the role of textual connectives in the comprehension of the meaning of a sentence. The resource is arranged in two sections, corresponding to a distinct challenge task conceived to test how subtle modifications involving connectives in real usage sentences influence the perceived acceptability of the sentence by native speakers and Neural Language Models (NLMs). Although the main focus is the presentation of the dataset, we also provided some preliminary data comparing human judgments and NLMs performance in the two tasks.</p></div><footer class=entry-footer><span title='2021-10-26 00:00:00 +0000 UTC'>October 2021</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to CLiC-it 2021 Papers" href=https://alemiaschi.github.io/news/clic-it_2021/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/science_web_festival_2021/featured_hu_11bd35ea34580e5d.png 360w,https://alemiaschi.github.io/news/science_web_festival_2021/featured_hu_4817f250d4c3643c.png 480w,https://alemiaschi.github.io/news/science_web_festival_2021/featured_hu_f49c9294ddac4c4b.png 720w,https://alemiaschi.github.io/news/science_web_festival_2021/featured.png 768w' src=https://alemiaschi.github.io/news/science_web_festival_2021/featured.png sizes="(min-width: 768px) 720px, 100vw" width=768 height=425 alt></figure><header class=entry-header><h2 class=entry-hint-parent>Science Web Festival Workshop</h2></header><div class=entry-content><p>Last saturday at the Science Web Festival we presented our educational workshop ‘Ehi Siri, che cos’è la Linguistica Computazionale?’ organized in collaboration with AILC (Associazione Italiana di Linguistica Computazionale). You can find the video of the presentation (in Italian) at the following link: https://www.youtube.com/watch?v=HGTpAXXRkWA.</p></div><footer class=entry-footer><span title='2021-04-19 00:00:00 +0000 UTC'>April 2021</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to Science Web Festival Workshop" href=https://alemiaschi.github.io/news/science_web_festival_2021/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/workshops_naacl_2021/featured_hu_1dfd469132d8193f.png 360w,https://alemiaschi.github.io/news/workshops_naacl_2021/featured_hu_569fcc2f889f5569.png 480w,https://alemiaschi.github.io/news/workshops_naacl_2021/featured.png 520w' src=https://alemiaschi.github.io/news/workshops_naacl_2021/featured.png sizes="(min-width: 768px) 720px, 100vw" width=520 height=212 alt></figure><header class=entry-header><h2 class=entry-hint-parent>NAACL 2021 Workshop Papers</h2></header><div class=entry-content><p>4 papers accepted at NAACL 2021 workshops!
'What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity' (with Dominique Brunato, Felice Dell'Orletta and Giulia Venturi): We studied how the linguistic structure of a sentence affects the perplexity of BERT and GPT-2 models (accepted at DeeLIO 2021). 'How Do BERT Embeddings Organize Linguistic Knowledge?' (with Giovanni Puccetti e Felice Dell'Orletta): We proposed a study, based on Lasso regression, to understand how the information encoded by BERT sentence-level representations is arrange within its hidden units (accepted at DeeLIO 2021). 'Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students' (with Ludovica Pannitto, Lucia Busso, Claudia Roberta Combei, Lucio Messina, Gabriele Sarti e Malvina Nissim): We illustrated an interactive workshop designed to delinate the basic principles of NLP and computational linguistics to high school Italian students aged between 13 and 18 years (accepted at Teaching NLP 2021). 'A dissemination workshop for introducing young Italian students to NLP' (with Lucio Messina, Lucia Busso, Claudia Roberta Combei, Ludovica Pannitto, Gabriele Sarti e Malvina Nissim): We described and made available the game-based material developed for the laboratory mentioned in 'A dissemination workshop for introducing young Italian students to NLP' (accepted at Teaching NLP 2021).</p></div><footer class=entry-footer><span title='2021-04-16 00:00:00 +0000 UTC'>April 2021</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to NAACL 2021 Workshop Papers" href=https://alemiaschi.github.io/news/workshops_naacl_2021/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/jowr_paper/featured_hu_fac8365ea49f860f.png 360w,https://alemiaschi.github.io/news/jowr_paper/featured_hu_69e2e787dfe61944.png 480w,https://alemiaschi.github.io/news/jowr_paper/featured.png 651w' src=https://alemiaschi.github.io/news/jowr_paper/featured.png sizes="(min-width: 768px) 720px, 100vw" width=651 height=166 alt></figure><header class=entry-header><h2 class=entry-hint-parent>Journal of Writing Research (JoWR) Paper</h2></header><div class=entry-content><p>Our paper ‘A NLP-based stylometric approach for tracking the evolution of L1 written language compentece’ (with Dominique Brunato and Felice Dell’Orletta) is finally out and will feature in the next issue of JoWR! In this paper we demonstrated that linguistic features automatically extracted from text not only allow making explicit the relevant transformations occurring in L1 learners’ writing competence but can be exploited as effective predictors in the automatic classification of the chronological order of essays written by the same student, especially at more distant temporal spans. We showed that features related to the error annotation, as well as features belonging to the use of grammatical categories and to the inflectional properties of verbs, acquire much more relevance as the temporal span increase. Finally, we found that the student’slearning curve varies according at least to the geographical area where the school is located: when a higher temporal span is considered, the classifier is more confident about its decision for texts written by students who belong to suburban schools.</p></div><footer class=entry-footer><span title='2021-04-13 00:00:00 +0000 UTC'>April 2021</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to Journal of Writing Research (JoWR) Paper" href=https://alemiaschi.github.io/news/jowr_paper/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/coling_outstanding_2020/featured_hu_a30478ab2c524472.png 360w,https://alemiaschi.github.io/news/coling_outstanding_2020/featured_hu_342edccafe74a9e5.png 480w,https://alemiaschi.github.io/news/coling_outstanding_2020/featured.png 670w' src=https://alemiaschi.github.io/news/coling_outstanding_2020/featured.png sizes="(min-width: 768px) 720px, 100vw" width=670 height=135 alt></figure><header class=entry-header><h2 class=entry-hint-parent>COLING 2020 Outstanding Paper Award</h2></header><div class=entry-content><p>I am very proud to announce that our paper ‘Linguistic Profiling of a Neural Language Model’ (with Dominique Brunato, Felice Dell’Orletta and Giulia Venturi) has been awarded as Outstanding Paper for COLING 2020! Here’s the official announcement: https://coling2020.org/2020/11/29/outstanding-papers.html The paper will be presented on Tuesday 8 at 17:00 - 17:30 (CET).</p></div><footer class=entry-footer><span title='2020-12-01 00:00:00 +0000 UTC'>December 2020</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to COLING 2020 Outstanding Paper Award" href=https://alemiaschi.github.io/news/coling_outstanding_2020/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/clic-it_2020/featured_hu_86ecfd3dd3de6d45.png 360w,https://alemiaschi.github.io/news/clic-it_2020/featured_hu_20d09f179991a40c.png 480w,https://alemiaschi.github.io/news/clic-it_2020/featured_hu_7d72ba8706714e6d.png 720w,https://alemiaschi.github.io/news/clic-it_2020/featured.png 848w' src=https://alemiaschi.github.io/news/clic-it_2020/featured.png sizes="(min-width: 768px) 720px, 100vw" width=848 height=249 alt></figure><header class=entry-header><h2 class=entry-hint-parent>CLiC-it 2020 Papers</h2></header><div class=entry-content><p>Two papers accepted at CLiC-it 2020! In ‘Italian Transformers Under the Linguistic Lens’ (with Gabriele Sarti, Dominique Brunato, Felice Dell’Orletta and Giulia Venturi) we present an in-depth investigation of the lingusitic knowledge encoded by the Transformer models currently available for the Italian language. In particular, we showed that Multilayer Perceptron is the best model for inferring the amount of information implicitly encoded in the Transformers representations. We also observed that BERT-base-italian achieved best scores in average, but the linguistic generalization abilities of the examined models vary according to specific groups of linguistic phenomena and according to distinct textual genres. In ‘Is Neural Language Model Perplexity Related to Readability?’ (with Chiara Alzetta, Dominique Brunato, Felice Dell’Orletta and Giulia Venturi) we explore the relationship between Neural Language Model (NLM) perplexity and (automatically assessed) sentence readability. Starting from the evidence that NLMs implicitly acquire sophisticated linguistic knowledge from a huge amount of training data, our goal is to investigate whether perplexity is affected by linguistic features used to automatically assess sentence readability and if there is a correlation between the two metrics. Our findings highlight that no significant correlation can be found, either between the two metrics and the set of linguistic features that mostly impact their values.</p></div><footer class=entry-footer><span title='2020-10-13 00:00:00 +0000 UTC'>October 2020</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to CLiC-it 2020 Papers" href=https://alemiaschi.github.io/news/clic-it_2020/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset='https://alemiaschi.github.io/news/coling_2020/featured_hu_a30478ab2c524472.png 360w,https://alemiaschi.github.io/news/coling_2020/featured_hu_342edccafe74a9e5.png 480w,https://alemiaschi.github.io/news/coling_2020/featured.png 670w' src=https://alemiaschi.github.io/news/coling_2020/featured.png sizes="(min-width: 768px) 720px, 100vw" width=670 height=135 alt></figure><header class=entry-header><h2 class=entry-hint-parent>COLING 2020 Paper</h2></header><div class=entry-content><p>Our paper ‘Linguistic Profiling of a Neural Language Model’ (with Dominique Brunato, Felice Dell’Orletta and Giulia Venturi) has been accepted at COLING 2020! In this paper we investigate the linguistic knowledge learned by a Neural Language Model (BERT) before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds toa distinct sentence-level feature extracted from different levels of linguistic annotation. In particular, we showed that BERT encodes a wide range of linguistic properites, but the order in which they are stored in the internal representations does not necessarily reflect the traditional division with respect to the linguistic annotation levels. We also found that BERT tends to lose its precision in encoding linguistic features after a fine-tuning process (Native Language Identification), probably because it is storing more task–related information for solving the task. Finally, we showed that the implicit linguistic knowledge encoded by the NLM positively affects its ability to solve the tested downstream tasks.</p></div><footer class=entry-footer><span title='2020-09-30 00:00:00 +0000 UTC'>September 2020</span>&nbsp;&#183;&nbsp;Alessio Miaschi</footer><a class=entry-link aria-label="post link to COLING 2020 Paper" href=https://alemiaschi.github.io/news/coling_2020/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://alemiaschi.github.io/news/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://alemiaschi.github.io/news/page/3/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 Alessio Miaschi</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>hugo</a>, <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>papermod</a>, &
<a href=https://github.com/pmichaillat/hugo-website/ rel=noopener target=_blank>hugo-website</a>.</span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script></body></html>