<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Alessio Miaschi</title>
    <link>https://alemiaschi.github.io/project/</link>
      <atom:link href="https://alemiaschi.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 26 May 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://alemiaschi.github.io/images/icon_hu17af01e179191f389923d1b802f60b03_242678_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://alemiaschi.github.io/project/</link>
    </image>
    
    <item>
      <title>LLMs Anatomy Course</title>
      <link>https://alemiaschi.github.io/project/llm_anatomy/</link>
      <pubDate>Mon, 26 May 2025 00:00:00 +0000</pubDate>
      <guid>https://alemiaschi.github.io/project/llm_anatomy/</guid>
      <description>&lt;p&gt;This is the repository for the LLMs Anatomy Course held by me and my colleague &lt;a href=&#39;https://michelepapucci.github.io/&#39;&gt;Michele Papucci&lt;/a&gt; and organized for the students of the Department of Ingegneria Civile e Industriale (University of Pisa).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LLM Profiling Data</title>
      <link>https://alemiaschi.github.io/project/llm_profiling/</link>
      <pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://alemiaschi.github.io/project/llm_profiling/</guid>
      <description>&lt;p&gt;This repository contains data associated with the EMNLP 2024 paper &lt;a href=&#39;https://aclanthology.org/2024.emnlp-main.166.pdf&#39;&gt;Evaluating Large Language Models via Linguistic Profiling&lt;/a&gt;.
&lt;br&gt;
&lt;b&gt;Abstract&lt;/b&gt;: Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models&#39; linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs&#39; sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling&#39; approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linguistically Informed T5</title>
      <link>https://alemiaschi.github.io/project/lit5/</link>
      <pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate>
      <guid>https://alemiaschi.github.io/project/lit5/</guid>
      <description>&lt;p&gt;This repository contains data and models associated to the LREC-COLING 2024 paper &lt;a href=&#39;https://aclanthology.org/2024.lrec-main.922.pdf&#39;&gt;&amp;ldquo;Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)&amp;quot;&lt;/a&gt;. &lt;br&gt;
&lt;b&gt;Abstract&lt;/b&gt;: In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LANGLEARN @ EVALITA 2023</title>
      <link>https://alemiaschi.github.io/project/langlearn/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://alemiaschi.github.io/project/langlearn/</guid>
      <description>&lt;p&gt;This is the official webpage of the LANGLEARN - Language Learning Development Shared Task organized at EVALITA 2023. LANGLEARN was proposed at EVALITA as the first shared task on automatic language development assessment. The task consists of predicting the relative order of two essays written by the same student.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>XNLM Lab</title>
      <link>https://alemiaschi.github.io/project/xnlm_lab/</link>
      <pubDate>Mon, 29 May 2023 00:00:00 +0000</pubDate>
      <guid>https://alemiaschi.github.io/project/xnlm_lab/</guid>
      <description>&lt;p&gt;This repository contains materials for the lab &amp;ldquo;Explaining Neural Language Models from Internal Representations to Model Predictions&amp;rdquo; by me and my colleague Gabriele Sarti at the AILC Lectures on Computational Linguistics 2023.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GPT-Dante</title>
      <link>https://alemiaschi.github.io/project/dante-gpt/</link>
      <pubDate>Wed, 27 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://alemiaschi.github.io/project/dante-gpt/</guid>
      <description>&lt;p&gt;This is the official repository for the graphical interface for text generation with the GePpeTto language model, specialized in texts in Dantean rhyme.
The interface allows the user to enter a query, to which the model responds in Dante‚Äôs writing style.
The interface and the GPT-Dante model were developed and used for the following outreach activities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Workshop ‚ÄúComputational Linguistics and Digital Philology in the Age of AI‚Äù (Bright Night, Pisa, September 29, 2023)&lt;/li&gt;
&lt;li&gt;Workshop ‚ÄúIs it Dante or not Dante? That is the question‚Äù (Festival della Scienza, Genoa, October 27‚Äì29, 2021)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Python for Beginners</title>
      <link>https://alemiaschi.github.io/project/python_classes/</link>
      <pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate>
      <guid>https://alemiaschi.github.io/project/python_classes/</guid>
      <description>&lt;p&gt;This repository contains the slides and the materials for the &amp;ldquo;Python for Beginners&amp;rdquo; course (Corsi Hands on: strumenti digitali per le DH). All materials are in Italian üáÆüáπ.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PRELEARN @ EVALITA 2020</title>
      <link>https://alemiaschi.github.io/project/prelearn/</link>
      <pubDate>Thu, 01 Jan 1970 01:33:40 +0100</pubDate>
      <guid>https://alemiaschi.github.io/project/prelearn/</guid>
      <description>&lt;p&gt;This is the official webpage of the PRELEARN - Prerequisite RElation LEARNing Shared Task organized at EVALITA 2020. PRELEARN (Prerequisite Relation Learning) is a shared task on concept prerequisite learning which consists of classifying prerequisite relations between pairs of concepts distinguishing between prerequisite pairs and non-prerequisite pairs. For the purposes of this task, prerequisite relation learning is proposed as a problem of binary classification between two distinct concepts (i.e. a concept pair).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ITA-PREREQ</title>
      <link>https://alemiaschi.github.io/project/ita-prereq/</link>
      <pubDate>Thu, 01 Jan 1970 01:33:39 +0100</pubDate>
      <guid>https://alemiaschi.github.io/project/ita-prereq/</guid>
      <description>&lt;p&gt;The ITA-PREREQ dataset is the first Italian dataset of concepts extracted from Wikipedia and annotated with prerequisite relations. Specifically, ITA-PREREQ is a conversion of the English AL-CPL dataset and contains a total of 455 concepts and 1,495 prerequisite relations extracted from four Wikipedia macro-domains: Data Mining, Geometry, Physics, and Numerical Analysis.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
