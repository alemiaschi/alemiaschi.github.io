---
title: "All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark"
date: 2025-10-02T00:00:00Z
venue: Findings of EMNLP 2025
author: ["Davide Testa", "Giovanni Bonetta", "Raffaella Bernardi", "Alessandro Bondielli", "Alessandro Lenci", "Alessio Miaschi", "Lucia Passaro", "Bernardo Magnini"]
venue: Findings of EMNLP 2025
summary: "We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs' consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers."

editPost:
    URL: "https://2025.emnlp.org/"
    Text: "Proceedings of the Findings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025, Suzhou, China)"

---

##### Download

+ [Paper](https://arxiv.org/abs/2502.16989)

---

##### Abstract

We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs' consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers.

---
##### Citation

```BibTeX
Proceedings of the Findings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025, Suzhou, China)

@misc{testa2025allinoneunderstandinggenerationmultimodal,
      title={All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark}, 
      author={Davide Testa and Giovanni Bonetta and Raffaella Bernardi and Alessandro Bondielli and Alessandro Lenci and Alessio Miaschi and Lucia Passaro and Bernardo Magnini},
      year={2025},
      eprint={2502.16989},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.16989}, 
}
```

---
