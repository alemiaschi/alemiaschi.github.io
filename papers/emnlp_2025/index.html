<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark | Alessio Miaschi</title><meta name=keywords content><meta name=description content="We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs&rsquo; consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers."><meta name=author content="Davide Testa,&#8201;Giovanni Bonetta,&#8201;Raffaella Bernardi,&#8201;Alessandro Bondielli,&#8201;Alessandro Lenci,&#8201;Alessio Miaschi,&#8201;Lucia Passaro,&#8201;Bernardo Magnini"><link rel=canonical href=https://alemiaschi.github.io/papers/emnlp_2025/><link crossorigin=anonymous href=/assets/css/stylesheet.ac30954b7ea20660ca1cb473400ecede9e601f17674446536180d3ca00f03328.css integrity="sha256-rDCVS36iBmDKHLRzQA7O3p5gHxdnREZTYYDTygDwMyg=" rel="preload stylesheet" as=style><link rel=icon href=https://alemiaschi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://alemiaschi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://alemiaschi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://alemiaschi.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://alemiaschi.github.io/papers/emnlp_2025/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://alemiaschi.github.io/papers/emnlp_2025/"><meta property="og:site_name" content="Alessio Miaschi"><meta property="og:title" content="All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark"><meta property="og:description" content="We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs’ consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="papers"><meta property="article:published_time" content="2025-10-02T00:00:00+00:00"><meta property="article:modified_time" content="2025-10-02T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark"><meta name=twitter:description content="We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs&rsquo; consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publications","item":"https://alemiaschi.github.io/papers/"},{"@type":"ListItem","position":2,"name":"All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark","item":"https://alemiaschi.github.io/papers/emnlp_2025/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark","name":"All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark","description":"We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs\u0026rsquo; consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers.","keywords":[],"articleBody":"Download Paper Abstract We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs’ consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers.\nCitation Proceedings of the Findings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025, Suzhou, China) @misc{testa2025allinoneunderstandinggenerationmultimodal, title={All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark}, author={Davide Testa and Giovanni Bonetta and Raffaella Bernardi and Alessandro Bondielli and Alessandro Lenci and Alessio Miaschi and Lucia Passaro and Bernardo Magnini}, year={2025}, eprint={2502.16989}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2502.16989}, } ","wordCount":"238","inLanguage":"en","datePublished":"2025-10-02T00:00:00Z","dateModified":"2025-10-02T00:00:00Z","author":[{"@type":"Person","name":"Davide Testa"},{"@type":"Person","name":"Giovanni Bonetta"},{"@type":"Person","name":"Raffaella Bernardi"},{"@type":"Person","name":"Alessandro Bondielli"},{"@type":"Person","name":"Alessandro Lenci"},{"@type":"Person","name":"Alessio Miaschi"},{"@type":"Person","name":"Lucia Passaro"},{"@type":"Person","name":"Bernardo Magnini"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://alemiaschi.github.io/papers/emnlp_2025/"},"publisher":{"@type":"Organization","name":"Alessio Miaschi","logo":{"@type":"ImageObject","url":"https://alemiaschi.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{equation*}",right:"\\end{equation*}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1})})</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://alemiaschi.github.io/ accesskey=h title="Alessio Miaschi"><img src=https://alemiaschi.github.io/logo.jpg alt aria-label=logo height=18 width=18>Alessio Miaschi</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://alemiaschi.github.io/news/ title=News><span>News</span></a></li><li><a href=https://alemiaschi.github.io/papers/ title=Papers><span>Papers</span></a></li><li><a href=https://alemiaschi.github.io/courses/ title="Courses & Theses"><span>Courses & Theses</span></a></li><li><a href=https://alemiaschi.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://alemiaschi.github.io/projects/ title="Resources & Projects"><span>Resources & Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark</h1><div class=post-meta><span title='2025-10-02 00:00:00 +0000 UTC'>October 2025</span>&nbsp;&#183;&nbsp;Davide Testa,&#8201;Giovanni Bonetta,&#8201;Raffaella Bernardi,&#8201;Alessandro Bondielli,&#8201;Alessandro Lenci,&#8201;Alessio Miaschi,&#8201;Lucia Passaro,&#8201;Bernardo Magnini&nbsp;&#183;&nbsp;<a href=https://2025.emnlp.org/ rel="noopener noreferrer" target=_blank>Proceedings of the Findings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025, Suzhou, China)</a></div></header><div class=post-content><h5 id=download>Download</h5><ul><li><a href=https://arxiv.org/abs/2502.16989 target=_blank>Paper</a></li></ul><hr><h5 id=abstract>Abstract</h5><p>We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs&rsquo; consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers.</p><hr><h5 id=citation>Citation</h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-BibTeX data-lang=BibTeX><span style=display:flex><span><span style=color:#aaa;font-style:italic>Proceedings of the Findings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025, Suzhou, China)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0a0;text-decoration:underline>@misc</span>{testa2025allinoneunderstandinggenerationmultimodal,
</span></span><span style=display:flex><span>      <span style=color:#1e90ff>title</span>=<span style=color:#a50>{All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark}</span>, 
</span></span><span style=display:flex><span>      <span style=color:#1e90ff>author</span>=<span style=color:#a50>{Davide Testa and Giovanni Bonetta and Raffaella Bernardi and Alessandro Bondielli and Alessandro Lenci and Alessio Miaschi and Lucia Passaro and Bernardo Magnini}</span>,
</span></span><span style=display:flex><span>      <span style=color:#1e90ff>year</span>=<span style=color:#a50>{2025}</span>,
</span></span><span style=display:flex><span>      <span style=color:#1e90ff>eprint</span>=<span style=color:#a50>{2502.16989}</span>,
</span></span><span style=display:flex><span>      <span style=color:#1e90ff>archivePrefix</span>=<span style=color:#a50>{arXiv}</span>,
</span></span><span style=display:flex><span>      <span style=color:#1e90ff>primaryClass</span>=<span style=color:#a50>{cs.CL}</span>,
</span></span><span style=display:flex><span>      <span style=color:#1e90ff>url</span>=<span style=color:#a50>{https://arxiv.org/abs/2502.16989}</span>, 
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><hr></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 Alessio Miaschi</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>hugo</a>, <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>papermod</a>, &
<a href=https://github.com/pmichaillat/hugo-website/ rel=noopener target=_blank>hugo-website</a>.</span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>