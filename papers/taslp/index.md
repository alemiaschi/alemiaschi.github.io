---
title: "On Robustness and Sensitivity of a Neural Language Model: A Case Study on Italian L1 Learner Errors"
date: 2022-12-02T00:00:00Z
venue: IEEE/ACM Transactions on Audio, Speech, and Language Processing	
author: ["Alessio Miaschi", "Dominique Brunato", "Felice Dell'Orletta", "Giulia Venturi"]
summary: "In this paper, we propose a comprehensive linguistic study aimed at assessing the implicit behaviour of one of the most prominent Neural Language Model (NLM) based on Transformer architectures, BERT (Devlin et al., 2019), when dealing with a particular source of noisy data, namely essays written by L1 Italian learners containing a variety of errors targeting grammar, orthography and lexicon. Differently from previous works, we focus on the pre-training stage and we devise two evaluation tasks aimed at assessing the impact of errors on sentence-level inner representations from two complementary perspectives, i.e. robustness and sensitivity. Our experiments show that BERT’s ability to compute sentence similarity and to correctly encode a set of raw and morpho-syntactic properties of a sentence are differently modulated by the category of errors and that the error hierarchies in terms of robustness and sensitivity change across layer-wise representations."

editPost:
    URL: "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6570655"
    Text: "IEEE/ACM Transactions on Audio, Speech, and Language Processing, Vol. 31, pages 426-438"

---

##### Download

+ [Paper](https://ieeexplore.ieee.org/document/9968310)

---

##### Abstract

In this paper, we propose a comprehensive linguistic study aimed at assessing the implicit behaviour of one of the most prominent Neural Language Model (NLM) based on Transformer architectures, BERT (Devlin et al., 2019), when dealing with a particular source of noisy data, namely essays written by L1 Italian learners containing a variety of errors targeting grammar, orthography and lexicon. Differently from previous works, we focus on the pre-training stage and we devise two evaluation tasks aimed at assessing the impact of errors on sentence-level inner representations from two complementary perspectives, i.e. robustness and sensitivity. Our experiments show that BERT’s ability to compute sentence similarity and to correctly encode a set of raw and morpho-syntactic properties of a sentence are differently modulated by the category of errors and that the error hierarchies in terms of robustness and sensitivity change across layer-wise representations.

---

##### Citation

```BibTeX
@ARTICLE{9968310,
  author={Miaschi, Alessio and Brunato, Dominique and Dell'Orletta, Felice and Venturi, Giulia},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={On Robustness and Sensitivity of a Neural Language Model: A Case Study on Italian L1 Learner Errors}, 
  year={2023},
  volume={31},
  number={},
  pages={426-438},
  doi={10.1109/TASLP.2022.3226333}}


