<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>How Do BERT Embeddings Organize Linguistic Knowledge? | Alessio Miaschi</title><meta name=keywords content><meta name=description content="Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties."><meta name=author content="Giovanni Puccetti,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta"><link rel=canonical href=https://alemiaschi.github.io/papers/deelio_2021_2/><link crossorigin=anonymous href=/assets/css/stylesheet.ac30954b7ea20660ca1cb473400ecede9e601f17674446536180d3ca00f03328.css integrity="sha256-rDCVS36iBmDKHLRzQA7O3p5gHxdnREZTYYDTygDwMyg=" rel="preload stylesheet" as=style><link rel=icon href=https://alemiaschi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://alemiaschi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://alemiaschi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://alemiaschi.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://alemiaschi.github.io/papers/deelio_2021_2/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://alemiaschi.github.io/papers/deelio_2021_2/"><meta property="og:site_name" content="Alessio Miaschi"><meta property="og:title" content="How Do BERT Embeddings Organize Linguistic Knowledge?"><meta property="og:description" content="Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="papers"><meta property="article:published_time" content="2021-05-10T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-10T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="How Do BERT Embeddings Organize Linguistic Knowledge?"><meta name=twitter:description content="Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publications","item":"https://alemiaschi.github.io/papers/"},{"@type":"ListItem","position":2,"name":"How Do BERT Embeddings Organize Linguistic Knowledge?","item":"https://alemiaschi.github.io/papers/deelio_2021_2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How Do BERT Embeddings Organize Linguistic Knowledge?","name":"How Do BERT Embeddings Organize Linguistic Knowledge?","description":"Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties.","keywords":[],"articleBody":"Download Paper Abstract Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties.\nCitation @inproceedings{puccetti-etal-2021-bert, title = \"How Do {BERT} Embeddings Organize Linguistic Knowledge?\", author = \"Puccetti, Giovanni and Miaschi, Alessio and Dell{'}Orletta, Felice\", booktitle = \"Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures\", month = jun, year = \"2021\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/2021.deelio-1.6\", pages = \"48--57\", abstract = \"Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties.\", } ","wordCount":"307","inLanguage":"en","datePublished":"2021-05-10T00:00:00Z","dateModified":"2021-05-10T00:00:00Z","author":[{"@type":"Person","name":"Giovanni Puccetti"},{"@type":"Person","name":"Alessio Miaschi"},{"@type":"Person","name":"Felice Dell'Orletta"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://alemiaschi.github.io/papers/deelio_2021_2/"},"publisher":{"@type":"Organization","name":"Alessio Miaschi","logo":{"@type":"ImageObject","url":"https://alemiaschi.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{equation*}",right:"\\end{equation*}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1})})</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://alemiaschi.github.io/ accesskey=h title="Alessio Miaschi"><img src=https://alemiaschi.github.io/logo.jpg alt aria-label=logo height=18 width=18>Alessio Miaschi</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://alemiaschi.github.io/news/ title=News><span>News</span></a></li><li><a href=https://alemiaschi.github.io/papers/ title=Papers><span>Papers</span></a></li><li><a href=https://alemiaschi.github.io/courses/ title="Courses & Theses"><span>Courses & Theses</span></a></li><li><a href=https://alemiaschi.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://alemiaschi.github.io/projects/ title="Resources & Projects"><span>Resources & Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">How Do BERT Embeddings Organize Linguistic Knowledge?</h1><div class=post-meta><span title='2021-05-10 00:00:00 +0000 UTC'>May 2021</span>&nbsp;&#183;&nbsp;Giovanni Puccetti,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta&nbsp;&#183;&nbsp;<a href=https://aclanthology.org/volumes/2021.deelio-1/ rel="noopener noreferrer" target=_blank>Proceedings of the 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures (NAACL 2021, Online)</a></div></header><div class=post-content><h5 id=download>Download</h5><ul><li><a href=https://aclanthology.org/2021.deelio-1.6.pdf target=_blank>Paper</a></li></ul><hr><h5 id=abstract>Abstract</h5><p>Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties.</p><hr><h5 id=citation>Citation</h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-BibTeX data-lang=BibTeX><span style=display:flex><span><span style=color:#0a0;text-decoration:underline>@inproceedings</span>{puccetti-etal-2021-bert,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>title</span> = <span style=color:#a50>&#34;How Do {BERT} Embeddings Organize Linguistic Knowledge?&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>author</span> = <span style=color:#a50>&#34;Puccetti, Giovanni  and
</span></span></span><span style=display:flex><span><span style=color:#a50>      Miaschi, Alessio  and
</span></span></span><span style=display:flex><span><span style=color:#a50>      Dell{&#39;}Orletta, Felice&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>booktitle</span> = <span style=color:#a50>&#34;Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>month</span> = <span style=color:#a00>jun</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>year</span> = <span style=color:#a50>&#34;2021&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>address</span> = <span style=color:#a50>&#34;Online&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>publisher</span> = <span style=color:#a50>&#34;Association for Computational Linguistics&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>url</span> = <span style=color:#a50>&#34;https://www.aclweb.org/anthology/2021.deelio-1.6&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>pages</span> = <span style=color:#a50>&#34;48--57&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>abstract</span> = <span style=color:#a50>&#34;Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties.&#34;</span>,
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 Alessio Miaschi</span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>hugo</a>, <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>papermod</a>, &
<a href=https://github.com/pmichaillat/hugo-website/ rel=noopener target=_blank>hugo-website</a>.</span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>