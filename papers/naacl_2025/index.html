<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation | Alessio Miaschi</title>
<meta name="keywords" content="">
<meta name="description" content="An increasing number of pretrained Large Language Models (LLMs) are being released, though the majority are predominantly designed for English. While they can often handle other languages due to contamination or some degree of multilingual pretraining data, English-centric LLMs are not optimized for non-English languages. This leads to inefficient encoding (high token ‘fertility’) and slower inference times for those languages. In this work, we explore various vocabulary adaptation techniques to tailor English LLMs for the Italian language. We introduce Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that learns neural mapping to accomplish vocabulary substitution, which achieve state-of-the-art performances on several downstream tasks. We adapted two LLMs: Mistral-7b-v0.1, reducing token fertility by 25%, and Llama-3.1-8b, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, after the adaptation of the vocabulary, these models can recover their performances with a relatively limited stage of continual training on the target language. Finally, we test the adapted models&rsquo; capabilities on several multi-choice and generative tasks.">
<meta name="author" content="Luca Moroni,&thinsp;Giovanni Puccetti,&thinsp;Pere-Lluís Huguet Cabot,&thinsp;Andrei Stefan Bejgu,&thinsp;Alessio Miaschi,&thinsp;Edoardo Barba,&thinsp;Felice Dell&#39;Orletta,&thinsp;Andrea Esuli,&thinsp;Roberto Navigli">
<link rel="canonical" href="http://localhost:1313/papers/naacl_2025/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ac30954b7ea20660ca1cb473400ecede9e601f17674446536180d3ca00f03328.css" integrity="sha256-rDCVS36iBmDKHLRzQA7O3p5gHxdnREZTYYDTygDwMyg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/papers/naacl_2025/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/papers/naacl_2025/">
  <meta property="og:site_name" content="Alessio Miaschi">
  <meta property="og:title" content="Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation">
  <meta property="og:description" content="An increasing number of pretrained Large Language Models (LLMs) are being released, though the majority are predominantly designed for English. While they can often handle other languages due to contamination or some degree of multilingual pretraining data, English-centric LLMs are not optimized for non-English languages. This leads to inefficient encoding (high token ‘fertility’) and slower inference times for those languages. In this work, we explore various vocabulary adaptation techniques to tailor English LLMs for the Italian language. We introduce Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that learns neural mapping to accomplish vocabulary substitution, which achieve state-of-the-art performances on several downstream tasks. We adapted two LLMs: Mistral-7b-v0.1, reducing token fertility by 25%, and Llama-3.1-8b, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, after the adaptation of the vocabulary, these models can recover their performances with a relatively limited stage of continual training on the target language. Finally, we test the adapted models’ capabilities on several multi-choice and generative tasks.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">
    <meta property="article:published_time" content="2025-04-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-04-10T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation">
<meta name="twitter:description" content="An increasing number of pretrained Large Language Models (LLMs) are being released, though the majority are predominantly designed for English. While they can often handle other languages due to contamination or some degree of multilingual pretraining data, English-centric LLMs are not optimized for non-English languages. This leads to inefficient encoding (high token ‘fertility’) and slower inference times for those languages. In this work, we explore various vocabulary adaptation techniques to tailor English LLMs for the Italian language. We introduce Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that learns neural mapping to accomplish vocabulary substitution, which achieve state-of-the-art performances on several downstream tasks. We adapted two LLMs: Mistral-7b-v0.1, reducing token fertility by 25%, and Llama-3.1-8b, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, after the adaptation of the vocabulary, these models can recover their performances with a relatively limited stage of continual training on the target language. Finally, we test the adapted models&rsquo; capabilities on several multi-choice and generative tasks.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Publications",
      "item": "http://localhost:1313/papers/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation",
      "item": "http://localhost:1313/papers/naacl_2025/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation",
  "name": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation",
  "description": "An increasing number of pretrained Large Language Models (LLMs) are being released, though the majority are predominantly designed for English. While they can often handle other languages due to contamination or some degree of multilingual pretraining data, English-centric LLMs are not optimized for non-English languages. This leads to inefficient encoding (high token ‘fertility’) and slower inference times for those languages. In this work, we explore various vocabulary adaptation techniques to tailor English LLMs for the Italian language. We introduce Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that learns neural mapping to accomplish vocabulary substitution, which achieve state-of-the-art performances on several downstream tasks. We adapted two LLMs: Mistral-7b-v0.1, reducing token fertility by 25%, and Llama-3.1-8b, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, after the adaptation of the vocabulary, these models can recover their performances with a relatively limited stage of continual training on the target language. Finally, we test the adapted models\u0026rsquo; capabilities on several multi-choice and generative tasks.",
  "keywords": [
    
  ],
  "articleBody": "Download Paper Abstract An increasing number of pretrained Large Language Models (LLMs) are being released, though the majority are predominantly designed for English. While they can often handle other languages due to contamination or some degree of multilingual pretraining data, English-centric LLMs are not optimized for non-English languages. This leads to inefficient encoding (high token ‘fertility’) and slower inference times for those languages. In this work, we explore various vocabulary adaptation techniques to tailor English LLMs for the Italian language. We introduce Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that learns neural mapping to accomplish vocabulary substitution, which achieve state-of-the-art performances on several downstream tasks. We adapted two LLMs: Mistral-7b-v0.1, reducing token fertility by 25%, and Llama-3.1-8b, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, after the adaptation of the vocabulary, these models can recover their performances with a relatively limited stage of continual training on the target language. Finally, we test the adapted models’ capabilities on several multi-choice and generative tasks.\nCitation @inproceedings{moroni-etal-2025-optimizing, title = \"Optimizing {LLM}s for {I}talian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation\", author = \"Moroni, Luca and Puccetti, Giovanni and Huguet Cabot, Pere-Llu{\\'i}s and Bejgu, Andrei Stefan and Miaschi, Alessio and Barba, Edoardo and Dell{'}Orletta, Felice and Esuli, Andrea and Navigli, Roberto\", editor = \"Chiruzzo, Luis and Ritter, Alan and Wang, Lu\", booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2025\", month = apr, year = \"2025\", address = \"Albuquerque, New Mexico\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2025.findings-naacl.371/\", pages = \"6646--6660\", ISBN = \"979-8-89176-195-7\", abstract = \"The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token {\\textquotedblleft}fertility{\\textquotedblright}) and slower inference speed.In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7B-v0.1, reducing token fertility by 25{\\%}, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.\" } ",
  "wordCount" : "439",
  "inLanguage": "en",
  "datePublished": "2025-04-10T00:00:00Z",
  "dateModified": "2025-04-10T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Luca Moroni"
  }, {
    "@type": "Person",
    "name": "Giovanni Puccetti"
  }, {
    "@type": "Person",
    "name": "Pere-Lluís Huguet Cabot"
  }, {
    "@type": "Person",
    "name": "Andrei Stefan Bejgu"
  }, {
    "@type": "Person",
    "name": "Alessio Miaschi"
  }, {
    "@type": "Person",
    "name": "Edoardo Barba"
  }, {
    "@type": "Person",
    "name": "Felice Dell'Orletta"
  }, {
    "@type": "Person",
    "name": "Andrea Esuli"
  }, {
    "@type": "Person",
    "name": "Roberto Navigli"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/papers/naacl_2025/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Alessio Miaschi",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Alessio Miaschi">
                <img src="http://localhost:1313/logo.jpg" alt="" aria-label="logo"
                    height="18"
                    width="18">Alessio Miaschi</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/news/" title="News">
                    <span>News</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/courses/" title="Courses &amp; Theses">
                    <span>Courses &amp; Theses</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Resources &amp; Projects">
                    <span>Resources &amp; Projects</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation
    </h1>
    <div class="post-meta"><span title='2025-04-10 00:00:00 +0000 UTC'>April 2025</span>&nbsp;&middot;&nbsp;Luca Moroni,&thinsp;Giovanni Puccetti,&thinsp;Pere-Lluís Huguet Cabot,&thinsp;Andrei Stefan Bejgu,&thinsp;Alessio Miaschi,&thinsp;Edoardo Barba,&thinsp;Felice Dell'Orletta,&thinsp;Andrea Esuli,&thinsp;Roberto Navigli&nbsp;&middot;&nbsp;<a href="https://2025.naacl.org/" rel="noopener noreferrer" target="_blank">Proceedings of the Findings of the 2025 Annual Conference of the Nations of the Americas Chapter of the ACL (Findings of NAACL 2025, Albuquerque, Nex Mexico)</a>

</div>
  </header> 
  <div class="post-content"><h5 id="download">Download</h5>
<ul>
<li><a href="https://aclanthology.org/2025.findings-naacl.371.pdf" target="_blank">Paper</a></li>
</ul>
<hr>
<h5 id="abstract">Abstract</h5>
<p>An increasing number of pretrained Large Language Models (LLMs) are being released, though the majority are predominantly designed for English. While they can often handle other languages due to contamination or some degree of multilingual pretraining data, English-centric LLMs are not optimized for non-English languages. This leads to inefficient encoding (high token ‘fertility’) and slower inference times for those languages. In this work, we explore various vocabulary adaptation techniques to tailor English LLMs for the Italian language. We introduce Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that learns neural mapping to accomplish vocabulary substitution, which achieve state-of-the-art performances on several downstream tasks. We adapted two LLMs: Mistral-7b-v0.1, reducing token fertility by 25%, and Llama-3.1-8b, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, after the adaptation of the vocabulary, these models can recover their performances with a relatively limited stage of continual training on the target language. Finally, we test the adapted models&rsquo; capabilities on several multi-choice and generative tasks.</p>
<hr>
<h5 id="citation">Citation</h5>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-BibTeX" data-lang="BibTeX"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@inproceedings</span>{moroni-etal-2025-optimizing,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">title</span> = <span style="color:#a50">&#34;Optimizing {LLM}s for {I}talian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">author</span> = <span style="color:#a50">&#34;Moroni, Luca  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Puccetti, Giovanni  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Huguet Cabot, Pere-Llu{\&#39;i}s  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Bejgu, Andrei Stefan  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Miaschi, Alessio  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Barba, Edoardo  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Dell{&#39;}Orletta, Felice  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Esuli, Andrea  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Navigli, Roberto&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">editor</span> = <span style="color:#a50">&#34;Chiruzzo, Luis  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Ritter, Alan  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Wang, Lu&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">booktitle</span> = <span style="color:#a50">&#34;Findings of the Association for Computational Linguistics: NAACL 2025&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">month</span> = <span style="color:#a00">apr</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">year</span> = <span style="color:#a50">&#34;2025&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">address</span> = <span style="color:#a50">&#34;Albuquerque, New Mexico&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">publisher</span> = <span style="color:#a50">&#34;Association for Computational Linguistics&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">url</span> = <span style="color:#a50">&#34;https://aclanthology.org/2025.findings-naacl.371/&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">pages</span> = <span style="color:#a50">&#34;6646--6660&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">ISBN</span> = <span style="color:#a50">&#34;979-8-89176-195-7&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">abstract</span> = <span style="color:#a50">&#34;The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token {\textquotedblleft}fertility{\textquotedblright}) and slower inference speed.In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7B-v0.1, reducing token fertility by 25{\%}, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 Alessio Miaschi</span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">hugo</a>, <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">papermod</a>, &amp;
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">hugo-website</a>.
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
