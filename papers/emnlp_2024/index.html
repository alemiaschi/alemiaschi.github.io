<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Evaluating Large Language Models via Linguistic Profiling | Alessio Miaschi</title>
<meta name="keywords" content="">
<meta name="description" content="Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models&rsquo; linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs&rsquo; sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling&rsquo; approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.">
<meta name="author" content="Alessio Miaschi,&thinsp;Felice Dell&#39;Orletta,&thinsp;Giulia Venturi">
<link rel="canonical" href="http://localhost:1313/papers/emnlp_2024/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ac30954b7ea20660ca1cb473400ecede9e601f17674446536180d3ca00f03328.css" integrity="sha256-rDCVS36iBmDKHLRzQA7O3p5gHxdnREZTYYDTygDwMyg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/papers/emnlp_2024/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/papers/emnlp_2024/">
  <meta property="og:site_name" content="Alessio Miaschi">
  <meta property="og:title" content="Evaluating Large Language Models via Linguistic Profiling">
  <meta property="og:description" content="Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models’ linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs’ sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling’ approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">
    <meta property="article:published_time" content="2024-11-12T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-11-12T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Evaluating Large Language Models via Linguistic Profiling">
<meta name="twitter:description" content="Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models&rsquo; linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs&rsquo; sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling&rsquo; approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Publications",
      "item": "http://localhost:1313/papers/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Evaluating Large Language Models via Linguistic Profiling",
      "item": "http://localhost:1313/papers/emnlp_2024/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Evaluating Large Language Models via Linguistic Profiling",
  "name": "Evaluating Large Language Models via Linguistic Profiling",
  "description": "Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models\u0026rsquo; linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs\u0026rsquo; sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling\u0026rsquo; approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.",
  "keywords": [
    
  ],
  "articleBody": "Download Paper Abstract Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models’ linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs’ sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling’ approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.\nCitation @inproceedings{miaschi-etal-2024-evaluating, title = \"Evaluating Large Language Models via Linguistic Profiling\", author = \"Miaschi, Alessio and Dell{'}Orletta, Felice and Venturi, Giulia\", editor = \"Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung\", booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\", month = nov, year = \"2024\", address = \"Miami, Florida, USA\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2024.emnlp-main.166\", pages = \"2835--2848\", abstract = \"Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models{'} linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs{'} sentence generation abilities under specific linguistic constraints. Drawing on the {`}linguistic profiling{'} approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.\", } ",
  "wordCount" : "302",
  "inLanguage": "en",
  "datePublished": "2024-11-12T00:00:00Z",
  "dateModified": "2024-11-12T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Alessio Miaschi"
  }, {
    "@type": "Person",
    "name": "Felice Dell'Orletta"
  }, {
    "@type": "Person",
    "name": "Giulia Venturi"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/papers/emnlp_2024/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Alessio Miaschi",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Alessio Miaschi">
                <img src="http://localhost:1313/logo.jpg" alt="" aria-label="logo"
                    height="18"
                    width="18">Alessio Miaschi</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/news/" title="News">
                    <span>News</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/courses/" title="Courses &amp; Theses">
                    <span>Courses &amp; Theses</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Resources &amp; Projects">
                    <span>Resources &amp; Projects</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Evaluating Large Language Models via Linguistic Profiling
    </h1>
    <div class="post-meta"><span title='2024-11-12 00:00:00 +0000 UTC'>November 2024</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Felice Dell'Orletta,&thinsp;Giulia Venturi&nbsp;&middot;&nbsp;<a href="https://2024.emnlp.org/" rel="noopener noreferrer" target="_blank">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024, Miami, Florida)</a>

</div>
  </header> 
  <div class="post-content"><h5 id="download">Download</h5>
<ul>
<li><a href="https://aclanthology.org/2024.emnlp-main.166/" target="_blank">Paper</a></li>
</ul>
<hr>
<h5 id="abstract">Abstract</h5>
<p>Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models&rsquo; linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs&rsquo; sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling&rsquo; approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.</p>
<hr>
<h5 id="citation">Citation</h5>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-BibTeX" data-lang="BibTeX"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@inproceedings</span>{miaschi-etal-2024-evaluating,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">title</span> = <span style="color:#a50">&#34;Evaluating Large Language Models via Linguistic Profiling&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">author</span> = <span style="color:#a50">&#34;Miaschi, Alessio  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Dell{&#39;}Orletta, Felice  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Venturi, Giulia&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">editor</span> = <span style="color:#a50">&#34;Al-Onaizan, Yaser  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Bansal, Mohit  and
</span></span></span><span style="display:flex;"><span><span style="color:#a50">      Chen, Yun-Nung&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">booktitle</span> = <span style="color:#a50">&#34;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">month</span> = <span style="color:#a00">nov</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">year</span> = <span style="color:#a50">&#34;2024&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">address</span> = <span style="color:#a50">&#34;Miami, Florida, USA&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">publisher</span> = <span style="color:#a50">&#34;Association for Computational Linguistics&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">url</span> = <span style="color:#a50">&#34;https://aclanthology.org/2024.emnlp-main.166&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">pages</span> = <span style="color:#a50">&#34;2835--2848&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#1e90ff">abstract</span> = <span style="color:#a50">&#34;Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models{&#39;} linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs{&#39;} sentence generation abilities under specific linguistic constraints. Drawing on the {`}linguistic profiling{&#39;} approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.&#34;</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 Alessio Miaschi</span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">hugo</a>, <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">papermod</a>, &amp;
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">hugo-website</a>.
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
