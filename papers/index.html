<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Publications | Alessio Miaschi</title><meta name=keywords content><meta name=description content="Preprints and articles by Alessio Miaschi"><meta name=author content="Alessio Miaschi"><link rel=canonical href=https://alemiaschi.github.io/papers/><link crossorigin=anonymous href=/assets/css/stylesheet.ac30954b7ea20660ca1cb473400ecede9e601f17674446536180d3ca00f03328.css integrity="sha256-rDCVS36iBmDKHLRzQA7O3p5gHxdnREZTYYDTygDwMyg=" rel="preload stylesheet" as=style><link rel=icon href=https://alemiaschi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://alemiaschi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://alemiaschi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://alemiaschi.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://alemiaschi.github.io/papers/index.xml><link rel=alternate hreflang=en href=https://alemiaschi.github.io/papers/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://alemiaschi.github.io/papers/"><meta property="og:site_name" content="Alessio Miaschi"><meta property="og:title" content="Publications"><meta property="og:description" content="Preprints and articles by Alessio Miaschi"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="Preprints and articles by Alessio Miaschi"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publications","item":"https://alemiaschi.github.io/papers/"}]}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{equation*}",right:"\\end{equation*}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1})})</script></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://alemiaschi.github.io/ accesskey=h title="Alessio Miaschi"><img src=https://alemiaschi.github.io/logo.jpg alt aria-label=logo height=18 width=18>Alessio Miaschi</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://alemiaschi.github.io/news/ title=News><span>News</span></a></li><li><a href=https://alemiaschi.github.io/papers/ title=Papers><span class=active>Papers</span></a></li><li><a href=https://alemiaschi.github.io/courses/ title="Courses & Theses"><span>Courses & Theses</span></a></li><li><a href=https://alemiaschi.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://alemiaschi.github.io/projects/ title="Resources & Projects"><span>Resources & Projects</span></a></li></ul></nav></header><main class=main><header class=page-header><h1></h1></header><div class=post-content><h1 id=publications>Publications</h1><p>You can also check at the following link: <a href="https://scholar.google.it/citations?user=BaL8eJ0AAAAJ&amp;hl=it" target=_blank>https://scholar.google.it/citations?user=BaL8eJ0AAAAJ&hl=it</a></p></div><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>Findings of EMNLP 2025</p></header><div class=entry-content><p>We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs‚Äô consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers.</p></div><footer class=entry-footer><span title='2025-10-02 00:00:00 +0000 UTC'>October 2025</span>&nbsp;&#183;&nbsp;Davide Testa,&#8201;Giovanni Bonetta,&#8201;Raffaella Bernardi,&#8201;Alessandro Bondielli,&#8201;Alessandro Lenci,&#8201;Alessio Miaschi,&#8201;Lucia Passaro,&#8201;Bernardo Magnini</footer><a class=entry-link aria-label="post link to All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark" href=https://alemiaschi.github.io/papers/emnlp_2025/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Crossword Space: Latent Manifold Learning for Italian Crosswords and Beyond [üèÜ Best Student Paper Award üèÜ]</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>CLiC-it 2025</p></header><div class=entry-content><p>Answering crossword puzzle clues presents a challenging retrieval task that requires matching linguistically rich and often ambiguous clues with appropriate solutions. While traditional retrieval-based strategies can commonly be used to address this issue, wordplays and other lateral thinking strategies limit the effectiveness of conventional lexical and semantic approaches. In this work, we address the clue answering task as an information retrieval problem exploiting the potential of encoder-based Transformer models to learn a shared latent space between clues and solutions. In particular, we propose for the first time a collection of siamese and asymmetric dual encoder architectures trained to capture the complex properties and relation characterizing crossword clues and their solutions for the Italian language. After comparing various architectures for this task, we show that the strong retrieval capabilities of these systems extend to neologisms and dictionary terms, suggesting their potential use in linguistic analyses beyond the scope of language games.</p></div><footer class=entry-footer><span title='2025-09-24 00:00:00 +0000 UTC'>September 2025</span>&nbsp;&#183;&nbsp;Cristiano Ciaccio,&#8201;Gabriele Sarti,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta</footer><a class=entry-link aria-label="post link to Crossword Space: Latent Manifold Learning for Italian Crosswords and Beyond [üèÜ Best Student Paper Award üèÜ]" href=https://alemiaschi.github.io/papers/clic_2025_1/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>MAIA: a Benchmark for Multimodal AI Assessment</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>CLiC-it 2025</p></header><div class=entry-content><p>We introduce MAIA (Multimodal AI Assessment), a multimodal dataset developed as a core component of a competence-oriented benchmark designed for fine-grained investigation of the reasoning abilities of Visual Language Models (VLMs) on videos. The MAIA benchmark is characterized by several distinctive features. To the best of our knowledge, MAIA is the first Italian-native benchmark addressing video understanding: videos were carefully selected to reflect Italian culture, and the language data (i.e., questions and reference answers) were produced by native-Italian speakers. Second, MAIA explicitly includes twelve reasoning categories that are specifically designed to assess the reasoning abilities of VLMs on videos. Third, we structured the dataset to support two aligned tasks (i.e., a statement verification and an open-ended visual question answering) built on the same datapoints, this way allowing to assess VLM coherence across task formats. Finally MAIA integrates, by design, state-of-the-art LLMs in the development process of the benchmark, taking advantage of their linguistic and reasoning capabilities both for data augmentation and for assessing and improving the overall quality of the data. In the paper we focus on the design principles and the data collection methodology, highlighting how MAIA provides a significant advancement with respect to other available dataset for VLM benchmarking.</p></div><footer class=entry-footer><span title='2025-09-24 00:00:00 +0000 UTC'>September 2025</span>&nbsp;&#183;&nbsp;Davide Testa,&#8201;Giovanni Bonetta,&#8201;Raffaella Bernardi,&#8201;Alessandro Bondielli,&#8201;Alessandro Lenci,&#8201;Alessio Miaschi,&#8201;Lucia Passaro,&#8201;Bernardo Magnini</footer><a class=entry-link aria-label="post link to MAIA: a Benchmark for Multimodal AI Assessment" href=https://alemiaschi.github.io/papers/clic_2025_3/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>The OuLiBench Benchmark: Formal Constraints as a Lens into LLM Linguistic Competence</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>CLiC-it 2025</p></header><div class=entry-content><p>Recent progress in Large Language Models (LLMs) has led to impressive capabilities in Natural Language Generation (NLG). However, standard evaluation benchmarks often focus on surface-level performance and are predominantly English-centric, limiting insights into models‚Äô deeper linguistic competences, especially in other languages. In this paper, we introduce OuLiBench, a novel benchmark inspired by the literary movement OuLiPo, designed to evaluate LLMs‚Äô ability to generate Italian text under explicit linguistic constraints, ranging from morpho-syntactic requirements to creative and structural challenges. Our goal is to assess the extent to which LLMs can understand and manipulate language when guided by specific, sometimes artificial constraints. We evaluate a range of state-of-the-art models in both zero- and few-shot settings, comparing performance across constraint types and difficulty levels. Our results highlight significant variability across models and tasks, shedding light on the limits of controllable text generation and offering a new lens for probing LLMs‚Äô generative and linguistic competence beyond traditional benchmarks.</p></div><footer class=entry-footer><span title='2025-09-24 00:00:00 +0000 UTC'>September 2025</span>&nbsp;&#183;&nbsp;Silvio Calderaro,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta</footer><a class=entry-link aria-label="post link to The OuLiBench Benchmark: Formal Constraints as a Lens into LLM Linguistic Competence" href=https://alemiaschi.github.io/papers/clic_2025_2/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>Findings of ACL 2025</p></header><div class=entry-content><p>Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are ‚Äòcharacter-blind‚Äô and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots.</p></div><footer class=entry-footer><span title='2025-07-27 00:00:00 +0000 UTC'>July 2025</span>&nbsp;&#183;&nbsp;Cristiano Ciaccio,&#8201;Marta Sartor,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta</footer><a class=entry-link aria-label="post link to Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models" href=https://alemiaschi.github.io/papers/acl_2025_2/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Evaluating Lexical Proficiency in Neural Language Models</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>ACL 2025</p></header><div class=entry-content><p>We present a novel evaluation framework designed to assess the lexical proficiency and linguistic creativity of Transformer-based Language Models (LMs). We validate the framework by analyzing the performance of a set of LMs of different sizes, in both mono- and multilingual configuration, across tasks involving the generation, definition, and contextual usage of lexicalized words, neologisms, and nonce words. To support these evaluations, we developed a novel dataset of lexical entries for the Italian language, including curated definitions and usage examples sourced from various online platforms. The results highlight the robustness and effectiveness of our framework in evaluating multiple dimensions of LMs‚Äô linguistic understanding and offer an insight, through the assessment of their linguistic creativity, on the lexical generalization abilities of LMs.</p></div><footer class=entry-footer><span title='2025-07-27 00:00:00 +0000 UTC'>July 2025</span>&nbsp;&#183;&nbsp;Cristiano Ciaccio,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta</footer><a class=entry-link aria-label="post link to Evaluating Lexical Proficiency in Neural Language Models" href=https://alemiaschi.github.io/papers/acl_2025_3/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>Findings of ACL 2025</p></header><div class=entry-content><p>Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we evaluate the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. We develop a pipeline that fine-tunes language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT), obtaining generations more challenging to detect by current models. Additionally, we analyze the linguistic shifts induced by the alignment and how detectors rely on ‚Äúlinguistic shortcuts‚Äù to detect texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detecting performances. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts. We release code, models, and data to support future research on more robust MGT detection benchmarks.</p></div><footer class=entry-footer><span title='2025-07-27 00:00:00 +0000 UTC'>July 2025</span>&nbsp;&#183;&nbsp;Andrea Pedrotti,&#8201;Michele Papucci,&#8201;Cristiano Ciaccio,&#8201;Alessio Miaschi,&#8201;Giovanni Puccetti,&#8201;Felice Dell'Orletta,&#8201;Andrea Esuli</footer><a class=entry-link aria-label="post link to Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors" href=https://alemiaschi.github.io/papers/acl_2025_1/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Parallel Trees: a novel resource with aligned dependency and constituency syntactic representations</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>Language Resources and Evaluation (LRE)</p></header><div class=entry-content><p>The paper introduces Parallel Trees, a novel multilingual treebank collection that includes 20 treebanks for 10 languages. The distinguishing property of this resource is that the sentences of each language are annotated using two syntactic representation paradigms (SRPs), respectively based on the notions of dependency and constituency. By aligning the annotations of existing resources, Parallel Trees represents an example of exploiting pre-existing treebanks to adapt them to novel applications. To illustrate its potential, we present a case study where the resource is employed as a benchmark to investigate whether and how BERT, one of the first prominent neural language models (NLMs), is sensitive to the dependency- and constituency-based approaches for representing the syntactic structure of a sentence. The case study results indicate that the model‚Äôs sensitivity fluctuates across languages and experimental settings. The unique nature of the Parallel Trees resource creates the prerequisites for innovative studies comparing dependency and phrase-structure trees, allowing for more focused investigations without the interference of lexical variation.</p></div><footer class=entry-footer><span title='2025-06-10 00:00:00 +0000 UTC'>June 2025</span>&nbsp;&#183;&nbsp;Chiara Alzetta,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta,&#8201;Giulia Venturi,&#8201;Simonetta Montemagni</footer><a class=entry-link aria-label="post link to Parallel Trees: a novel resource with aligned dependency and constituency syntactic representations" href=https://alemiaschi.github.io/papers/lre/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>Findings of NAACL 2025</p></header><div class=entry-content><p>An increasing number of pretrained Large Language Models (LLMs) are being released, though the majority are predominantly designed for English. While they can often handle other languages due to contamination or some degree of multilingual pretraining data, English-centric LLMs are not optimized for non-English languages. This leads to inefficient encoding (high token ‚Äòfertility‚Äô) and slower inference times for those languages. In this work, we explore various vocabulary adaptation techniques to tailor English LLMs for the Italian language. We introduce Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that learns neural mapping to accomplish vocabulary substitution, which achieve state-of-the-art performances on several downstream tasks. We adapted two LLMs: Mistral-7b-v0.1, reducing token fertility by 25%, and Llama-3.1-8b, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, after the adaptation of the vocabulary, these models can recover their performances with a relatively limited stage of continual training on the target language. Finally, we test the adapted models‚Äô capabilities on several multi-choice and generative tasks.</p></div><footer class=entry-footer><span title='2025-04-10 00:00:00 +0000 UTC'>April 2025</span>&nbsp;&#183;&nbsp;Luca Moroni,&#8201;Giovanni Puccetti,&#8201;Pere-Llu√≠s Huguet Cabot,&#8201;Andrei Stefan Bejgu,&#8201;Alessio Miaschi,&#8201;Edoardo Barba,&#8201;Felice Dell'Orletta,&#8201;Andrea Esuli,&#8201;Roberto Navigli</footer><a class=entry-link aria-label="post link to Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation" href=https://alemiaschi.github.io/papers/naacl_2025/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Leveraging encoder-only large language models for mobile app review feature extraction</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>Empirical Software Engineering, Volume 30 (2025)</p></header><div class=entry-content><p>Mobile app review analysis presents unique challenges due to the low quality, subjective bias, and noisy content of user-generated documents. Extracting features from these reviews is essential for tasks such as feature prioritization and sentiment analysis, but it remains a challenging task. Meanwhile, encoder-only models based on the Transformer architecture have shown promising results for classification and information extraction tasks for multiple software engineering processes. This study explores the hypothesis that encoder-only large language models can enhance feature extraction from mobile app reviews. By leveraging crowdsourced annotations from an industrial context, we redefine feature extraction as a supervised token classification task. Our approach includes extending the pre-training of these models with a large corpus of user reviews to improve contextual understanding and employing instance selection techniques to optimize model fine-tuning. Empirical evaluations demonstrate that these methods improve the precision and recall of extracted features and enhance performance efficiency. Key contributions include a novel approach to feature extraction, annotated datasets, extended pre-trained models, and an instance selection mechanism for cost-effective fine-tuning. This research provides practical methods and empirical evidence in applying large language models to natural language processing tasks within mobile app reviews, offering improved performance in feature extraction.</p></div><footer class=entry-footer><span title='2025-04-05 00:00:00 +0000 UTC'>April 2025</span>&nbsp;&#183;&nbsp;Quim Motger,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta,&#8201;Xavier Franch,&#8201;Jordi Marco</footer><a class=entry-link aria-label="post link to Leveraging encoder-only large language models for mobile app review feature extraction" href=https://alemiaschi.github.io/papers/ese/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>Web Conference 2025 (WWW 2025)</p></header><div class=entry-content><p>AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.</p></div><footer class=entry-footer><span title='2025-04-01 00:00:00 +0000 UTC'>April 2025</span>&nbsp;&#183;&nbsp;Lorenzo Cima,&#8201;Alessio Miaschi,&#8201;Amaury Trujillo,&#8201;Marco Avvenuti,&#8201;Felice Dell'Orletta,&#8201;Stefano Cresci</footer><a class=entry-link aria-label="post link to Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation" href=https://alemiaschi.github.io/papers/www_2025/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Controllable Text Generation To Evaluate Linguistic Abilities of Italian LLMs</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>CLiC-it 2024</p></header><div class=entry-content><p>State-of-the-art Large Language Models (LLMs) demonstrate exceptional proficiency across diverse tasks, yet systematic evaluations of their linguistic abilities remain limited. This paper addresses this gap by proposing a new evaluation framework leveraging the potentialities of Controllable Text Generation. Our approach evaluates the models‚Äô capacity to generate sentences that adhere to specific linguistic constraints and their ability to recognize the linguistic properties of their own generated sentences, also in terms of consistency with the specified constraints. We tested our approach on six Italian LLMs using various linguistic constraints.</p></div><footer class=entry-footer><span title='2024-12-10 00:00:00 +0000 UTC'>December 2024</span>&nbsp;&#183;&nbsp;Cristiano Ciaccio,&#8201;Felice Dell'Orletta,&#8201;Alessio Miaschi,&#8201;Giulia Venturi</footer><a class=entry-link aria-label="post link to Controllable Text Generation To Evaluate Linguistic Abilities of Italian LLMs" href=https://alemiaschi.github.io/papers/clic_2024/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Fantastic Labels and Where to Find Them: Attention-Based Label Selection for Text-to-Text Classification</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>NL4AI @ AIxIA 2024</p></header><div class=entry-content><p>Generative language models, particularly adopting text-to-text frameworks, have shown significant success in NLP tasks. While much research has focused on input representations via prompting techniques, less attention has been given to optimizing output representations. Previous studies found inconsistent effects of label representations on model performance in classification tasks using these models. In this work, we introduce a novel method for selecting well-performing label representations by leveraging the attention mechanisms of Transformer models. We used an Italian T5 model fine-tuned on a topic classification task, trained on posts extracted from online forums and categorized into 11 classes, to evaluate different label representation selection strategies. We‚Äôve employed a context-mixing score called Value Zeroing to assess each token‚Äôs impact to select possible representations from the training set. Our results include a detailed qualitative analysis to identify which label choices most significantly affect classification outcomes, suggesting that using our approach to select label representations can enhance performance.</p></div><footer class=entry-footer><span title='2024-11-25 00:00:00 +0000 UTC'>November 2024</span>&nbsp;&#183;&nbsp;Michele Papucci,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta</footer><a class=entry-link aria-label="post link to Fantastic Labels and Where to Find Them: Attention-Based Label Selection for Text-to-Text Classification" href=https://alemiaschi.github.io/papers/nl4ai_2024/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Evaluating Large Language Models via Linguistic Profiling</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>EMNLP 2024</p></header><div class=entry-content><p>Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models‚Äô linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs‚Äô sentence generation abilities under specific linguistic constraints. Drawing on the `linguistic profiling‚Äô approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.</p></div><footer class=entry-footer><span title='2024-11-12 00:00:00 +0000 UTC'>November 2024</span>&nbsp;&#183;&nbsp;Alessio Miaschi,&#8201;Felice Dell'Orletta,&#8201;Giulia Venturi</footer><a class=entry-link aria-label="post link to Evaluating Large Language Models via Linguistic Profiling" href=https://alemiaschi.github.io/papers/emnlp_2024/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>LREC-COLING 2024</p></header><div class=entry-content><p>In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.</p></div><footer class=entry-footer><span title='2024-05-20 00:00:00 +0000 UTC'>May 2024</span>&nbsp;&#183;&nbsp;Alessio Miaschi,&#8201;Felice Dell'Orletta,&#8201;Giulia Venturi</footer><a class=entry-link aria-label="post link to Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)" href=https://alemiaschi.github.io/papers/lrec_coling_2024/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>T-FREX: A Transformer-based Feature Extraction Method for Mobile App Reviews</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>SANER 2024</p></header><div class=entry-content><p>Mobile app reviews are a large-scale data source for software-related knowledge generation activities, including software maintenance, evolution and feedback analysis. Effective extraction of features (i.e., functionalities or characteristics) from these reviews is key to support analysis on the acceptance of these features, identification of relevant new feature requests and prioritization of feature development, among others. Traditional methods focus on syntactic pattern-based approaches, typically context-agnostic, evaluated on a closed set of apps, difficult to replicate and limited to a reduced set and domain of apps. Meanwhile, the pervasiveness of Large Language Models (LLMs) based on the Transformer architecture in software engineering tasks lays the groundwork for empirical evaluation of the performance of these models to support feature extraction. In this study, we present T-FREX, a Transformer-based, fully automatic approach for mobile app review feature extraction. First, we collect a set of ground truth features from users in a real crowdsourced software recommendation platform and transfer them automatically into a dataset of app reviews. Then, we use this newly created dataset to fine-tune multiple LLMs on a named entity recognition task under different data configurations. We assess the performance of T-FREX with respect to this ground truth, and we complement our analysis by comparing T-FREX with a baseline method from the field. Finally, we assess the quality of new features predicted by T-FREX through an external human evaluation. Results show that T-FREX outperforms on average the traditional syntactic-based method, especially when discovering new features from a domain for which the model has been fine-tuned.</p></div><footer class=entry-footer><span title='2024-03-12 00:00:00 +0000 UTC'>March 2024</span>&nbsp;&#183;&nbsp;Quim Motger,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta,&#8201;Xavier Franch,&#8201;Jordi Marco</footer><a class=entry-link aria-label="post link to T-FREX: A Transformer-based Feature Extraction Method for Mobile App Reviews" href=https://alemiaschi.github.io/papers/saner_2024/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lost in Labels: An Ongoing Quest to Optimize Text-to-Text Label Selection for Classification</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>CLiC-it 2023</p></header><div class=entry-content><p>In this paper, we present an evaluation of the influence of label selection on the performance of a Sequence-to-Sequence Transformer model in a classification task. Our study investigates whether the choice of words used to represent classification categories affects the model‚Äôs performance, and if there exists a relationship between the model‚Äôs performance and the selected words. To achieve this, we fine-tuned an Italian T5 model on topic classification using various labels. Our results indicate that the different label choices can significantly impact the model‚Äôs performance. That being said, we did not find a clear answer on how these choices affect the model performances, highlighting the need for further research in optimizing label selection.</p></div><footer class=entry-footer><span title='2023-11-30 00:00:00 +0000 UTC'>November 2023</span>&nbsp;&#183;&nbsp;Michele Papucci,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta</footer><a class=entry-link aria-label="post link to Lost in Labels: An Ongoing Quest to Optimize Text-to-Text Label Selection for Classification" href=https://alemiaschi.github.io/papers/clic_2023_2/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Unmasking the Wordsmith: Revealing Author Identity through Reader Reviews</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>CLiC-it 2023</p></header><div class=entry-content><p>Traditional genre-based approaches for book recommendations face challenges due to the vague definition of genres. To overcome this, we propose a novel task called Book Author Prediction, where we predict the author of a book based on user-generated reviews‚Äô writing style. To this aim, we first introduce the `Literary Voices Corpus‚Äô (LVC), a dataset of Italian book reviews, and use it to train and test machine learning models. Our study contributes valuable insights for developing user-centric systems that recommend leisure readings based on individual readers‚Äô interests and writing styles.</p></div><footer class=entry-footer><span title='2023-11-30 00:00:00 +0000 UTC'>November 2023</span>&nbsp;&#183;&nbsp;Chiara Alzetta,&#8201;Felice Dell'Orletta,&#8201;Chiara Fazzone,&#8201;Alessio Miaschi,&#8201;Giulia Venturi</footer><a class=entry-link aria-label="post link to Unmasking the Wordsmith: Revealing Author Identity through Reader Reviews" href=https://alemiaschi.github.io/papers/clic_2023_1/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>LangLearn at EVALITA 2023: Overview of the Language Learning Development Task</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>EVALITA 2023</p></header><div class=entry-content><p>Language Learning Development (LangLearn) is the EVALITA 2023 shared task on automatic language development assessment, which consists in predicting the evolution of the written language abilities of learners across time. LangLearn is conceived to be multilingual, relying on written productions of Italian and Spanish learners, and representative of L1 and L2 learning scenarios. A total of 9 systems were submitted by 5 teams. The results highlight the open challenges of automatic language development assessment.</p></div><footer class=entry-footer><span title='2023-09-07 00:00:00 +0000 UTC'>September 2023</span>&nbsp;&#183;&nbsp;Chiara Alzetta,&#8201;Dominique Brunato,&#8201;Felice Dell'Orletta,&#8201;Alessio Miaschi,&#8201;Kenji Sagae,&#8201;Claudia H. S√°nchez-Guti√©rrez,&#8201;Giulia Venturi</footer><a class=entry-link aria-label="post link to LangLearn at EVALITA 2023: Overview of the Language Learning Development Task" href=https://alemiaschi.github.io/papers/evalita_2023/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Tell me how you write and I'll tell you what you read: a study on the writing style of book reviews</h2><p class=entry-venue style=font-weight:600;font-size:.9em;margin-top:.25em;color:#555>Journal of Documentation</p></header><div class=entry-content><p>The authors‚Äô goal is to investigate variations in the writing style of book reviews published on different social reading platforms and referring to books of different genres, which enables acquiring insights into communication strategies adopted by readers to share their reading experiences. The authors propose a corpus-based study focused on the analysis of A Good Review, a novel corpus of online book reviews written in Italian, posted on Amazon and Goodreads, and covering six literary fiction genres. The authors rely on stylometric analysis to explore the linguistic properties and lexicon of reviews and the authors conducted automatic classification experiments using multiple approaches and feature configurations to predict either the review‚Äôs platform or the literary genre. The analysis of user-generated reviews demonstrates that language is a quite variable dimension across reading platforms, but not as much across book genres. The classification experiments revealed that features modelling the syntactic structure of the sentence are reliable proxies for discerning Amazon and Goodreads reviews, whereas lexical information showed a higher predictive role for automatically discriminating the genre. The high availability of cultural products makes information services necessary to help users navigate these resources and acquire information from unstructured data. This study contributes to a better understanding of the linguistic characteristics of user-generated book reviews, which can support the development of linguistically-informed recommendation services. Additionally, the authors release a novel corpus of online book reviews meant to support the reproducibility and advancements of the research.</p></div><footer class=entry-footer><span title='2023-06-10 00:00:00 +0000 UTC'>June 2023</span>&nbsp;&#183;&nbsp;Chiara Alzetta,&#8201;Felice Dell'Orletta,&#8201;Alessio Miaschi,&#8201;Giulia Venturi</footer><a class=entry-link aria-label="post link to Tell me how you write and I'll tell you what you read: a study on the writing style of book reviews" href=https://alemiaschi.github.io/papers/documentation/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://alemiaschi.github.io/papers/page/2/>Next&nbsp;&nbsp;¬ª</a></nav></footer></main><footer class=footer><span>&copy; 2025 Alessio Miaschi</span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>hugo</a>, <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>papermod</a>, &
<a href=https://github.com/pmichaillat/hugo-website/ rel=noopener target=_blank>hugo-website</a>.</span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script></body></html>