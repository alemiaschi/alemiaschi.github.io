<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Publications | Alessio Miaschi</title>
<meta name="keywords" content="">
<meta name="description" content="Preprints and articles by Alessio Miaschi">
<meta name="author" content="Alessio Miaschi">
<link rel="canonical" href="http://localhost:1313/papers/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ac30954b7ea20660ca1cb473400ecede9e601f17674446536180d3ca00f03328.css" integrity="sha256-rDCVS36iBmDKHLRzQA7O3p5gHxdnREZTYYDTygDwMyg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/papers/index.xml">
<link rel="alternate" hreflang="en" href="http://localhost:1313/papers/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/papers/">
  <meta property="og:site_name" content="Alessio Miaschi">
  <meta property="og:title" content="Publications">
  <meta property="og:description" content="Preprints and articles by Alessio Miaschi">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Publications">
<meta name="twitter:description" content="Preprints and articles by Alessio Miaschi">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Publications",
      "item": "http://localhost:1313/papers/"
    }
  ]
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="list" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Alessio Miaschi">
                <img src="http://localhost:1313/logo.jpg" alt="" aria-label="logo"
                    height="18"
                    width="18">Alessio Miaschi</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/news/" title="News">
                    <span>News</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/papers/" title="Papers">
                    <span class="active">Papers</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/courses/" title="Courses &amp; Theses">
                    <span>Courses &amp; Theses</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Resources &amp; Projects">
                    <span>Resources &amp; Projects</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main"> 
<header class="page-header">
  <h1>
  </h1>
</header>
<div class="post-content"><h1 id="publications">Publications</h1>
<p>You can also check at the following link: <a href="https://scholar.google.it/citations?user=BaL8eJ0AAAAJ&amp;hl=it" target="_blank">https://scholar.google.it/citations?user=BaL8eJ0AAAAJ&hl=it</a></p>

</div>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Testing the Effectiveness of the Diagnostic Probing Paradigm on Italian Treebanks
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">Information</p>
</header>
  <div class="entry-content">
    <p>The outstanding performance recently reached by neural language models (NLMs) across many natural language processing (NLP) tasks has steered the debate towards understanding whether NLMs implicitly learn linguistic competence. Probes, i.e., supervised models trained using NLM representations to predict linguistic properties, are frequently adopted to investigate this issue. However, it is still questioned if probing classification tasks really enable such investigation or if they simply hint at surface patterns in the data. This work contributes to this debate by presenting an approach to assessing the effectiveness of a suite of probing tasks aimed at testing the linguistic knowledge implicitly encoded by one of the most prominent NLMs, BERT. To this aim, we compared the performance of probes when predicting gold and automatically altered values of a set of linguistic features. Our experiments were performed on Italian and were evaluated across BERT‚Äôs layers and for sentences with different lengths. As a general result, we observed higher performance in the prediction of gold values, thus suggesting that the probing model is sensitive to the distortion of feature values. However, our experiments also showed that the length of a sentence is a highly influential factor that is able to confound the probing model‚Äôs predictions.</p>
  </div>
  <footer class="entry-footer"><span title='2023-02-05 00:00:00 +0000 UTC'>February 2023</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Chiara Alzetta,&thinsp;Dominique Brunato,&thinsp;Felice Dell'Orletta,&thinsp;Giulia Venturi</footer>
  <a class="entry-link" aria-label="post link to Testing the Effectiveness of the Diagnostic Probing Paradigm on Italian Treebanks" href="http://localhost:1313/papers/information/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">On Robustness and Sensitivity of a Neural Language Model: A Case Study on Italian L1 Learner Errors
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</p>
</header>
  <div class="entry-content">
    <p>In this paper, we propose a comprehensive linguistic study aimed at assessing the implicit behaviour of one of the most prominent Neural Language Model (NLM) based on Transformer architectures, BERT (Devlin et al., 2019), when dealing with a particular source of noisy data, namely essays written by L1 Italian learners containing a variety of errors targeting grammar, orthography and lexicon. Differently from previous works, we focus on the pre-training stage and we devise two evaluation tasks aimed at assessing the impact of errors on sentence-level inner representations from two complementary perspectives, i.e. robustness and sensitivity. Our experiments show that BERT‚Äôs ability to compute sentence similarity and to correctly encode a set of raw and morpho-syntactic properties of a sentence are differently modulated by the category of errors and that the error hierarchies in terms of robustness and sensitivity change across layer-wise representations.</p>
  </div>
  <footer class="entry-footer"><span title='2022-12-02 00:00:00 +0000 UTC'>December 2022</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Dominique Brunato,&thinsp;Felice Dell'Orletta,&thinsp;Giulia Venturi</footer>
  <a class="entry-link" aria-label="post link to On Robustness and Sensitivity of a Neural Language Model: A Case Study on Italian L1 Learner Errors" href="http://localhost:1313/papers/taslp/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Evaluating Text-To-Text Framework for Topic and Style Classification of Italian texts
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">NL4AI @ AIxIA 2022</p>
</header>
  <div class="entry-content">
    <p>In this paper, we propose an extensive evaluation of the first text-to-text Italian Neural Language Model (NLM), IT5, on a classification scenario. In particular, we test the performance of IT5 on several tasks involving both the classification of the topic and the style of a set of Italian posts. We assess the model in two different configurations, single- and multi-task classification, and we compare it with a more traditional NLM based on the Transformer architecture (i.e. BERT). Moreover, we test its performance in a few-shot learning scenario. We also perform a qualitative investigation on the impact of label representations in modeling the classification of the IT5 model. Results show that IT5 could achieve good results, although generally lower than the BERT model. Nevertheless, we observe a significant performance improvement of the Text-to-text model in a multi-task classification scenario. Finally, we found that altering the representation of the labels mainly impacts the classification of the topic</p>
  </div>
  <footer class="entry-footer"><span title='2022-11-10 00:00:00 +0000 UTC'>November 2022</span>&nbsp;&middot;&nbsp;Michele Papucci,&thinsp;Chiara De Nigris,&thinsp;Alessio Miaschi,&thinsp;Felice Dell'Orletta</footer>
  <a class="entry-link" aria-label="post link to Evaluating Text-To-Text Framework for Topic and Style Classification of Italian texts" href="http://localhost:1313/papers/nl4ai_2022/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">Italian Journal of Computational Linguistics (IJCoL)</p>
</header>
  <div class="entry-content">
    <p>In this paper, we present an in-depth investigation of the linguistic knowledge encoded by the transformer models currently available for the Italian language. In particular, we investigate how the complexity of two different architectures of probing models affects the performance of the Transformers in encoding a wide spectrum of linguistic features. Moreover, we explore how this implicit knowledge varies according to different textual genres and language varieties.</p>
  </div>
  <footer class="entry-footer"><span title='2022-07-10 00:00:00 +0000 UTC'>July 2022</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Gabriele Sarti,&thinsp;Dominique Brunato,&thinsp;Felice Dell'Orletta,&thinsp;Giulia Venturi</footer>
  <a class="entry-link" aria-label="post link to Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties" href="http://localhost:1313/papers/ijcol_2022/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Punctuation Restoration in Spoken Italian Transcripts with Transformers
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">AIxIA 2021 ‚Äì Advances in Artificial Intelligence. AIxIA 2021</p>
</header>
  <div class="entry-content">
    <p>In this paper, we propose an evaluation of a Transformer-based punctuation restoration model for the Italian language. Experimenting with a BERT-base model, we perform several fine-tuning with different training data and sizes and tested them in an in- and cross-domain scenario. Moreover, we conducted an error analysis of the main weaknesses of the model related to specific punctuation marks. Finally, we test our system either quantitatively and qualitatively, by offering a typical task-oriented and a perception-based acceptability evaluation.</p>
  </div>
  <footer class="entry-footer"><span title='2022-07-05 00:00:00 +0000 UTC'>July 2022</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Andrea Amelio Ravelli,&thinsp;Felice Dell'Orletta</footer>
  <a class="entry-link" aria-label="post link to Punctuation Restoration in Spoken Italian Transcripts with Transformers" href="http://localhost:1313/papers/lecture_notes/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Tracking Linguistic Abilities in Neural Language Models
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">PhD Thesis</p>
</header>
  <div class="entry-content">
    <p>In the last few years, the analysis of the inner workings of state-of-the-art Neural Language Models (NLMs) has become one of the most addressed line of research in Natural Language Processing (NLP). Several techniques have been devised to obtain meaningful explanations and to understand how these models are able to capture semantic and linguistic knowledge. The goal of this thesis is to investigate whether exploiting NLP methods for studying human linguistic competence and, specifically, the process of written language evolution is it possible to understand the behaviour of state-of-the-art Neural Language Models (NLMs). First, we present an NLP-based stylometric approach for tracking the evolution of written language competence in L1 and L2 learners using a wide set of linguistically motivated features capturing stylistic aspects of a text. Then, relying on the same set of linguistic features, we propose different approaches aimed at investigating the linguistic knowledge implicitly learned by NLMs. Finally, we propose a study in order to investigate the robustness of one of the most prominent NLM, i.e. BERT, when dealing with different types of errors extracted from authentic texts written by L1 Italian learners.</p>
  </div>
  <footer class="entry-footer"><span title='2022-05-20 00:00:00 +0000 UTC'>May 2022</span>&nbsp;&middot;&nbsp;Alessio Miaschi</footer>
  <a class="entry-link" aria-label="post link to Tracking Linguistic Abilities in Neural Language Models" href="http://localhost:1313/papers/phd_thesis/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">On the role of Textual Connectives in Sentence Comprehension: a new Dataset for Italian
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">CLiC-it 2021</p>
</header>
  <div class="entry-content">
    <p>In this paper we present a new evaluation resource for Italian aimed at assessing the role of textual connectives in the comprehension of the meaning of a sentence. The resource is arranged in two sections (acceptability assessment and cloze test), each one corresponding to a distinct challenge task conceived to test how subtle modifications involving connectives in real usage sentences influence the perceived acceptability of the sentence by native speakers and Neural Language Models (NLMs). Although the main focus is the presentation of the dataset, we also provide some preliminary data comparing human judgments and NLMs performance in the two tasks.</p>
  </div>
  <footer class="entry-footer"><span title='2021-12-10 00:00:00 +0000 UTC'>December 2021</span>&nbsp;&middot;&nbsp;Giorgia Albertin,&thinsp;Alessio Miaschi,&thinsp;Dominique Brunato</footer>
  <a class="entry-link" aria-label="post link to On the role of Textual Connectives in Sentence Comprehension: a new Dataset for Italian" href="http://localhost:1313/papers/clic_2021_2/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Probing Tasks Under Pressure
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">CLiC-it 2021</p>
</header>
  <div class="entry-content">
    <p>Probing tasks are frequently used to evaluate whether the representations of Neural Language Models (NLMs) encode linguistic information. However, it is still questioned if probing classification tasks really enable such investigation or they simply hint for surface patterns in the data. We present a method to investigate this question by comparing the accuracies of a set of probing tasks on gold and automatically generated control datasets. Our results suggest that probing tasks can be used as reliable diagnostic methods to investigate the linguistic information encoded in NLMs representations.</p>
  </div>
  <footer class="entry-footer"><span title='2021-12-10 00:00:00 +0000 UTC'>December 2021</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Chiara Alzetta,&thinsp;Dominique Brunato,&thinsp;Felice Dell'Orletta,&thinsp;Giulia Venturi</footer>
  <a class="entry-link" aria-label="post link to Probing Tasks Under Pressure" href="http://localhost:1313/papers/clic_2021_1/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Evaluating Transformer Models for Punctuation Restoration in Italian
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">NL4AI @ AIxIA 2021</p>
</header>
  <div class="entry-content">
    <p>In this paper, we propose an evaluation of a Transformer-based punctuation restoration model for the Italian language. Experimenting with a BERT-base model, we perform several fine-tuning with different training data and sizes and tested them in an in- and cross-domain scenario. Moreover, we offer a comparison in a multilingual setting with the same model fine-tuned on English transcriptions. Finally, we conclude with an error analysis of the main weaknesses of the model related to specific punctuation marks.</p>
  </div>
  <footer class="entry-footer"><span title='2021-11-10 00:00:00 +0000 UTC'>November 2021</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Andrea Amelio Ravelli,&thinsp;Felice Dell'Orletta</footer>
  <a class="entry-link" aria-label="post link to Evaluating Transformer Models for Punctuation Restoration in Italian" href="http://localhost:1313/papers/nl4ai_2021/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">A dissemination workshop for introducing young Italian students to NLP
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">Teaching NLP @ NAACL 2021</p>
</header>
  <div class="entry-content">
    <p>We describe and make available the game-based material developed for a laboratory run at several Italian science festivals to popularize NLP among young students.</p>
  </div>
  <footer class="entry-footer"><span title='2021-05-10 00:00:00 +0000 UTC'>May 2021</span>&nbsp;&middot;&nbsp;Lucio Messina,&thinsp;Lucia Busso,&thinsp;Claudia Roberta Combei,&thinsp;Ludovica Pannitto,&thinsp;Alessio Miaschi,&thinsp;Gabriele Sarti,&thinsp;Malvina Nissim</footer>
  <a class="entry-link" aria-label="post link to A dissemination workshop for introducing young Italian students to NLP" href="http://localhost:1313/papers/teachinnlp_2021_2/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">How Do BERT Embeddings Organize Linguistic Knowledge?
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">DeeLIO @ NAACL 2021</p>
</header>
  <div class="entry-content">
    <p>Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties.</p>
  </div>
  <footer class="entry-footer"><span title='2021-05-10 00:00:00 +0000 UTC'>May 2021</span>&nbsp;&middot;&nbsp;Giovanni Puccetti,&thinsp;Alessio Miaschi,&thinsp;Felice Dell'Orletta</footer>
  <a class="entry-link" aria-label="post link to How Do BERT Embeddings Organize Linguistic Knowledge?" href="http://localhost:1313/papers/deelio_2021_2/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">Teaching NLP @ NAACL 2021</p>
</header>
  <div class="entry-content">
    <p>Although Natural Language Processing (NLP) is at the core of many tools young people use in their everyday life, high school curricula (in Italy) do not include any computational linguistics education. This lack of exposure makes the use of such tools less responsible than it could be and makes choosing computational linguistics as a university degree unlikely. To raise awareness, curiosity, and longer-term interest in young people, we have developed an interactive workshop designed to illustrate the basic principles of NLP and computational linguistics to high school Italian students aged between 13 and 18 years. The workshop takes the form of a game in which participants play the role of machines needing to solve some of the most common problems a computer faces in understanding language: from voice recognition to Markov chains to syntactic parsing. Participants are guided through the workshop with the help of instructors, who present the activities and explain core concepts from computational linguistics. The workshop was presented at numerous outlets in Italy between 2019 and 2021, both face-to-face and online.</p>
  </div>
  <footer class="entry-footer"><span title='2021-05-10 00:00:00 +0000 UTC'>May 2021</span>&nbsp;&middot;&nbsp;Ludovica Pannitto,&thinsp;Lucia Busso,&thinsp;Claudia Roberta Combei,&thinsp;Lucio Messina,&thinsp;Alessio Miaschi,&thinsp;Gabriele Sarti,&thinsp;Malvina Nissim</footer>
  <a class="entry-link" aria-label="post link to Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students" href="http://localhost:1313/papers/teachinnlp_2021_1/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">DeeLIO @ NAACL 2021</p>
</header>
  <div class="entry-content">
    <p>This paper presents an investigation aimed at studying how the linguistic structure of a sentence affects the perplexity of two of the most popular Neural Language Models (NLMs), BERT and GPT-2. We first compare the sentence-level likelihood computed with BERT and the GPT-2‚Äôs perplexity showing that the two metrics are correlated. In addition, we exploit linguistic features capturing a wide set of morpho-syntactic and syntactic phenomena showing how they contribute to predict the perplexity of the two NLMs.</p>
  </div>
  <footer class="entry-footer"><span title='2021-05-10 00:00:00 +0000 UTC'>May 2021</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Andrea Amelio Ravelli,&thinsp;Felice Dell'Orletta</footer>
  <a class="entry-link" aria-label="post link to What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity" href="http://localhost:1313/papers/deelio_2021_1/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">A NLP-based stylometric approach for tracking the evolution of L1 written language compentece
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">Journal of Writing Research (JoWR)</p>
</header>
  <div class="entry-content">
    <p>In this study we present a Natural Language Processing (NLP)-based stylometric approach for tracking the evolution of written language competence in Italian L1 learners. The approach relies on a wide set of linguistically motivated features capturing stylistic aspects of a text, which were extracted from students‚Äô essays contained in CItA (Corpus Italiano di Apprendenti L1), the first longitudinal corpus of texts written by Italian L1 learners enrolled in the first and second year of lower secondary school. We address the problem of modeling written language development as a supervised classification task consisting in predicting the chronological order of essays written by the same student at different temporal spans. The promising results obtained in several classification scenarios allow us to conclude that it is possible to automatically model the highly relevant changes affecting written language evolution across time, as well as identifying which features are more predictive of this process. In the last part of the article, we focus the attention on the possible influence of background variables on language learning and we present preliminary results of a pilot study aiming at understanding how the observed developmental patterns are affected by information related to the school environment of the student.</p>
  </div>
  <footer class="entry-footer"><span title='2021-04-02 00:00:00 +0000 UTC'>April 2021</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Dominique Brunato,&thinsp;Felice Dell'Orletta</footer>
  <a class="entry-link" aria-label="post link to A NLP-based stylometric approach for tracking the evolution of L1 written language compentece" href="http://localhost:1313/papers/writing_research/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">ATE_ABSITA @ EVALITA2020: Overview of the Aspect Term Extraction and Aspect-based Sentiment Analysis Task
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">EVALITA 2020</p>
</header>
  <div class="entry-content">
    <p>Over the last years, the rise of novel sentiment analysis techniques to assess aspect-based opinions on product reviews has become a key component for providing valuable insights to both consumers and businesses. To this extent, we propose ATE ABSITA: the EVALITA 2020 shared task on Aspect Term Extraction and Aspect-Based Sentiment Analysis. In particular, we approach the task as a cascade of three subtasks: Aspect Term Extraction (ATE), Aspect-based Sentiment Analysis (ABSA) and Sentiment Analysis (SA). Therefore, we invited participants to submit systems designed to automatically identify the ‚Äòaspect term‚Äô in each review and to predict the sentiment expressed for each aspect, along with the sentiment of the entire review. The task received broad interest, with 27 teams registered and more than 45 participants. However, only three teams submitted their working systems. The results obtained underline the task‚Äôs difficulty, but they also show how it is possible to deal with it using innovative approaches and models. Indeed, two of them are based on large pre-trained language models as typical in the current state of the art for the English language.</p>
  </div>
  <footer class="entry-footer"><span title='2020-12-10 00:00:00 +0000 UTC'>December 2020</span>&nbsp;&middot;&nbsp;Lorenzo De Mattei,&thinsp;Graziella De Martino,&thinsp;Andrea Iovine,&thinsp;Alessio Miaschi,&thinsp;Marco Polignano,&thinsp;Giulia Rambelli</footer>
  <a class="entry-link" aria-label="post link to ATE_ABSITA @ EVALITA2020: Overview of the Aspect Term Extraction and Aspect-based Sentiment Analysis Task" href="http://localhost:1313/papers/evalita_2020_2/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Is Neural Language Model Perplexity Related to Readability?
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">CLiC-it 2020</p>
</header>
  <div class="entry-content">
    <p>This paper explores the relationship between Neural Language Model (NLM) perplexity and sentence readability. Starting from the evidence that NLMs implicitly acquire sophisticated linguistic knowledge from a huge amount of training data, our goal is to investigate whether perplexity is affected by linguistic features used to automatically assess sentence readability and if there is a correlation between the two metrics. Our findings suggest that this correlation is actually quite weak and the two metrics are affected by different linguistic phenomena.</p>
  </div>
  <footer class="entry-footer"><span title='2020-12-10 00:00:00 +0000 UTC'>December 2020</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Chiara Alzetta,&thinsp;Dominique Brunato,&thinsp;Felice Dell'Orletta,&thinsp;Giulia Venturi</footer>
  <a class="entry-link" aria-label="post link to Is Neural Language Model Perplexity Related to Readability?" href="http://localhost:1313/papers/clic_2020_2/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Italian Transformers Under the Linguistic Lens
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">CLiC-it 2020</p>
</header>
  <div class="entry-content">
    <p>In this paper we present an in-depth investigation of the linguistic knowledge encoded by the transformer models currently available for the Italian language. In particular, we investigate whether and how using different architectures of probing models affects the performance of Italian transformers in encoding a wide spectrum of linguistic features. Moreover, we explore how this implicit knowledge varies according to different textual genres.</p>
  </div>
  <footer class="entry-footer"><span title='2020-12-10 00:00:00 +0000 UTC'>December 2020</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Gabriele Sarti,&thinsp;Dominique Brunato,&thinsp;Felice Dell'Orletta,&thinsp;Giulia Venturi</footer>
  <a class="entry-link" aria-label="post link to Italian Transformers Under the Linguistic Lens" href="http://localhost:1313/papers/clic_2020_1/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">PRELEARN @ EVALITA 2020: Overview of the Prerequisite Relation Learning Task for Italian
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">EVALITA 2020</p>
</header>
  <div class="entry-content">
    <p>The Prerequisite Relation Learning (PRELEARN) task is the EVALITA 2020 shared task on concept prerequisite learning, which consists of classifying prerequisite relations between pairs of concepts distinguishing between prerequisite pairs and non-prerequisite pairs. Four sub-tasks were defined: two of them define different types of features that participants are allowed to use when training their model, while the other two define the classification scenarios where the proposed models would be tested. In total, 14 runs were submitted by 3 teams comprising 9 total individual participants.</p>
  </div>
  <footer class="entry-footer"><span title='2020-12-10 00:00:00 +0000 UTC'>December 2020</span>&nbsp;&middot;&nbsp;Chiara Alzetta,&thinsp;Alessio Miaschi,&thinsp;Felice Dell'Orletta,&thinsp;Frosina Koceva,&thinsp;Ilaria Torre</footer>
  <a class="entry-link" aria-label="post link to PRELEARN @ EVALITA 2020: Overview of the Prerequisite Relation Learning Task for Italian" href="http://localhost:1313/papers/evalita_2020_1/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Linguistic Profiling of a Neural Language Model [üèÜ Outstanding Paper Award üèÜ]
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">LREC-COLING 2024</p>
</header>
  <div class="entry-content">
    <p>In this paper we investigate the linguistic knowledge learned by a Neural Language Model (NLM) before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT‚Äôs capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence.</p>
  </div>
  <footer class="entry-footer"><span title='2020-06-20 00:00:00 +0000 UTC'>June 2020</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Dominique Brunato,&thinsp;Felice Dell'Orletta,&thinsp;Giulia Venturi</footer>
  <a class="entry-link" aria-label="post link to Linguistic Profiling of a Neural Language Model [üèÜ Outstanding Paper Award üèÜ]" href="http://localhost:1313/papers/coling_2020/"></a>
</article>

<article class="post-entry"> 
<header class="entry-header">
  <h2 class="entry-hint-parent">Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation
  </h2>
  <p class="entry-venue" style="font-weight: 600; font-size: 0.9em; margin-top: 0.25em; color: #555555;">RepL4NLP @ ACL 2020</p>
</header>
  <div class="entry-content">
    <p>In this paper we present a comparison between the linguistic knowledge encoded in the internal representations of a contextual Language Model (BERT) and a contextual-independent one (Word2vec). We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that, although BERT is capable of understanding the full context of each word in an input sequence, the implicit knowledge encoded in its aggregated sentence representations is still comparable to that of a contextual-independent model. We also find that BERT is able to encode sentence-level properties even within single-word embeddings, obtaining comparable or even superior results than those obtained with sentence representations.</p>
  </div>
  <footer class="entry-footer"><span title='2020-06-15 00:00:00 +0000 UTC'>June 2020</span>&nbsp;&middot;&nbsp;Alessio Miaschi,&thinsp;Felice Dell'Orletta</footer>
  <a class="entry-link" aria-label="post link to Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation" href="http://localhost:1313/papers/repl4nlp_2020/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="http://localhost:1313/papers/">
      ¬´&nbsp;Prev&nbsp;
    </a>
    <a class="next" href="http://localhost:1313/papers/page/3/">Next&nbsp;&nbsp;¬ª
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 Alessio Miaschi</span> ¬∑     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">hugo</a>, <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">papermod</a>, &amp;
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">hugo-website</a>.
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
</body>
</html>
