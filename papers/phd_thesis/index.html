<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Tracking Linguistic Abilities in Neural Language Models | Alessio Miaschi</title>
<meta name="keywords" content="">
<meta name="description" content="In the last few years, the analysis of the inner workings of state-of-the-art Neural Language Models (NLMs) has become one of the most addressed line of research in Natural Language Processing (NLP). Several techniques have been devised to obtain meaningful explanations and to understand how these models are able to capture semantic and linguistic knowledge. The goal of this thesis is to investigate whether exploiting NLP methods for studying human linguistic competence and, specifically, the process of written language evolution is it possible to understand the behaviour of state-of-the-art Neural Language Models (NLMs). First, we present an NLP-based stylometric approach for tracking the evolution of written language competence in L1 and L2 learners using a wide set of linguistically motivated features capturing stylistic aspects of a text. Then, relying on the same set of linguistic features, we propose different approaches aimed at investigating the linguistic knowledge implicitly learned by NLMs. Finally, we propose a study in order to investigate the robustness of one of the most prominent NLM, i.e. BERT, when dealing with different types of errors extracted from authentic texts written by L1 Italian learners.">
<meta name="author" content="Alessio Miaschi">
<link rel="canonical" href="http://localhost:1313/papers/phd_thesis/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ac30954b7ea20660ca1cb473400ecede9e601f17674446536180d3ca00f03328.css" integrity="sha256-rDCVS36iBmDKHLRzQA7O3p5gHxdnREZTYYDTygDwMyg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/papers/phd_thesis/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/papers/phd_thesis/">
  <meta property="og:site_name" content="Alessio Miaschi">
  <meta property="og:title" content="Tracking Linguistic Abilities in Neural Language Models">
  <meta property="og:description" content="In the last few years, the analysis of the inner workings of state-of-the-art Neural Language Models (NLMs) has become one of the most addressed line of research in Natural Language Processing (NLP). Several techniques have been devised to obtain meaningful explanations and to understand how these models are able to capture semantic and linguistic knowledge. The goal of this thesis is to investigate whether exploiting NLP methods for studying human linguistic competence and, specifically, the process of written language evolution is it possible to understand the behaviour of state-of-the-art Neural Language Models (NLMs). First, we present an NLP-based stylometric approach for tracking the evolution of written language competence in L1 and L2 learners using a wide set of linguistically motivated features capturing stylistic aspects of a text. Then, relying on the same set of linguistic features, we propose different approaches aimed at investigating the linguistic knowledge implicitly learned by NLMs. Finally, we propose a study in order to investigate the robustness of one of the most prominent NLM, i.e. BERT, when dealing with different types of errors extracted from authentic texts written by L1 Italian learners.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">
    <meta property="article:published_time" content="2022-05-20T00:00:00+00:00">
    <meta property="article:modified_time" content="2022-05-20T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tracking Linguistic Abilities in Neural Language Models">
<meta name="twitter:description" content="In the last few years, the analysis of the inner workings of state-of-the-art Neural Language Models (NLMs) has become one of the most addressed line of research in Natural Language Processing (NLP). Several techniques have been devised to obtain meaningful explanations and to understand how these models are able to capture semantic and linguistic knowledge. The goal of this thesis is to investigate whether exploiting NLP methods for studying human linguistic competence and, specifically, the process of written language evolution is it possible to understand the behaviour of state-of-the-art Neural Language Models (NLMs). First, we present an NLP-based stylometric approach for tracking the evolution of written language competence in L1 and L2 learners using a wide set of linguistically motivated features capturing stylistic aspects of a text. Then, relying on the same set of linguistic features, we propose different approaches aimed at investigating the linguistic knowledge implicitly learned by NLMs. Finally, we propose a study in order to investigate the robustness of one of the most prominent NLM, i.e. BERT, when dealing with different types of errors extracted from authentic texts written by L1 Italian learners.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Publications",
      "item": "http://localhost:1313/papers/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Tracking Linguistic Abilities in Neural Language Models",
      "item": "http://localhost:1313/papers/phd_thesis/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Tracking Linguistic Abilities in Neural Language Models",
  "name": "Tracking Linguistic Abilities in Neural Language Models",
  "description": "In the last few years, the analysis of the inner workings of state-of-the-art Neural Language Models (NLMs) has become one of the most addressed line of research in Natural Language Processing (NLP). Several techniques have been devised to obtain meaningful explanations and to understand how these models are able to capture semantic and linguistic knowledge. The goal of this thesis is to investigate whether exploiting NLP methods for studying human linguistic competence and, specifically, the process of written language evolution is it possible to understand the behaviour of state-of-the-art Neural Language Models (NLMs). First, we present an NLP-based stylometric approach for tracking the evolution of written language competence in L1 and L2 learners using a wide set of linguistically motivated features capturing stylistic aspects of a text. Then, relying on the same set of linguistic features, we propose different approaches aimed at investigating the linguistic knowledge implicitly learned by NLMs. Finally, we propose a study in order to investigate the robustness of one of the most prominent NLM, i.e. BERT, when dealing with different types of errors extracted from authentic texts written by L1 Italian learners.",
  "keywords": [
    
  ],
  "articleBody": "Download Paper Abstract In the last few years, the analysis of the inner workings of state-of-the-art Neural Language Models (NLMs) has become one of the most addressed line of research in Natural Language Processing (NLP). Several techniques have been devised to obtain meaningful explanations and to understand how these models are able to capture semantic and linguistic knowledge. The goal of this thesis is to investigate whether exploiting NLP methods for studying human linguistic competence and, specifically, the process of written language evolution is it possible to understand the behaviour of state-of-the-art Neural Language Models (NLMs). First, we present an NLP-based stylometric approach for tracking the evolution of written language competence in L1 and L2 learners using a wide set of linguistically motivated features capturing stylistic aspects of a text. Then, relying on the same set of linguistic features, we propose different approaches aimed at investigating the linguistic knowledge implicitly learned by NLMs. Finally, we propose a study in order to investigate the robustness of one of the most prominent NLM, i.e. BERT, when dealing with different types of errors extracted from authentic texts written by L1 Italian learners.\nCitation @phdthesis{miaschi-2022-tracking-linguistic-abilities, author = \"Miaschi, Alessio\", title = \"Tracking Linguistic Abilities in Neural Language Models\", school = \"Department of Computer Science, University of Pisa, Pisa\", year = \"2022\", month = {may} } ",
  "wordCount" : "221",
  "inLanguage": "en",
  "datePublished": "2022-05-20T00:00:00Z",
  "dateModified": "2022-05-20T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Alessio Miaschi"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/papers/phd_thesis/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Alessio Miaschi",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Alessio Miaschi">
                <img src="http://localhost:1313/logo.jpg" alt="" aria-label="logo"
                    height="18"
                    width="18">Alessio Miaschi</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/news/" title="News">
                    <span>News</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/courses/" title="Courses &amp; Theses">
                    <span>Courses &amp; Theses</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Resources &amp; Projects">
                    <span>Resources &amp; Projects</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Tracking Linguistic Abilities in Neural Language Models
    </h1>
    <div class="post-meta"><span title='2022-05-20 00:00:00 +0000 UTC'>May 2022</span>&nbsp;&middot;&nbsp;Alessio Miaschi&nbsp;&middot;&nbsp;<a href="https://etd.adm.unipi.it/theses/available/etd-05062022-162420/" rel="noopener noreferrer" target="_blank">PhD Thesis in Computer Science, University of Pisa</a>

</div>
  </header> 
  <div class="post-content"><h5 id="download">Download</h5>
<ul>
<li><a href="https://etd.adm.unipi.it/theses/available/etd-05062022-162420/" target="_blank">Paper</a></li>
</ul>
<hr>
<h5 id="abstract">Abstract</h5>
<p>In the last few years, the analysis of the inner workings of state-of-the-art Neural Language Models (NLMs) has become one of the most addressed line of research in Natural Language Processing (NLP). Several techniques have been devised to obtain meaningful explanations and to understand how these models are able to capture semantic and linguistic knowledge. The goal of this thesis is to investigate whether exploiting NLP methods for studying human linguistic competence and, specifically, the process of written language evolution is it possible to understand the behaviour of state-of-the-art Neural Language Models (NLMs). First, we present an NLP-based stylometric approach for tracking the evolution of written language competence in L1 and L2 learners using a wide set of linguistically motivated features capturing stylistic aspects of a text. Then, relying on the same set of linguistic features, we propose different approaches aimed at investigating the linguistic knowledge implicitly learned by NLMs. Finally, we propose a study in order to investigate the robustness of one of the most prominent NLM, i.e. BERT, when dealing with different types of errors extracted from authentic texts written by L1 Italian learners.</p>
<hr>
<h5 id="citation">Citation</h5>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-BibTeX" data-lang="BibTeX"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@phdthesis</span>{miaschi-2022-tracking-linguistic-abilities,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span>  = <span style="color:#a50">&#34;Miaschi, Alessio&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span>   = <span style="color:#a50">&#34;Tracking Linguistic Abilities in Neural Language Models&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">school</span>  = <span style="color:#a50">&#34;Department of Computer Science, University of Pisa, Pisa&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span>    = <span style="color:#a50">&#34;2022&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">month</span> = <span style="color:#a50">{may}</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 Alessio Miaschi</span> Â·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">hugo</a>, <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">papermod</a>, &amp;
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">hugo-website</a>.
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
