<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models | Alessio Miaschi</title><meta name=keywords content><meta name=description content="Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are &lsquo;character-blind&rsquo; and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots."><meta name=author content="Cristiano Ciaccio,&#8201;Marta Sartor,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta"><link rel=canonical href=https://alemiaschi.github.io/papers/acl_2025_2/><link crossorigin=anonymous href=/assets/css/stylesheet.ac30954b7ea20660ca1cb473400ecede9e601f17674446536180d3ca00f03328.css integrity="sha256-rDCVS36iBmDKHLRzQA7O3p5gHxdnREZTYYDTygDwMyg=" rel="preload stylesheet" as=style><link rel=icon href=https://alemiaschi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://alemiaschi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://alemiaschi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://alemiaschi.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://alemiaschi.github.io/papers/acl_2025_2/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://alemiaschi.github.io/papers/acl_2025_2/"><meta property="og:site_name" content="Alessio Miaschi"><meta property="og:title" content="Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models"><meta property="og:description" content="Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are ‘character-blind’ and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="papers"><meta property="article:published_time" content="2025-07-27T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-27T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models"><meta name=twitter:description content="Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are &lsquo;character-blind&rsquo; and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publications","item":"https://alemiaschi.github.io/papers/"},{"@type":"ListItem","position":2,"name":"Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models","item":"https://alemiaschi.github.io/papers/acl_2025_2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models","name":"Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models","description":"Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are \u0026lsquo;character-blind\u0026rsquo; and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots.","keywords":[],"articleBody":"Download Paper Abstract Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are ‘character-blind’ and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots.\nCitation @inproceedings{ciaccio-etal-2025-beyond, title = \"Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models\", author = \"Ciaccio, Cristiano and Sartor, Marta and Miaschi, Alessio and Dell{'}Orletta, Felice\", editor = \"Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher\", booktitle = \"Findings of the Association for Computational Linguistics: ACL 2025\", month = jul, year = \"2025\", address = \"Vienna, Austria\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2025.findings-acl.593/\", doi = \"10.18653/v1/2025.findings-acl.593\", pages = \"11361--11372\", ISBN = \"979-8-89176-256-5\", abstract = \"Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are ``character-blind'' and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots.\" }``` --- ","wordCount":"328","inLanguage":"en","datePublished":"2025-07-27T00:00:00Z","dateModified":"2025-07-27T00:00:00Z","author":[{"@type":"Person","name":"Cristiano Ciaccio"},{"@type":"Person","name":"Marta Sartor"},{"@type":"Person","name":"Alessio Miaschi"},{"@type":"Person","name":"Felice Dell'Orletta"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://alemiaschi.github.io/papers/acl_2025_2/"},"publisher":{"@type":"Organization","name":"Alessio Miaschi","logo":{"@type":"ImageObject","url":"https://alemiaschi.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{equation*}",right:"\\end{equation*}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1})})</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://alemiaschi.github.io/ accesskey=h title="Alessio Miaschi"><img src=https://alemiaschi.github.io/logo.jpg alt aria-label=logo height=18 width=18>Alessio Miaschi</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://alemiaschi.github.io/news/ title=News><span>News</span></a></li><li><a href=https://alemiaschi.github.io/papers/ title=Papers><span>Papers</span></a></li><li><a href=https://alemiaschi.github.io/courses/ title="Courses & Theses"><span>Courses & Theses</span></a></li><li><a href=https://alemiaschi.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://alemiaschi.github.io/projects/ title="Resources & Projects"><span>Resources & Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models</h1><div class=post-meta><span title='2025-07-27 00:00:00 +0000 UTC'>July 2025</span>&nbsp;&#183;&nbsp;Cristiano Ciaccio,&#8201;Marta Sartor,&#8201;Alessio Miaschi,&#8201;Felice Dell'Orletta&nbsp;&#183;&nbsp;<a href=https://2025.aclweb.org/ rel="noopener noreferrer" target=_blank>Proceedings of the Findings of the 2025 Annual Meeting of the Association for Computational Linguistics (Findings of ACL 2025, Vienna, Austria)</a></div></header><div class=post-content><h5 id=download>Download</h5><ul><li><a href=https://aclanthology.org/2025.findings-acl.593.pdf target=_blank>Paper</a></li></ul><hr><h5 id=abstract>Abstract</h5><p>Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are &lsquo;character-blind&rsquo; and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots.</p><hr><h5 id=citation>Citation</h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-BibTeX data-lang=BibTeX><span style=display:flex><span><span style=color:#0a0;text-decoration:underline>@inproceedings</span>{ciaccio-etal-2025-beyond,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>title</span> = <span style=color:#a50>&#34;Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>author</span> = <span style=color:#a50>&#34;Ciaccio, Cristiano  and
</span></span></span><span style=display:flex><span><span style=color:#a50>      Sartor, Marta  and
</span></span></span><span style=display:flex><span><span style=color:#a50>      Miaschi, Alessio  and
</span></span></span><span style=display:flex><span><span style=color:#a50>      Dell{&#39;}Orletta, Felice&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>editor</span> = <span style=color:#a50>&#34;Che, Wanxiang  and
</span></span></span><span style=display:flex><span><span style=color:#a50>      Nabende, Joyce  and
</span></span></span><span style=display:flex><span><span style=color:#a50>      Shutova, Ekaterina  and
</span></span></span><span style=display:flex><span><span style=color:#a50>      Pilehvar, Mohammad Taher&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>booktitle</span> = <span style=color:#a50>&#34;Findings of the Association for Computational Linguistics: ACL 2025&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>month</span> = <span style=color:#a00>jul</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>year</span> = <span style=color:#a50>&#34;2025&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>address</span> = <span style=color:#a50>&#34;Vienna, Austria&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>publisher</span> = <span style=color:#a50>&#34;Association for Computational Linguistics&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>url</span> = <span style=color:#a50>&#34;https://aclanthology.org/2025.findings-acl.593/&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>doi</span> = <span style=color:#a50>&#34;10.18653/v1/2025.findings-acl.593&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>pages</span> = <span style=color:#a50>&#34;11361--11372&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>ISBN</span> = <span style=color:#a50>&#34;979-8-89176-256-5&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#1e90ff>abstract</span> = <span style=color:#a50>&#34;Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are ``character-blind&#39;&#39; and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots.&#34;</span>
</span></span><span style=display:flex><span>}<span style=color:#aaa;font-style:italic>```</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#aaa;font-style:italic>---</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 Alessio Miaschi</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>hugo</a>, <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>papermod</a>, &
<a href=https://github.com/pmichaillat/hugo-website/ rel=noopener target=_blank>hugo-website</a>.</span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>